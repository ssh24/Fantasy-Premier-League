{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and the test weekly fpl scrape\n",
    "season=\"2019-20\"\n",
    "gw=17\n",
    "model_type=\"nn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(os.getcwd(), \"..\", \"..\", \"data\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"x-train.csv\");\n",
    "X_test = pd.read_csv(\"x-test.csv\");\n",
    "X_val = pd.read_csv(\"x-val.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv(\"y-train.csv\");\n",
    "y_test = pd.read_csv(\"y-test.csv\");\n",
    "y_val = pd.read_csv(\"y-val.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape:  (4404, 68)\n",
      "y train shape:  (4404, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X train shape: \", X_train.shape)\n",
    "print(\"y train shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X test shape:  (1468, 68)\n",
      "y test shape:  (1468, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X test shape: \", X_test.shape)\n",
    "print(\"y test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X val shape:  (1468, 68)\n",
      "y val shape:  (1468, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X val shape: \", X_val.shape)\n",
    "print(\"y val shape: \", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_scaled = scaler.fit_transform(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim, dropout=0.1, l2_reg=0.01, regress=False):\n",
    "    # define our MLP network\n",
    "    model = Sequential();\n",
    "    \n",
    "    model.add(Dense(8, input_dim=dim, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    "    model.add(Dense(7, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    "    model.add(Dense(6, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    "    model.add(Dense(5, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    "    model.add(Dense(4, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    "    model.add(Dense(3, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    "    model.add(Dense(2, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    " \n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        model.add(Dense(1, kernel_regularizer=regularizers.l2(l2_reg), activation=\"linear\"));\n",
    " \n",
    "    # return our model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1213 11:29:30.515365 4705365312 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1213 11:29:30.530734 4705365312 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1213 11:29:30.533368 4705365312 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1213 11:29:30.549865 4705365312 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1213 11:29:30.556805 4705365312 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1213 11:29:30.783942 4705365312 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = create_mlp(X_train.shape[1], dropout=0.07, l2_reg=0.07, regress=True)\n",
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=opt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=10,\n",
    "                              verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1213 11:29:31.262804 4705365312 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4404 samples, validate on 1468 samples\n",
      "Epoch 1/500\n",
      "4404/4404 [==============================] - 1s 224us/step - loss: 56.0384 - val_loss: 31.0094\n",
      "Epoch 2/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 45.5764 - val_loss: 30.4903\n",
      "Epoch 3/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 38.7110 - val_loss: 29.9829\n",
      "Epoch 4/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 36.2550 - val_loss: 29.4965\n",
      "Epoch 5/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 33.2755 - val_loss: 29.0186\n",
      "Epoch 6/500\n",
      "4404/4404 [==============================] - 0s 62us/step - loss: 32.1752 - val_loss: 28.5618\n",
      "Epoch 7/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 30.2467 - val_loss: 28.1036\n",
      "Epoch 8/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 29.2088 - val_loss: 27.6609\n",
      "Epoch 9/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 28.5537 - val_loss: 27.1974\n",
      "Epoch 10/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 27.9593 - val_loss: 26.6710\n",
      "Epoch 11/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 27.1589 - val_loss: 26.0455\n",
      "Epoch 12/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 26.3621 - val_loss: 25.1963\n",
      "Epoch 13/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 25.4315 - val_loss: 24.3119\n",
      "Epoch 14/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 24.7900 - val_loss: 23.2383\n",
      "Epoch 15/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 23.8839 - val_loss: 22.1966\n",
      "Epoch 16/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 22.8030 - val_loss: 21.3695\n",
      "Epoch 17/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 22.1462 - val_loss: 20.6802\n",
      "Epoch 18/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 21.6145 - val_loss: 20.1241\n",
      "Epoch 19/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 21.0358 - val_loss: 19.6334\n",
      "Epoch 20/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 20.9031 - val_loss: 19.2246\n",
      "Epoch 21/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 20.2994 - val_loss: 18.8527\n",
      "Epoch 22/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 19.9749 - val_loss: 18.5117\n",
      "Epoch 23/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 19.6992 - val_loss: 18.2022\n",
      "Epoch 24/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 19.2511 - val_loss: 17.9106\n",
      "Epoch 25/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 19.1410 - val_loss: 17.6388\n",
      "Epoch 26/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 18.7777 - val_loss: 17.3755\n",
      "Epoch 27/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 18.4680 - val_loss: 17.1249\n",
      "Epoch 28/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 18.2289 - val_loss: 16.8885\n",
      "Epoch 29/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 17.8684 - val_loss: 16.6518\n",
      "Epoch 30/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 17.7890 - val_loss: 16.4268\n",
      "Epoch 31/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 17.7120 - val_loss: 16.2165\n",
      "Epoch 32/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 17.2853 - val_loss: 16.0012\n",
      "Epoch 33/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 17.0909 - val_loss: 15.7899\n",
      "Epoch 34/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 16.8405 - val_loss: 15.5881\n",
      "Epoch 35/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 16.6770 - val_loss: 15.3928\n",
      "Epoch 36/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 16.2942 - val_loss: 15.1970\n",
      "Epoch 37/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 16.0609 - val_loss: 15.0048\n",
      "Epoch 38/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 15.8866 - val_loss: 14.8175\n",
      "Epoch 39/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 15.6937 - val_loss: 14.6342\n",
      "Epoch 40/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 15.5336 - val_loss: 14.4586\n",
      "Epoch 41/500\n",
      "4404/4404 [==============================] - 0s 62us/step - loss: 15.3288 - val_loss: 14.2835\n",
      "Epoch 42/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 14.9931 - val_loss: 14.1102\n",
      "Epoch 43/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 14.8968 - val_loss: 13.9425\n",
      "Epoch 44/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 14.6046 - val_loss: 13.7763\n",
      "Epoch 45/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 14.4826 - val_loss: 13.6162\n",
      "Epoch 46/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 14.2681 - val_loss: 13.4581\n",
      "Epoch 47/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 14.1443 - val_loss: 13.3043\n",
      "Epoch 48/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 14.0268 - val_loss: 13.1530\n",
      "Epoch 49/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 13.8057 - val_loss: 13.0061\n",
      "Epoch 50/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 13.6320 - val_loss: 12.8624\n",
      "Epoch 51/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 13.4825 - val_loss: 12.7211\n",
      "Epoch 52/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 13.4643 - val_loss: 12.5877\n",
      "Epoch 53/500\n",
      "4404/4404 [==============================] - 0s 61us/step - loss: 13.2633 - val_loss: 12.4548\n",
      "Epoch 54/500\n",
      "4404/4404 [==============================] - 0s 62us/step - loss: 12.9833 - val_loss: 12.3241\n",
      "Epoch 55/500\n",
      "4404/4404 [==============================] - 0s 61us/step - loss: 12.9517 - val_loss: 12.1977\n",
      "Epoch 56/500\n",
      "4404/4404 [==============================] - 0s 70us/step - loss: 12.7405 - val_loss: 12.0748\n",
      "Epoch 57/500\n",
      "4404/4404 [==============================] - 0s 71us/step - loss: 12.7325 - val_loss: 11.9526\n",
      "Epoch 58/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 12.5406 - val_loss: 11.8350\n",
      "Epoch 59/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 12.4320 - val_loss: 11.7194\n",
      "Epoch 60/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 12.6751 - val_loss: 11.6135\n",
      "Epoch 61/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 12.3093 - val_loss: 11.5088\n",
      "Epoch 62/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 12.0293 - val_loss: 11.4061\n",
      "Epoch 63/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 12.0595 - val_loss: 11.3061\n",
      "Epoch 64/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 11.9174 - val_loss: 11.2079\n",
      "Epoch 65/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 11.7526 - val_loss: 11.1123\n",
      "Epoch 66/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 11.7784 - val_loss: 11.0201\n",
      "Epoch 67/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 11.6380 - val_loss: 10.9299\n",
      "Epoch 68/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 11.5002 - val_loss: 10.8426\n",
      "Epoch 69/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 11.4471 - val_loss: 10.7562\n",
      "Epoch 70/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 11.1980 - val_loss: 10.6724\n",
      "Epoch 71/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 11.1894 - val_loss: 10.5912\n",
      "Epoch 72/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 11.2119 - val_loss: 10.5128\n",
      "Epoch 73/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 11.0862 - val_loss: 10.4372\n",
      "Epoch 74/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 11.0655 - val_loss: 10.3639\n",
      "Epoch 75/500\n",
      "4404/4404 [==============================] - 0s 61us/step - loss: 11.1740 - val_loss: 10.2934\n",
      "Epoch 76/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 10.8513 - val_loss: 10.2259\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4404/4404 [==============================] - 0s 60us/step - loss: 10.8019 - val_loss: 10.1606\n",
      "Epoch 78/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 10.8510 - val_loss: 10.0972\n",
      "Epoch 79/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 10.6259 - val_loss: 10.0377\n",
      "Epoch 80/500\n",
      "4404/4404 [==============================] - 0s 64us/step - loss: 10.5750 - val_loss: 9.9786\n",
      "Epoch 81/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 10.6899 - val_loss: 9.9220\n",
      "Epoch 82/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 10.5235 - val_loss: 9.8677\n",
      "Epoch 83/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 10.4748 - val_loss: 9.8153\n",
      "Epoch 84/500\n",
      "4404/4404 [==============================] - 0s 74us/step - loss: 10.5233 - val_loss: 9.7672\n",
      "Epoch 85/500\n",
      "4404/4404 [==============================] - 0s 71us/step - loss: 10.3940 - val_loss: 9.7186\n",
      "Epoch 86/500\n",
      "4404/4404 [==============================] - 0s 72us/step - loss: 10.2054 - val_loss: 9.6726\n",
      "Epoch 87/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 10.4240 - val_loss: 9.6296\n",
      "Epoch 88/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 10.2337 - val_loss: 9.5857\n",
      "Epoch 89/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 10.1644 - val_loss: 9.5465\n",
      "Epoch 90/500\n",
      "4404/4404 [==============================] - 0s 49us/step - loss: 10.1632 - val_loss: 9.5064\n",
      "Epoch 91/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 10.1232 - val_loss: 9.4689\n",
      "Epoch 92/500\n",
      "4404/4404 [==============================] - 0s 49us/step - loss: 10.0556 - val_loss: 9.4346\n",
      "Epoch 93/500\n",
      "4404/4404 [==============================] - 0s 49us/step - loss: 10.0247 - val_loss: 9.4015\n",
      "Epoch 94/500\n",
      "4404/4404 [==============================] - 0s 49us/step - loss: 10.0027 - val_loss: 9.3722\n",
      "Epoch 95/500\n",
      "4404/4404 [==============================] - 0s 50us/step - loss: 9.8691 - val_loss: 9.3421\n",
      "Epoch 96/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 9.9450 - val_loss: 9.3157\n",
      "Epoch 97/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 9.9429 - val_loss: 9.2890\n",
      "Epoch 98/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 9.9097 - val_loss: 9.2640\n",
      "Epoch 99/500\n",
      "4404/4404 [==============================] - 0s 62us/step - loss: 9.9069 - val_loss: 9.2399\n",
      "Epoch 100/500\n",
      "4404/4404 [==============================] - 0s 51us/step - loss: 9.7692 - val_loss: 9.2169\n",
      "Epoch 101/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 9.8528 - val_loss: 9.1966\n",
      "Epoch 102/500\n",
      "4404/4404 [==============================] - 0s 50us/step - loss: 9.8596 - val_loss: 9.1763\n",
      "Epoch 103/500\n",
      "4404/4404 [==============================] - 0s 48us/step - loss: 9.9347 - val_loss: 9.1580\n",
      "Epoch 104/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 9.7654 - val_loss: 9.1384\n",
      "Epoch 105/500\n",
      "4404/4404 [==============================] - 0s 49us/step - loss: 9.7048 - val_loss: 9.1210\n",
      "Epoch 106/500\n",
      "4404/4404 [==============================] - 0s 49us/step - loss: 9.7779 - val_loss: 9.1041\n",
      "Epoch 107/500\n",
      "4404/4404 [==============================] - 0s 66us/step - loss: 9.7841 - val_loss: 9.0878\n",
      "Epoch 108/500\n",
      "4404/4404 [==============================] - 0s 68us/step - loss: 9.7609 - val_loss: 9.0719\n",
      "Epoch 109/500\n",
      "4404/4404 [==============================] - 0s 63us/step - loss: 9.7078 - val_loss: 9.0564\n",
      "Epoch 110/500\n",
      "4404/4404 [==============================] - 0s 93us/step - loss: 9.7319 - val_loss: 9.0423\n",
      "Epoch 111/500\n",
      "4404/4404 [==============================] - 0s 85us/step - loss: 9.7231 - val_loss: 9.0289\n",
      "Epoch 112/500\n",
      "4404/4404 [==============================] - 0s 70us/step - loss: 9.5741 - val_loss: 9.0150\n",
      "Epoch 113/500\n",
      "4404/4404 [==============================] - 0s 68us/step - loss: 9.5872 - val_loss: 9.0028\n",
      "Epoch 114/500\n",
      "4404/4404 [==============================] - 0s 73us/step - loss: 9.6495 - val_loss: 8.9904\n",
      "Epoch 115/500\n",
      "4404/4404 [==============================] - 0s 73us/step - loss: 9.5555 - val_loss: 8.9788\n",
      "Epoch 116/500\n",
      "4404/4404 [==============================] - 0s 65us/step - loss: 9.6623 - val_loss: 8.9674\n",
      "Epoch 117/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 9.6384 - val_loss: 8.9556\n",
      "Epoch 118/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 9.6529 - val_loss: 8.9453\n",
      "Epoch 119/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 9.4899 - val_loss: 8.9344\n",
      "Epoch 120/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 9.4961 - val_loss: 8.9244\n",
      "Epoch 121/500\n",
      "4404/4404 [==============================] - 0s 51us/step - loss: 9.6491 - val_loss: 8.9146\n",
      "Epoch 122/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 9.6067 - val_loss: 8.9040\n",
      "Epoch 123/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 9.5279 - val_loss: 8.8933\n",
      "Epoch 124/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 9.5014 - val_loss: 8.8828\n",
      "Epoch 125/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 9.5008 - val_loss: 8.8737\n",
      "Epoch 126/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 9.5484 - val_loss: 8.8649\n",
      "Epoch 127/500\n",
      "4404/4404 [==============================] - 0s 50us/step - loss: 9.5741 - val_loss: 8.8564\n",
      "Epoch 128/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 9.4304 - val_loss: 8.8466\n",
      "Epoch 129/500\n",
      "4404/4404 [==============================] - 0s 50us/step - loss: 9.3782 - val_loss: 8.8367\n",
      "Epoch 130/500\n",
      "4404/4404 [==============================] - 0s 51us/step - loss: 9.4976 - val_loss: 8.8278\n",
      "Epoch 131/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 9.4048 - val_loss: 8.8175\n",
      "Epoch 132/500\n",
      "4404/4404 [==============================] - 0s 80us/step - loss: 9.4249 - val_loss: 8.8086\n",
      "Epoch 133/500\n",
      "4404/4404 [==============================] - 0s 67us/step - loss: 9.3253 - val_loss: 8.7977\n",
      "Epoch 134/500\n",
      "4404/4404 [==============================] - 0s 73us/step - loss: 9.4579 - val_loss: 8.7857\n",
      "Epoch 135/500\n",
      "4404/4404 [==============================] - 0s 67us/step - loss: 9.4243 - val_loss: 8.7743\n",
      "Epoch 136/500\n",
      "4404/4404 [==============================] - 0s 65us/step - loss: 9.4292 - val_loss: 8.7569\n",
      "Epoch 137/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 9.4342 - val_loss: 8.7338\n",
      "Epoch 138/500\n",
      "4404/4404 [==============================] - 0s 65us/step - loss: 9.4271 - val_loss: 8.7056\n",
      "Epoch 139/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 9.1705 - val_loss: 8.6572\n",
      "Epoch 140/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 9.2801 - val_loss: 8.6223\n",
      "Epoch 141/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 9.2004 - val_loss: 8.5838\n",
      "Epoch 142/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 9.1907 - val_loss: 8.5495\n",
      "Epoch 143/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 9.1889 - val_loss: 8.5154\n",
      "Epoch 144/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 9.1925 - val_loss: 8.4764\n",
      "Epoch 145/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 9.0960 - val_loss: 8.4375\n",
      "Epoch 146/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 9.0386 - val_loss: 8.4012\n",
      "Epoch 147/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.9616 - val_loss: 8.3589\n",
      "Epoch 148/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 9.0744 - val_loss: 8.3308\n",
      "Epoch 149/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 9.0699 - val_loss: 8.3217\n",
      "Epoch 150/500\n",
      "4404/4404 [==============================] - 0s 61us/step - loss: 9.1173 - val_loss: 8.3076\n",
      "Epoch 151/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 9.0134 - val_loss: 8.2889\n",
      "Epoch 152/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.8697 - val_loss: 8.2739\n",
      "Epoch 153/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.8319 - val_loss: 8.2554\n",
      "Epoch 154/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4404/4404 [==============================] - 0s 61us/step - loss: 8.9409 - val_loss: 8.2605\n",
      "Epoch 155/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.9562 - val_loss: 8.2415\n",
      "Epoch 156/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.9599 - val_loss: 8.2319\n",
      "Epoch 157/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.9125 - val_loss: 8.2227\n",
      "Epoch 158/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.9656 - val_loss: 8.2212\n",
      "Epoch 159/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 8.8185 - val_loss: 8.2140\n",
      "Epoch 160/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.7976 - val_loss: 8.2094\n",
      "Epoch 161/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.9037 - val_loss: 8.2084\n",
      "Epoch 162/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.9430 - val_loss: 8.2071\n",
      "Epoch 163/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.8423 - val_loss: 8.1905\n",
      "Epoch 164/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.7547 - val_loss: 8.1830\n",
      "Epoch 165/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.7409 - val_loss: 8.1815\n",
      "Epoch 166/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.8872 - val_loss: 8.1776\n",
      "Epoch 167/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 8.8334 - val_loss: 8.1725\n",
      "Epoch 168/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.9055 - val_loss: 8.1676\n",
      "Epoch 169/500\n",
      "4404/4404 [==============================] - 0s 61us/step - loss: 8.8530 - val_loss: 8.1626\n",
      "Epoch 170/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 8.8829 - val_loss: 8.1619\n",
      "Epoch 171/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.8753 - val_loss: 8.1547\n",
      "Epoch 172/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.7625 - val_loss: 8.1510\n",
      "Epoch 173/500\n",
      "4404/4404 [==============================] - 0s 64us/step - loss: 8.7024 - val_loss: 8.1397\n",
      "Epoch 174/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 8.6995 - val_loss: 8.1417\n",
      "Epoch 175/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.7857 - val_loss: 8.1270\n",
      "Epoch 176/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 8.7176 - val_loss: 8.1353\n",
      "Epoch 177/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.7357 - val_loss: 8.1221\n",
      "Epoch 178/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.6558 - val_loss: 8.1169\n",
      "Epoch 179/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.7023 - val_loss: 8.1122\n",
      "Epoch 180/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.7110 - val_loss: 8.1102\n",
      "Epoch 181/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.7359 - val_loss: 8.1000\n",
      "Epoch 182/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.7633 - val_loss: 8.1030\n",
      "Epoch 183/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.7503 - val_loss: 8.0920\n",
      "Epoch 184/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 8.7243 - val_loss: 8.0880\n",
      "Epoch 185/500\n",
      "4404/4404 [==============================] - 0s 61us/step - loss: 8.7690 - val_loss: 8.0910\n",
      "Epoch 186/500\n",
      "4404/4404 [==============================] - 0s 61us/step - loss: 8.6811 - val_loss: 8.0872\n",
      "Epoch 187/500\n",
      "4404/4404 [==============================] - 0s 75us/step - loss: 8.7300 - val_loss: 8.0828\n",
      "Epoch 188/500\n",
      "4404/4404 [==============================] - 0s 61us/step - loss: 8.7443 - val_loss: 8.0785\n",
      "Epoch 189/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.6324 - val_loss: 8.0733\n",
      "Epoch 190/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.7020 - val_loss: 8.0718\n",
      "Epoch 191/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.6550 - val_loss: 8.0667\n",
      "Epoch 192/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.6014 - val_loss: 8.0646\n",
      "Epoch 193/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.6613 - val_loss: 8.0650\n",
      "Epoch 194/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 8.6714 - val_loss: 8.0596\n",
      "Epoch 195/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 8.5823 - val_loss: 8.0540\n",
      "Epoch 196/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 8.5879 - val_loss: 8.0552\n",
      "Epoch 197/500\n",
      "4404/4404 [==============================] - 0s 68us/step - loss: 8.6722 - val_loss: 8.0535\n",
      "Epoch 198/500\n",
      "4404/4404 [==============================] - 0s 73us/step - loss: 8.5863 - val_loss: 8.0455\n",
      "Epoch 199/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.6003 - val_loss: 8.0415\n",
      "Epoch 200/500\n",
      "4404/4404 [==============================] - 0s 62us/step - loss: 8.6437 - val_loss: 8.0411\n",
      "Epoch 201/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.6154 - val_loss: 8.0374\n",
      "Epoch 202/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.5749 - val_loss: 8.0349\n",
      "Epoch 203/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.6075 - val_loss: 8.0291\n",
      "Epoch 204/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.6464 - val_loss: 8.0259\n",
      "Epoch 205/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.5803 - val_loss: 8.0232\n",
      "Epoch 206/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 8.6219 - val_loss: 8.0217\n",
      "Epoch 207/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.6814 - val_loss: 8.0177\n",
      "Epoch 208/500\n",
      "4404/4404 [==============================] - 0s 61us/step - loss: 8.6418 - val_loss: 8.0134\n",
      "Epoch 209/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 8.5524 - val_loss: 8.0103\n",
      "Epoch 210/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 8.6577 - val_loss: 8.0094\n",
      "Epoch 211/500\n",
      "4404/4404 [==============================] - 0s 61us/step - loss: 8.5379 - val_loss: 8.0100\n",
      "Epoch 212/500\n",
      "4404/4404 [==============================] - 0s 61us/step - loss: 8.5602 - val_loss: 8.0065\n",
      "Epoch 213/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.5969 - val_loss: 7.9995\n",
      "Epoch 214/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.5183 - val_loss: 8.0024\n",
      "Epoch 215/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.5858 - val_loss: 7.9951\n",
      "Epoch 216/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.4860 - val_loss: 7.9925\n",
      "Epoch 217/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 8.5994 - val_loss: 7.9891\n",
      "Epoch 218/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.5760 - val_loss: 7.9876\n",
      "Epoch 219/500\n",
      "4404/4404 [==============================] - 0s 67us/step - loss: 8.5528 - val_loss: 7.9822\n",
      "Epoch 220/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.4190 - val_loss: 7.9795\n",
      "Epoch 221/500\n",
      "4404/4404 [==============================] - 0s 61us/step - loss: 8.4817 - val_loss: 7.9787\n",
      "Epoch 222/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.5209 - val_loss: 7.9737\n",
      "Epoch 223/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.5536 - val_loss: 7.9716\n",
      "Epoch 224/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.4749 - val_loss: 7.9688\n",
      "Epoch 225/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.4396 - val_loss: 7.9665\n",
      "Epoch 226/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.5201 - val_loss: 7.9642\n",
      "Epoch 227/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.4799 - val_loss: 7.9604\n",
      "Epoch 228/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.5127 - val_loss: 7.9589\n",
      "Epoch 229/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.4004 - val_loss: 7.9575\n",
      "Epoch 230/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.4314 - val_loss: 7.9537\n",
      "Epoch 231/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.6354 - val_loss: 7.9533\n",
      "Epoch 232/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.5602 - val_loss: 7.9530\n",
      "Epoch 233/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.5004 - val_loss: 7.9501\n",
      "Epoch 234/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.4863 - val_loss: 7.9468\n",
      "Epoch 235/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.5561 - val_loss: 7.9449\n",
      "Epoch 236/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 8.3816 - val_loss: 7.9425\n",
      "Epoch 237/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.4677 - val_loss: 7.9400\n",
      "Epoch 238/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.3424 - val_loss: 7.9380\n",
      "Epoch 239/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.4992 - val_loss: 7.9352\n",
      "Epoch 240/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.4320 - val_loss: 7.9339\n",
      "Epoch 241/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.3912 - val_loss: 7.9316\n",
      "Epoch 242/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.5155 - val_loss: 7.9290\n",
      "Epoch 243/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 8.4337 - val_loss: 7.9267\n",
      "Epoch 244/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 8.4506 - val_loss: 7.9246\n",
      "Epoch 245/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.3651 - val_loss: 7.9210\n",
      "Epoch 246/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.5302 - val_loss: 7.9197\n",
      "Epoch 247/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.5186 - val_loss: 7.9178\n",
      "Epoch 248/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.4156 - val_loss: 7.9149\n",
      "Epoch 249/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.3394 - val_loss: 7.9121\n",
      "Epoch 250/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.3076 - val_loss: 7.9088\n",
      "Epoch 251/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.4331 - val_loss: 7.9078\n",
      "Epoch 252/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.4002 - val_loss: 7.9053\n",
      "Epoch 253/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.3253 - val_loss: 7.9021\n",
      "Epoch 254/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.3482 - val_loss: 7.8993\n",
      "Epoch 255/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.3247 - val_loss: 7.8979\n",
      "Epoch 256/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.3585 - val_loss: 7.8937\n",
      "Epoch 257/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.3608 - val_loss: 7.8923\n",
      "Epoch 258/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.3568 - val_loss: 7.8897\n",
      "Epoch 259/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 8.3718 - val_loss: 7.8884\n",
      "Epoch 260/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.4359 - val_loss: 7.8911\n",
      "Epoch 261/500\n",
      "4404/4404 [==============================] - 0s 63us/step - loss: 8.3810 - val_loss: 7.8872\n",
      "Epoch 262/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.4008 - val_loss: 7.8824\n",
      "Epoch 263/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.3781 - val_loss: 7.8810\n",
      "Epoch 264/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.3965 - val_loss: 7.8792\n",
      "Epoch 265/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.4095 - val_loss: 7.8785\n",
      "Epoch 266/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.3447 - val_loss: 7.8748\n",
      "Epoch 267/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 8.3633 - val_loss: 7.8746\n",
      "Epoch 268/500\n",
      "4404/4404 [==============================] - 0s 65us/step - loss: 8.3459 - val_loss: 7.8706\n",
      "Epoch 269/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 8.2824 - val_loss: 7.8673\n",
      "Epoch 270/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.2466 - val_loss: 7.8653\n",
      "Epoch 271/500\n",
      "4404/4404 [==============================] - 0s 62us/step - loss: 8.2736 - val_loss: 7.8634\n",
      "Epoch 272/500\n",
      "4404/4404 [==============================] - 0s 61us/step - loss: 8.3613 - val_loss: 7.8631\n",
      "Epoch 273/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 8.2703 - val_loss: 7.8595\n",
      "Epoch 274/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.3063 - val_loss: 7.8596\n",
      "Epoch 275/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 8.3239 - val_loss: 7.8559\n",
      "Epoch 276/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.2866 - val_loss: 7.8555\n",
      "Epoch 277/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.3103 - val_loss: 7.8521\n",
      "Epoch 278/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.3099 - val_loss: 7.8509\n",
      "Epoch 279/500\n",
      "4404/4404 [==============================] - 0s 61us/step - loss: 8.2854 - val_loss: 7.8532\n",
      "Epoch 280/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.3637 - val_loss: 7.8533\n",
      "Epoch 281/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.3102 - val_loss: 7.8472\n",
      "Epoch 282/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.3228 - val_loss: 7.8448\n",
      "Epoch 283/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.2906 - val_loss: 7.8431\n",
      "Epoch 284/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.3100 - val_loss: 7.8432\n",
      "Epoch 285/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.2273 - val_loss: 7.8408\n",
      "Epoch 286/500\n",
      "4404/4404 [==============================] - 0s 61us/step - loss: 8.2729 - val_loss: 7.8413\n",
      "Epoch 287/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 8.2902 - val_loss: 7.8372\n",
      "Epoch 288/500\n",
      "4404/4404 [==============================] - 0s 74us/step - loss: 8.2965 - val_loss: 7.8397\n",
      "Epoch 289/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 8.2967 - val_loss: 7.8344\n",
      "Epoch 290/500\n",
      "4404/4404 [==============================] - 0s 66us/step - loss: 8.3212 - val_loss: 7.8374\n",
      "Epoch 291/500\n",
      "4404/4404 [==============================] - 0s 65us/step - loss: 8.2754 - val_loss: 7.8340\n",
      "Epoch 292/500\n",
      "4404/4404 [==============================] - 0s 66us/step - loss: 8.2598 - val_loss: 7.8309\n",
      "Epoch 293/500\n",
      "4404/4404 [==============================] - 0s 64us/step - loss: 8.2879 - val_loss: 7.8295\n",
      "Epoch 294/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 8.3208 - val_loss: 7.8301\n",
      "Epoch 295/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.2813 - val_loss: 7.8274\n",
      "Epoch 296/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.2920 - val_loss: 7.8258\n",
      "Epoch 297/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.2681 - val_loss: 7.8253\n",
      "Epoch 298/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.2622 - val_loss: 7.8221\n",
      "Epoch 299/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.2593 - val_loss: 7.8222\n",
      "Epoch 300/500\n",
      "4404/4404 [==============================] - 0s 61us/step - loss: 8.2978 - val_loss: 7.8182\n",
      "Epoch 301/500\n",
      "4404/4404 [==============================] - 0s 73us/step - loss: 8.3407 - val_loss: 7.8208\n",
      "Epoch 302/500\n",
      "4404/4404 [==============================] - 0s 72us/step - loss: 8.2403 - val_loss: 7.8155\n",
      "Epoch 303/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.2522 - val_loss: 7.8148\n",
      "Epoch 304/500\n",
      "4404/4404 [==============================] - 0s 63us/step - loss: 8.2773 - val_loss: 7.8158\n",
      "Epoch 305/500\n",
      "4404/4404 [==============================] - 0s 80us/step - loss: 8.2510 - val_loss: 7.8142\n",
      "Epoch 306/500\n",
      "4404/4404 [==============================] - 0s 69us/step - loss: 8.2680 - val_loss: 7.8111\n",
      "Epoch 307/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 8.2744 - val_loss: 7.8094\n",
      "Epoch 308/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4404/4404 [==============================] - 0s 71us/step - loss: 8.1942 - val_loss: 7.8076\n",
      "Epoch 309/500\n",
      "4404/4404 [==============================] - 0s 78us/step - loss: 8.2963 - val_loss: 7.8061\n",
      "Epoch 310/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 8.1466 - val_loss: 7.8059\n",
      "Epoch 311/500\n",
      "4404/4404 [==============================] - 0s 70us/step - loss: 8.2231 - val_loss: 7.8046\n",
      "Epoch 312/500\n",
      "4404/4404 [==============================] - 0s 65us/step - loss: 8.2634 - val_loss: 7.8025\n",
      "Epoch 313/500\n",
      "4404/4404 [==============================] - 0s 75us/step - loss: 8.1765 - val_loss: 7.8012\n",
      "Epoch 314/500\n",
      "4404/4404 [==============================] - 0s 77us/step - loss: 8.3039 - val_loss: 7.8003\n",
      "Epoch 315/500\n",
      "4404/4404 [==============================] - 0s 67us/step - loss: 8.1730 - val_loss: 7.8006\n",
      "Epoch 316/500\n",
      "4404/4404 [==============================] - 0s 67us/step - loss: 8.1756 - val_loss: 7.8015\n",
      "Epoch 317/500\n",
      "4404/4404 [==============================] - 0s 73us/step - loss: 8.2053 - val_loss: 7.7966\n",
      "Epoch 318/500\n",
      "4404/4404 [==============================] - 0s 67us/step - loss: 8.1785 - val_loss: 7.7950\n",
      "Epoch 319/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.2092 - val_loss: 7.7939\n",
      "Epoch 320/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.2954 - val_loss: 7.7942\n",
      "Epoch 321/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.1972 - val_loss: 7.7956\n",
      "Epoch 322/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.2962 - val_loss: 7.7922\n",
      "Epoch 323/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.2791 - val_loss: 7.7906\n",
      "Epoch 324/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.2896 - val_loss: 7.7893\n",
      "Epoch 325/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 8.1675 - val_loss: 7.7885\n",
      "Epoch 326/500\n",
      "4404/4404 [==============================] - 0s 62us/step - loss: 8.2365 - val_loss: 7.7877\n",
      "Epoch 327/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.2834 - val_loss: 7.7867\n",
      "Epoch 328/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.2361 - val_loss: 7.7851\n",
      "Epoch 329/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 8.2550 - val_loss: 7.7863\n",
      "Epoch 330/500\n",
      "4404/4404 [==============================] - 0s 71us/step - loss: 8.1503 - val_loss: 7.7849\n",
      "Epoch 331/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.1719 - val_loss: 7.7809\n",
      "Epoch 332/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.2272 - val_loss: 7.7798\n",
      "Epoch 333/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 8.1742 - val_loss: 7.7782\n",
      "Epoch 334/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.1553 - val_loss: 7.7779\n",
      "Epoch 335/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 8.1278 - val_loss: 7.7844\n",
      "Epoch 336/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 8.1792 - val_loss: 7.7744\n",
      "Epoch 337/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.2327 - val_loss: 7.7739\n",
      "Epoch 338/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.1926 - val_loss: 7.7733\n",
      "Epoch 339/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.1136 - val_loss: 7.7732\n",
      "Epoch 340/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.1998 - val_loss: 7.7772\n",
      "Epoch 341/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.2081 - val_loss: 7.7713\n",
      "Epoch 342/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.2282 - val_loss: 7.7729\n",
      "Epoch 343/500\n",
      "4404/4404 [==============================] - 0s 51us/step - loss: 8.2326 - val_loss: 7.7708\n",
      "Epoch 344/500\n",
      "4404/4404 [==============================] - 0s 49us/step - loss: 8.1715 - val_loss: 7.7682\n",
      "Epoch 345/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.1750 - val_loss: 7.7684\n",
      "Epoch 346/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.2149 - val_loss: 7.7720\n",
      "Epoch 347/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.2338 - val_loss: 7.7681\n",
      "Epoch 348/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.1617 - val_loss: 7.7646\n",
      "Epoch 349/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.1829 - val_loss: 7.7629\n",
      "Epoch 350/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.1150 - val_loss: 7.7625\n",
      "Epoch 351/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.1795 - val_loss: 7.7622\n",
      "Epoch 352/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.1851 - val_loss: 7.7641\n",
      "Epoch 353/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 8.1515 - val_loss: 7.7591\n",
      "Epoch 354/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.1695 - val_loss: 7.7579\n",
      "Epoch 355/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.1401 - val_loss: 7.7569\n",
      "Epoch 356/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.0981 - val_loss: 7.7560\n",
      "Epoch 357/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.1566 - val_loss: 7.7597\n",
      "Epoch 358/500\n",
      "4404/4404 [==============================] - 0s 71us/step - loss: 8.1765 - val_loss: 7.7553\n",
      "Epoch 359/500\n",
      "4404/4404 [==============================] - 0s 65us/step - loss: 8.1444 - val_loss: 7.7539\n",
      "Epoch 360/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.1439 - val_loss: 7.7546\n",
      "Epoch 361/500\n",
      "4404/4404 [==============================] - 0s 65us/step - loss: 8.0685 - val_loss: 7.7522\n",
      "Epoch 362/500\n",
      "4404/4404 [==============================] - 0s 65us/step - loss: 8.1688 - val_loss: 7.7527\n",
      "Epoch 363/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.0691 - val_loss: 7.7525\n",
      "Epoch 364/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.1338 - val_loss: 7.7512\n",
      "Epoch 365/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 8.1409 - val_loss: 7.7505\n",
      "Epoch 366/500\n",
      "4404/4404 [==============================] - 0s 68us/step - loss: 8.1462 - val_loss: 7.7496\n",
      "Epoch 367/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.1757 - val_loss: 7.7481\n",
      "Epoch 368/500\n",
      "4404/4404 [==============================] - 1s 129us/step - loss: 8.1869 - val_loss: 7.7475\n",
      "Epoch 369/500\n",
      "4404/4404 [==============================] - 0s 63us/step - loss: 8.1484 - val_loss: 7.7480\n",
      "Epoch 370/500\n",
      "4404/4404 [==============================] - 0s 61us/step - loss: 8.1918 - val_loss: 7.7444\n",
      "Epoch 371/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.1135 - val_loss: 7.7433\n",
      "Epoch 372/500\n",
      "4404/4404 [==============================] - 0s 70us/step - loss: 8.2053 - val_loss: 7.7431\n",
      "Epoch 373/500\n",
      "4404/4404 [==============================] - 0s 67us/step - loss: 8.1588 - val_loss: 7.7439\n",
      "Epoch 374/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.0914 - val_loss: 7.7417\n",
      "Epoch 375/500\n",
      "4404/4404 [==============================] - 0s 64us/step - loss: 8.1561 - val_loss: 7.7404\n",
      "Epoch 376/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 8.1295 - val_loss: 7.7393\n",
      "Epoch 377/500\n",
      "4404/4404 [==============================] - 0s 67us/step - loss: 8.1265 - val_loss: 7.7385\n",
      "Epoch 378/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.1476 - val_loss: 7.7379\n",
      "Epoch 379/500\n",
      "4404/4404 [==============================] - 0s 63us/step - loss: 8.1350 - val_loss: 7.7366\n",
      "Epoch 380/500\n",
      "4404/4404 [==============================] - 0s 86us/step - loss: 8.0964 - val_loss: 7.7362\n",
      "Epoch 381/500\n",
      "4404/4404 [==============================] - 0s 95us/step - loss: 8.1433 - val_loss: 7.7337\n",
      "Epoch 382/500\n",
      "4404/4404 [==============================] - 0s 96us/step - loss: 8.0926 - val_loss: 7.7343\n",
      "Epoch 383/500\n",
      "4404/4404 [==============================] - 0s 93us/step - loss: 8.1394 - val_loss: 7.7335\n",
      "Epoch 384/500\n",
      "4404/4404 [==============================] - 0s 93us/step - loss: 8.0953 - val_loss: 7.7329\n",
      "Epoch 385/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4404/4404 [==============================] - 0s 92us/step - loss: 8.1086 - val_loss: 7.7320\n",
      "Epoch 386/500\n",
      "4404/4404 [==============================] - 0s 96us/step - loss: 8.1383 - val_loss: 7.7310\n",
      "Epoch 387/500\n",
      "4404/4404 [==============================] - 0s 96us/step - loss: 8.0942 - val_loss: 7.7292\n",
      "Epoch 388/500\n",
      "4404/4404 [==============================] - 0s 91us/step - loss: 8.0663 - val_loss: 7.7293\n",
      "Epoch 389/500\n",
      "4404/4404 [==============================] - 1s 301us/step - loss: 8.1433 - val_loss: 7.7288\n",
      "Epoch 390/500\n",
      "4404/4404 [==============================] - 2s 405us/step - loss: 8.1284 - val_loss: 7.7278\n",
      "Epoch 391/500\n",
      "4404/4404 [==============================] - 1s 319us/step - loss: 8.1915 - val_loss: 7.7384\n",
      "Epoch 392/500\n",
      "4404/4404 [==============================] - 1s 198us/step - loss: 8.0728 - val_loss: 7.7255\n",
      "Epoch 393/500\n",
      "4404/4404 [==============================] - 0s 98us/step - loss: 8.0502 - val_loss: 7.7254\n",
      "Epoch 394/500\n",
      "4404/4404 [==============================] - 1s 220us/step - loss: 8.1360 - val_loss: 7.7249\n",
      "Epoch 395/500\n",
      "4404/4404 [==============================] - 1s 330us/step - loss: 8.1101 - val_loss: 7.7289\n",
      "Epoch 396/500\n",
      "4404/4404 [==============================] - 1s 295us/step - loss: 8.0798 - val_loss: 7.7209\n",
      "Epoch 397/500\n",
      "4404/4404 [==============================] - 1s 253us/step - loss: 8.1695 - val_loss: 7.7199\n",
      "Epoch 398/500\n",
      "4404/4404 [==============================] - 0s 109us/step - loss: 8.1169 - val_loss: 7.7195\n",
      "Epoch 399/500\n",
      "4404/4404 [==============================] - 1s 123us/step - loss: 8.1279 - val_loss: 7.7217\n",
      "Epoch 400/500\n",
      "4404/4404 [==============================] - 0s 104us/step - loss: 8.0736 - val_loss: 7.7176\n",
      "Epoch 401/500\n",
      "4404/4404 [==============================] - 0s 92us/step - loss: 8.1518 - val_loss: 7.7207\n",
      "Epoch 402/500\n",
      "4404/4404 [==============================] - 0s 81us/step - loss: 8.0830 - val_loss: 7.7162\n",
      "Epoch 403/500\n",
      "4404/4404 [==============================] - 0s 68us/step - loss: 8.0405 - val_loss: 7.7161\n",
      "Epoch 404/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 8.0999 - val_loss: 7.7175\n",
      "Epoch 405/500\n",
      "4404/4404 [==============================] - 0s 71us/step - loss: 8.0936 - val_loss: 7.7140\n",
      "Epoch 406/500\n",
      "4404/4404 [==============================] - 0s 69us/step - loss: 8.1151 - val_loss: 7.7140\n",
      "Epoch 407/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.0882 - val_loss: 7.7135\n",
      "Epoch 408/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.0688 - val_loss: 7.7116\n",
      "Epoch 409/500\n",
      "4404/4404 [==============================] - 0s 60us/step - loss: 8.0347 - val_loss: 7.7118\n",
      "Epoch 410/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.0873 - val_loss: 7.7102\n",
      "Epoch 411/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.1354 - val_loss: 7.7117\n",
      "Epoch 412/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.0883 - val_loss: 7.7088\n",
      "Epoch 413/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.0673 - val_loss: 7.7096\n",
      "Epoch 414/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.0432 - val_loss: 7.7061\n",
      "Epoch 415/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.0902 - val_loss: 7.7059\n",
      "Epoch 416/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 8.1008 - val_loss: 7.7043\n",
      "Epoch 417/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.0396 - val_loss: 7.7049\n",
      "Epoch 418/500\n",
      "4404/4404 [==============================] - 0s 51us/step - loss: 8.0523 - val_loss: 7.7027\n",
      "Epoch 419/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.0981 - val_loss: 7.7020\n",
      "Epoch 420/500\n",
      "4404/4404 [==============================] - 0s 51us/step - loss: 8.1416 - val_loss: 7.7014\n",
      "Epoch 421/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.0670 - val_loss: 7.7015\n",
      "Epoch 422/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.0930 - val_loss: 7.7010\n",
      "Epoch 423/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 8.0754 - val_loss: 7.7018\n",
      "Epoch 424/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.0741 - val_loss: 7.7014\n",
      "Epoch 425/500\n",
      "4404/4404 [==============================] - 0s 66us/step - loss: 8.1094 - val_loss: 7.7004\n",
      "Epoch 426/500\n",
      "4404/4404 [==============================] - 0s 48us/step - loss: 8.0621 - val_loss: 7.6985\n",
      "Epoch 427/500\n",
      "4404/4404 [==============================] - 0s 49us/step - loss: 8.0380 - val_loss: 7.6977\n",
      "Epoch 428/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 8.0843 - val_loss: 7.6964\n",
      "Epoch 429/500\n",
      "4404/4404 [==============================] - 0s 62us/step - loss: 8.0572 - val_loss: 7.6954\n",
      "Epoch 430/500\n",
      "4404/4404 [==============================] - 0s 68us/step - loss: 8.0712 - val_loss: 7.6942\n",
      "Epoch 431/500\n",
      "4404/4404 [==============================] - 0s 68us/step - loss: 8.0693 - val_loss: 7.6924\n",
      "Epoch 432/500\n",
      "4404/4404 [==============================] - 0s 69us/step - loss: 8.0543 - val_loss: 7.6927\n",
      "Epoch 433/500\n",
      "4404/4404 [==============================] - 0s 61us/step - loss: 8.0598 - val_loss: 7.6922\n",
      "Epoch 434/500\n",
      "4404/4404 [==============================] - 0s 56us/step - loss: 8.1071 - val_loss: 7.6910\n",
      "Epoch 435/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.0042 - val_loss: 7.6892\n",
      "Epoch 436/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.0900 - val_loss: 7.6879\n",
      "Epoch 437/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 8.0719 - val_loss: 7.6869\n",
      "Epoch 438/500\n",
      "4404/4404 [==============================] - 0s 51us/step - loss: 8.0587 - val_loss: 7.6878\n",
      "Epoch 439/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 8.0448 - val_loss: 7.6878\n",
      "Epoch 440/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.0490 - val_loss: 7.6850\n",
      "Epoch 441/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 8.0219 - val_loss: 7.6845\n",
      "Epoch 442/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 8.0200 - val_loss: 7.6825\n",
      "Epoch 443/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 8.0521 - val_loss: 7.6817\n",
      "Epoch 444/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 8.0003 - val_loss: 7.6797\n",
      "Epoch 445/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 8.0298 - val_loss: 7.6805\n",
      "Epoch 446/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 7.9948 - val_loss: 7.6788\n",
      "Epoch 447/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 8.0593 - val_loss: 7.6800\n",
      "Epoch 448/500\n",
      "4404/4404 [==============================] - 0s 50us/step - loss: 8.1199 - val_loss: 7.6792\n",
      "Epoch 449/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 8.0598 - val_loss: 7.6773\n",
      "Epoch 450/500\n",
      "4404/4404 [==============================] - 0s 50us/step - loss: 8.0294 - val_loss: 7.6801\n",
      "Epoch 451/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 7.9916 - val_loss: 7.6814\n",
      "Epoch 452/500\n",
      "4404/4404 [==============================] - 0s 51us/step - loss: 7.9857 - val_loss: 7.6759\n",
      "Epoch 453/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 8.0393 - val_loss: 7.6758\n",
      "Epoch 454/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 8.0289 - val_loss: 7.6764\n",
      "Epoch 455/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 8.0321 - val_loss: 7.6740\n",
      "Epoch 456/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.1096 - val_loss: 7.6735\n",
      "Epoch 457/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 8.0323 - val_loss: 7.6723\n",
      "Epoch 458/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 7.9753 - val_loss: 7.6711\n",
      "Epoch 459/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.0378 - val_loss: 7.6714\n",
      "Epoch 460/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 8.0067 - val_loss: 7.6708\n",
      "Epoch 461/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 8.0397 - val_loss: 7.6710\n",
      "Epoch 462/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4404/4404 [==============================] - 0s 51us/step - loss: 8.0205 - val_loss: 7.6715\n",
      "Epoch 463/500\n",
      "4404/4404 [==============================] - 0s 50us/step - loss: 8.0284 - val_loss: 7.6687\n",
      "Epoch 464/500\n",
      "4404/4404 [==============================] - 0s 51us/step - loss: 7.9948 - val_loss: 7.6685\n",
      "Epoch 465/500\n",
      "4404/4404 [==============================] - 0s 50us/step - loss: 8.0737 - val_loss: 7.6709\n",
      "Epoch 466/500\n",
      "4404/4404 [==============================] - 0s 50us/step - loss: 7.9693 - val_loss: 7.6687\n",
      "Epoch 467/500\n",
      "4404/4404 [==============================] - 0s 51us/step - loss: 8.0283 - val_loss: 7.6686\n",
      "Epoch 468/500\n",
      "4404/4404 [==============================] - 0s 50us/step - loss: 8.0299 - val_loss: 7.6683\n",
      "Epoch 469/500\n",
      "4404/4404 [==============================] - 0s 50us/step - loss: 7.9909 - val_loss: 7.6656\n",
      "Epoch 470/500\n",
      "4404/4404 [==============================] - 0s 51us/step - loss: 8.0181 - val_loss: 7.6667\n",
      "Epoch 471/500\n",
      "4404/4404 [==============================] - 0s 50us/step - loss: 8.0402 - val_loss: 7.6671\n",
      "Epoch 472/500\n",
      "4404/4404 [==============================] - 0s 50us/step - loss: 7.9972 - val_loss: 7.6642\n",
      "Epoch 473/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 8.0500 - val_loss: 7.6646\n",
      "Epoch 474/500\n",
      "4404/4404 [==============================] - 0s 49us/step - loss: 8.0805 - val_loss: 7.6634\n",
      "Epoch 475/500\n",
      "4404/4404 [==============================] - 0s 50us/step - loss: 8.0541 - val_loss: 7.6646\n",
      "Epoch 476/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 7.9894 - val_loss: 7.6602\n",
      "Epoch 477/500\n",
      "4404/4404 [==============================] - 0s 50us/step - loss: 8.0058 - val_loss: 7.6607\n",
      "Epoch 478/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 7.9909 - val_loss: 7.6587\n",
      "Epoch 479/500\n",
      "4404/4404 [==============================] - 0s 58us/step - loss: 7.9303 - val_loss: 7.6614\n",
      "Epoch 480/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 7.9797 - val_loss: 7.6567\n",
      "Epoch 481/500\n",
      "4404/4404 [==============================] - 0s 51us/step - loss: 8.0262 - val_loss: 7.6572\n",
      "Epoch 482/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 8.0318 - val_loss: 7.6576\n",
      "Epoch 483/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 8.0344 - val_loss: 7.6570\n",
      "Epoch 484/500\n",
      "4404/4404 [==============================] - 0s 57us/step - loss: 7.9503 - val_loss: 7.6561\n",
      "Epoch 485/500\n",
      "4404/4404 [==============================] - 0s 51us/step - loss: 8.0397 - val_loss: 7.6551\n",
      "Epoch 486/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 7.9997 - val_loss: 7.6542\n",
      "Epoch 487/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 7.9536 - val_loss: 7.6532\n",
      "Epoch 488/500\n",
      "4404/4404 [==============================] - 0s 55us/step - loss: 7.9594 - val_loss: 7.6538\n",
      "Epoch 489/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 7.9767 - val_loss: 7.6530\n",
      "Epoch 490/500\n",
      "4404/4404 [==============================] - 0s 52us/step - loss: 7.9165 - val_loss: 7.6512\n",
      "Epoch 491/500\n",
      "4404/4404 [==============================] - 0s 53us/step - loss: 7.9782 - val_loss: 7.6500\n",
      "Epoch 492/500\n",
      "4404/4404 [==============================] - 0s 64us/step - loss: 7.9619 - val_loss: 7.6494\n",
      "Epoch 493/500\n",
      "4404/4404 [==============================] - 0s 54us/step - loss: 7.9734 - val_loss: 7.6492\n",
      "Epoch 494/500\n",
      "4404/4404 [==============================] - 0s 65us/step - loss: 7.9889 - val_loss: 7.6487\n",
      "Epoch 495/500\n",
      "4404/4404 [==============================] - 0s 64us/step - loss: 8.0954 - val_loss: 7.6493\n",
      "Epoch 496/500\n",
      "4404/4404 [==============================] - 0s 71us/step - loss: 8.0418 - val_loss: 7.6555\n",
      "Epoch 497/500\n",
      "4404/4404 [==============================] - 0s 64us/step - loss: 8.0156 - val_loss: 7.6505\n",
      "Epoch 498/500\n",
      "4404/4404 [==============================] - 0s 63us/step - loss: 8.0040 - val_loss: 7.6475\n",
      "Epoch 499/500\n",
      "4404/4404 [==============================] - 0s 59us/step - loss: 7.9844 - val_loss: 7.6487\n",
      "Epoch 500/500\n",
      "4404/4404 [==============================] - 0s 50us/step - loss: 7.9564 - val_loss: 7.6489\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=500, shuffle=True, callbacks=[es], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(scaler, open(\"scaler.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(y_test[\"total_points\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'Actual': y_true.flatten(), 'Predicted': y_pred.flatten()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInRange(actual, predicted, lower_bound=0, upper_bound=1):\n",
    "    return ((actual - predicted) >= lower_bound) and ((actual - predicted) <= upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[\"in_range\"] = pred_df.apply(lambda row: getInRange(row[\"Actual\"], row[\"Predicted\"], 0, 1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In range: 0.09\n"
     ]
    }
   ],
   "source": [
    "print(\"In range: {0:.2f}\".format(pred_df[pred_df[\"in_range\"] == True].shape[0]/pred_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>in_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.928530</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>4.899164</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>4.645432</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.557984</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>4.890161</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.0</td>\n",
       "      <td>4.943027</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>4.988772</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.0</td>\n",
       "      <td>5.059024</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>4.768284</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.0</td>\n",
       "      <td>4.989361</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.972061</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9.0</td>\n",
       "      <td>4.731083</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.977622</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.513292</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.963515</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9.0</td>\n",
       "      <td>4.982170</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.0</td>\n",
       "      <td>5.024218</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.244057</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.759429</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.0</td>\n",
       "      <td>4.998136</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.948555</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8.0</td>\n",
       "      <td>4.951324</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.0</td>\n",
       "      <td>4.905863</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.026281</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9.0</td>\n",
       "      <td>5.004248</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual  Predicted  in_range\n",
       "0      4.0   4.928530     False\n",
       "1      7.0   4.899164     False\n",
       "2      8.0   4.645432     False\n",
       "3      5.0   3.557984     False\n",
       "4      8.0   4.890161     False\n",
       "5      7.0   4.943027     False\n",
       "6      6.0   4.988772     False\n",
       "7      9.0   5.059024     False\n",
       "8      9.0   4.768284     False\n",
       "9      8.0   4.989361     False\n",
       "10     1.0   4.972061     False\n",
       "11     9.0   4.731083     False\n",
       "12     1.0   4.977622     False\n",
       "13     2.0   2.513292     False\n",
       "14     1.0   4.963515     False\n",
       "15     9.0   4.982170     False\n",
       "16     8.0   5.024218     False\n",
       "17     5.0   3.244057     False\n",
       "18     0.0   2.759429     False\n",
       "19     7.0   4.998136     False\n",
       "20     4.0   4.948555     False\n",
       "21     8.0   4.951324     False\n",
       "22     7.0   4.905863     False\n",
       "23     4.0   5.026281     False\n",
       "24     9.0   5.004248     False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pred_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df.plot(kind='bar',figsize=(10,8))\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 2.324860680817908\n",
      "R2 score: 0.12415109303884064\n",
      "Mean Squared Error: 7.148633711307478\n",
      "Root Mean Squared Error: 2.673692897717963\n"
     ]
    }
   ],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('R2 score:', metrics.r2_score(y_test, y_pred))\n",
    "\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '{}-model.h5'.format(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_dir = os.path.join(os.getcwd(), \"..\", \"model\", season, str(gw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(gw_dir):\n",
    "    os.makedirs(gw_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(gw_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
