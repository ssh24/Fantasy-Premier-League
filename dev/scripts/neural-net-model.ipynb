{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(os.getcwd(), \"..\", \"data\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final-preprocessed-data.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70773, 69)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>GW</th>\n",
       "      <th>at_home</th>\n",
       "      <th>player_id</th>\n",
       "      <th>opponent_id</th>\n",
       "      <th>champion_ls</th>\n",
       "      <th>top5_ls</th>\n",
       "      <th>bottom5_ls</th>\n",
       "      <th>promoted_ts</th>\n",
       "      <th>assists_pgw</th>\n",
       "      <th>...</th>\n",
       "      <th>threat_pgw</th>\n",
       "      <th>threat_rlf</th>\n",
       "      <th>threat_rsf</th>\n",
       "      <th>total_points_pgw</th>\n",
       "      <th>total_points_rlf</th>\n",
       "      <th>total_points_rsf</th>\n",
       "      <th>yellow_cards_pgw</th>\n",
       "      <th>yellow_cards_rlf</th>\n",
       "      <th>yellow_cards_rsf</th>\n",
       "      <th>total_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  GW  at_home  player_id  opponent_id  champion_ls  top5_ls  \\\n",
       "0    2016   1        0          1            4            0        0   \n",
       "1    2016   1        1          2           17            0        1   \n",
       "2    2016   1        1          4            9            0        0   \n",
       "3    2016   1        0          6           13            0        0   \n",
       "4    2016   1        1          7           20            0        0   \n",
       "\n",
       "   bottom5_ls  promoted_ts  assists_pgw  ...  threat_pgw  threat_rlf  \\\n",
       "0           0            0          0.0  ...         0.0         0.0   \n",
       "1           0            0          0.0  ...         0.0         0.0   \n",
       "2           0            0          0.0  ...         0.0         0.0   \n",
       "3           0            0          0.0  ...         0.0         0.0   \n",
       "4           0            0          0.0  ...         0.0         0.0   \n",
       "\n",
       "   threat_rsf  total_points_pgw  total_points_rlf  total_points_rsf  \\\n",
       "0         0.0               0.0               0.0               0.0   \n",
       "1         0.0               0.0               0.0               0.0   \n",
       "2         0.0               0.0               0.0               0.0   \n",
       "3         0.0               0.0               0.0               0.0   \n",
       "4         0.0               0.0               0.0               0.0   \n",
       "\n",
       "   yellow_cards_pgw  yellow_cards_rlf  yellow_cards_rsf  total_points  \n",
       "0               0.0                 0                 0           0.0  \n",
       "1               0.0                 0                 0           1.0  \n",
       "2               0.0                 0                 0           2.0  \n",
       "3               0.0                 0                 0           0.0  \n",
       "4               0.0                 0                 0           0.0  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "season                    int64\n",
       "GW                        int64\n",
       "at_home                   int64\n",
       "player_id                 int64\n",
       "opponent_id               int64\n",
       "champion_ls               int64\n",
       "top5_ls                   int64\n",
       "bottom5_ls                int64\n",
       "promoted_ts               int64\n",
       "assists_pgw             float64\n",
       "assists_rlf               int64\n",
       "assists_rsf               int64\n",
       "bonus_pgw               float64\n",
       "bonus_rlf                 int64\n",
       "bonus_rsf                 int64\n",
       "bps_pgw                 float64\n",
       "bps_rlf                   int64\n",
       "bps_rsf                   int64\n",
       "clean_sheets_pgw        float64\n",
       "clean_sheets_rlf          int64\n",
       "clean_sheets_rsf          int64\n",
       "creativity_pgw          float64\n",
       "creativity_rlf          float64\n",
       "creativity_rsf          float64\n",
       "goals_conceded_pgw      float64\n",
       "goals_conceded_rlf        int64\n",
       "goals_conceded_rsf        int64\n",
       "goals_scored_pgw        float64\n",
       "goals_scored_rlf          int64\n",
       "goals_scored_rsf          int64\n",
       "                         ...   \n",
       "opponent_form           float64\n",
       "own_goals_pgw           float64\n",
       "own_goals_rlf             int64\n",
       "own_goals_rsf             int64\n",
       "penalties_missed_pgw    float64\n",
       "penalties_missed_rlf      int64\n",
       "penalties_missed_rsf      int64\n",
       "penalties_saved_pgw     float64\n",
       "penalties_saved_rlf       int64\n",
       "penalties_saved_rsf       int64\n",
       "player_form             float64\n",
       "red_cards_pgw           float64\n",
       "red_cards_rlf             int64\n",
       "red_cards_rsf             int64\n",
       "result_pgw              float64\n",
       "result_rlf              float64\n",
       "result_rsf              float64\n",
       "saves_pgw               float64\n",
       "saves_rlf                 int64\n",
       "saves_rsf                 int64\n",
       "threat_pgw              float64\n",
       "threat_rlf              float64\n",
       "threat_rsf              float64\n",
       "total_points_pgw        float64\n",
       "total_points_rlf        float64\n",
       "total_points_rsf        float64\n",
       "yellow_cards_pgw        float64\n",
       "yellow_cards_rlf          int64\n",
       "yellow_cards_rsf          int64\n",
       "total_points            float64\n",
       "Length: 69, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>season</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>2017.061464</td>\n",
       "      <td>0.902534</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>2016.00000</td>\n",
       "      <td>2017.00000</td>\n",
       "      <td>2018.000000</td>\n",
       "      <td>2019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GW</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>19.507453</td>\n",
       "      <td>11.347524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at_home</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.499654</td>\n",
       "      <td>0.500003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>player_id</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>486.980656</td>\n",
       "      <td>295.603861</td>\n",
       "      <td>1.0</td>\n",
       "      <td>239.00000</td>\n",
       "      <td>477.00000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>1211.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opponent_id</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>12.038334</td>\n",
       "      <td>7.271718</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>champion_ls</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.050358</td>\n",
       "      <td>0.218685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top5_ls</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.250745</td>\n",
       "      <td>0.433445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bottom5_ls</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.249191</td>\n",
       "      <td>0.432548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promoted_ts</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.149676</td>\n",
       "      <td>0.356756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assists_pgw</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.038956</td>\n",
       "      <td>0.209615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assists_rlf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.030365</td>\n",
       "      <td>0.186884</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assists_rsf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.020022</td>\n",
       "      <td>0.151698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonus_pgw</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.103034</td>\n",
       "      <td>0.479364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonus_rlf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.079211</td>\n",
       "      <td>0.422478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonus_rsf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.052464</td>\n",
       "      <td>0.345748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bps_pgw</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>6.046571</td>\n",
       "      <td>9.847342</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bps_rlf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>4.657058</td>\n",
       "      <td>9.036882</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bps_rsf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>3.118265</td>\n",
       "      <td>7.741569</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clean_sheets_pgw</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.103556</td>\n",
       "      <td>0.304686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clean_sheets_rlf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.077417</td>\n",
       "      <td>0.267253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clean_sheets_rsf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.050952</td>\n",
       "      <td>0.219901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>creativity_pgw</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>4.839635</td>\n",
       "      <td>10.935745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>170.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>creativity_rlf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>3.695414</td>\n",
       "      <td>9.780400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>134.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>creativity_rsf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>2.407374</td>\n",
       "      <td>8.074468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>118.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goals_conceded_pgw</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.491897</td>\n",
       "      <td>0.961009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goals_conceded_rlf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.383239</td>\n",
       "      <td>0.879442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goals_conceded_rsf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.256270</td>\n",
       "      <td>0.747015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goals_scored_pgw</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.042686</td>\n",
       "      <td>0.226602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goals_scored_rlf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.033134</td>\n",
       "      <td>0.200297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goals_scored_rsf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.021986</td>\n",
       "      <td>0.164013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opponent_form</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.502360</td>\n",
       "      <td>0.239758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37500</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>own_goals_pgw</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.037936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>own_goals_rlf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.032752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>own_goals_rsf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.027096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penalties_missed_pgw</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>0.030754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penalties_missed_rlf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.027612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penalties_missed_rsf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.023166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penalties_saved_pgw</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.028119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penalties_saved_rlf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.026304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penalties_saved_rsf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.022549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>player_form</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.125487</td>\n",
       "      <td>0.100509</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05481</td>\n",
       "      <td>0.07326</td>\n",
       "      <td>0.169729</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>red_cards_pgw</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>0.043147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>red_cards_rlf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>0.038306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>red_cards_rsf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.032536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>result_pgw</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>-0.007263</td>\n",
       "      <td>0.862848</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>result_rlf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>-0.003391</td>\n",
       "      <td>0.718143</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>result_rsf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>-0.003109</td>\n",
       "      <td>0.544900</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saves_pgw</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.094612</td>\n",
       "      <td>0.629271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saves_rlf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.074054</td>\n",
       "      <td>0.559278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saves_rsf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.051432</td>\n",
       "      <td>0.469116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threat_pgw</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>5.268944</td>\n",
       "      <td>13.406070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threat_rlf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>4.009142</td>\n",
       "      <td>11.912672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threat_rsf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>2.635680</td>\n",
       "      <td>9.863312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_points_pgw</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>1.324460</td>\n",
       "      <td>2.475975</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_points_rlf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>1.013564</td>\n",
       "      <td>2.244730</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_points_rsf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.669351</td>\n",
       "      <td>1.892498</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yellow_cards_pgw</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.053905</td>\n",
       "      <td>0.225831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yellow_cards_rlf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.041343</td>\n",
       "      <td>0.199085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yellow_cards_rsf</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.163240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_points</th>\n",
       "      <td>70773.0</td>\n",
       "      <td>1.302000</td>\n",
       "      <td>2.250155</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count         mean         std     min         25%  \\\n",
       "season                70773.0  2017.061464    0.902534  2016.0  2016.00000   \n",
       "GW                    70773.0    19.507453   11.347524     1.0     9.00000   \n",
       "at_home               70773.0     0.499654    0.500003     0.0     0.00000   \n",
       "player_id             70773.0   486.980656  295.603861     1.0   239.00000   \n",
       "opponent_id           70773.0    12.038334    7.271718     1.0     5.00000   \n",
       "champion_ls           70773.0     0.050358    0.218685     0.0     0.00000   \n",
       "top5_ls               70773.0     0.250745    0.433445     0.0     0.00000   \n",
       "bottom5_ls            70773.0     0.249191    0.432548     0.0     0.00000   \n",
       "promoted_ts           70773.0     0.149676    0.356756     0.0     0.00000   \n",
       "assists_pgw           70773.0     0.038956    0.209615     0.0     0.00000   \n",
       "assists_rlf           70773.0     0.030365    0.186884     0.0     0.00000   \n",
       "assists_rsf           70773.0     0.020022    0.151698     0.0     0.00000   \n",
       "bonus_pgw             70773.0     0.103034    0.479364     0.0     0.00000   \n",
       "bonus_rlf             70773.0     0.079211    0.422478     0.0     0.00000   \n",
       "bonus_rsf             70773.0     0.052464    0.345748     0.0     0.00000   \n",
       "bps_pgw               70773.0     6.046571    9.847342   -19.0     0.00000   \n",
       "bps_rlf               70773.0     4.657058    9.036882   -19.0     0.00000   \n",
       "bps_rsf               70773.0     3.118265    7.741569   -19.0     0.00000   \n",
       "clean_sheets_pgw      70773.0     0.103556    0.304686     0.0     0.00000   \n",
       "clean_sheets_rlf      70773.0     0.077417    0.267253     0.0     0.00000   \n",
       "clean_sheets_rsf      70773.0     0.050952    0.219901     0.0     0.00000   \n",
       "creativity_pgw        70773.0     4.839635   10.935745     0.0     0.00000   \n",
       "creativity_rlf        70773.0     3.695414    9.780400     0.0     0.00000   \n",
       "creativity_rsf        70773.0     2.407374    8.074468     0.0     0.00000   \n",
       "goals_conceded_pgw    70773.0     0.491897    0.961009     0.0     0.00000   \n",
       "goals_conceded_rlf    70773.0     0.383239    0.879442     0.0     0.00000   \n",
       "goals_conceded_rsf    70773.0     0.256270    0.747015     0.0     0.00000   \n",
       "goals_scored_pgw      70773.0     0.042686    0.226602     0.0     0.00000   \n",
       "goals_scored_rlf      70773.0     0.033134    0.200297     0.0     0.00000   \n",
       "goals_scored_rsf      70773.0     0.021986    0.164013     0.0     0.00000   \n",
       "...                       ...          ...         ...     ...         ...   \n",
       "opponent_form         70773.0     0.502360    0.239758     0.0     0.37500   \n",
       "own_goals_pgw         70773.0     0.001441    0.037936     0.0     0.00000   \n",
       "own_goals_rlf         70773.0     0.001074    0.032752     0.0     0.00000   \n",
       "own_goals_rsf         70773.0     0.000735    0.027096     0.0     0.00000   \n",
       "penalties_missed_pgw  70773.0     0.000947    0.030754     0.0     0.00000   \n",
       "penalties_missed_rlf  70773.0     0.000763    0.027612     0.0     0.00000   \n",
       "penalties_missed_rsf  70773.0     0.000537    0.023166     0.0     0.00000   \n",
       "penalties_saved_pgw   70773.0     0.000763    0.028119     0.0     0.00000   \n",
       "penalties_saved_rlf   70773.0     0.000664    0.026304     0.0     0.00000   \n",
       "penalties_saved_rsf   70773.0     0.000480    0.022549     0.0     0.00000   \n",
       "player_form           70773.0     0.125487    0.100509     0.0     0.05481   \n",
       "red_cards_pgw         70773.0     0.001865    0.043147     0.0     0.00000   \n",
       "red_cards_rlf         70773.0     0.001469    0.038306     0.0     0.00000   \n",
       "red_cards_rsf         70773.0     0.001060    0.032536     0.0     0.00000   \n",
       "result_pgw            70773.0    -0.007263    0.862848    -1.0    -1.00000   \n",
       "result_rlf            70773.0    -0.003391    0.718143    -1.0    -1.00000   \n",
       "result_rsf            70773.0    -0.003109    0.544900    -1.0     0.00000   \n",
       "saves_pgw             70773.0     0.094612    0.629271     0.0     0.00000   \n",
       "saves_rlf             70773.0     0.074054    0.559278     0.0     0.00000   \n",
       "saves_rsf             70773.0     0.051432    0.469116     0.0     0.00000   \n",
       "threat_pgw            70773.0     5.268944   13.406070     0.0     0.00000   \n",
       "threat_rlf            70773.0     4.009142   11.912672     0.0     0.00000   \n",
       "threat_rsf            70773.0     2.635680    9.863312     0.0     0.00000   \n",
       "total_points_pgw      70773.0     1.324460    2.475975    -4.0     0.00000   \n",
       "total_points_rlf      70773.0     1.013564    2.244730    -4.0     0.00000   \n",
       "total_points_rsf      70773.0     0.669351    1.892498    -4.0     0.00000   \n",
       "yellow_cards_pgw      70773.0     0.053905    0.225831     0.0     0.00000   \n",
       "yellow_cards_rlf      70773.0     0.041343    0.199085     0.0     0.00000   \n",
       "yellow_cards_rsf      70773.0     0.027397    0.163240     0.0     0.00000   \n",
       "total_points          70773.0     1.302000    2.250155    -1.5     0.00000   \n",
       "\n",
       "                             50%          75%     max  \n",
       "season                2017.00000  2018.000000  2019.0  \n",
       "GW                      20.00000    30.000000    38.0  \n",
       "at_home                  0.00000     1.000000     1.0  \n",
       "player_id              477.00000   700.000000  1211.0  \n",
       "opponent_id             11.00000    18.000000    29.0  \n",
       "champion_ls              0.00000     0.000000     1.0  \n",
       "top5_ls                  0.00000     1.000000     1.0  \n",
       "bottom5_ls               0.00000     0.000000     1.0  \n",
       "promoted_ts              0.00000     0.000000     1.0  \n",
       "assists_pgw              0.00000     0.000000     3.0  \n",
       "assists_rlf              0.00000     0.000000     3.0  \n",
       "assists_rsf              0.00000     0.000000     3.0  \n",
       "bonus_pgw                0.00000     0.000000     3.0  \n",
       "bonus_rlf                0.00000     0.000000     3.0  \n",
       "bonus_rsf                0.00000     0.000000     3.0  \n",
       "bps_pgw                  0.00000    10.000000   114.0  \n",
       "bps_rlf                  0.00000     6.000000   114.0  \n",
       "bps_rsf                  0.00000     0.000000   114.0  \n",
       "clean_sheets_pgw         0.00000     0.000000     1.0  \n",
       "clean_sheets_rlf         0.00000     0.000000     1.0  \n",
       "clean_sheets_rsf         0.00000     0.000000     1.0  \n",
       "creativity_pgw           0.00000     2.800000   170.9  \n",
       "creativity_rlf           0.00000     0.800000   134.3  \n",
       "creativity_rsf           0.00000     0.000000   118.1  \n",
       "goals_conceded_pgw       0.00000     1.000000     7.0  \n",
       "goals_conceded_rlf       0.00000     0.000000     7.0  \n",
       "goals_conceded_rsf       0.00000     0.000000     7.0  \n",
       "goals_scored_pgw         0.00000     0.000000     4.0  \n",
       "goals_scored_rlf         0.00000     0.000000     4.0  \n",
       "goals_scored_rsf         0.00000     0.000000     4.0  \n",
       "...                          ...          ...     ...  \n",
       "opponent_form            0.50000     0.625000     1.0  \n",
       "own_goals_pgw            0.00000     0.000000     1.0  \n",
       "own_goals_rlf            0.00000     0.000000     1.0  \n",
       "own_goals_rsf            0.00000     0.000000     1.0  \n",
       "penalties_missed_pgw     0.00000     0.000000     1.0  \n",
       "penalties_missed_rlf     0.00000     0.000000     1.0  \n",
       "penalties_missed_rsf     0.00000     0.000000     1.0  \n",
       "penalties_saved_pgw      0.00000     0.000000     2.0  \n",
       "penalties_saved_rlf      0.00000     0.000000     2.0  \n",
       "penalties_saved_rsf      0.00000     0.000000     2.0  \n",
       "player_form              0.07326     0.169729     1.0  \n",
       "red_cards_pgw            0.00000     0.000000     1.0  \n",
       "red_cards_rlf            0.00000     0.000000     1.0  \n",
       "red_cards_rsf            0.00000     0.000000     1.0  \n",
       "result_pgw               0.00000     1.000000     1.0  \n",
       "result_rlf               0.00000     1.000000     1.0  \n",
       "result_rsf               0.00000     0.000000     1.0  \n",
       "saves_pgw                0.00000     0.000000    14.0  \n",
       "saves_rlf                0.00000     0.000000    14.0  \n",
       "saves_rsf                0.00000     0.000000    14.0  \n",
       "threat_pgw               0.00000     2.000000   199.0  \n",
       "threat_rlf               0.00000     0.000000   199.0  \n",
       "threat_rsf               0.00000     0.000000   199.0  \n",
       "total_points_pgw         0.00000     2.000000    29.0  \n",
       "total_points_rlf         0.00000     1.000000    29.0  \n",
       "total_points_rsf         0.00000     0.000000    29.0  \n",
       "yellow_cards_pgw         0.00000     0.000000     1.0  \n",
       "yellow_cards_rlf         0.00000     0.000000     1.0  \n",
       "yellow_cards_rsf         0.00000     0.000000     1.0  \n",
       "total_points             0.00000     2.000000    14.5  \n",
       "\n",
       "[69 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"season\", \"GW\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != \"total_points\"];\n",
    "y = df.loc[:, df.columns == \"total_points\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.70, \n",
    "                                                    random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, train_size=0.70, \n",
    "                                                    random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape:  (49541, 66)\n",
      "y train shape:  (49541, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X train shape: \", X_train.shape)\n",
    "print(\"y train shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X test shape:  (6370, 66)\n",
      "y test shape:  (6370, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X test shape: \", X_test.shape)\n",
    "print(\"y test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X val shape:  (14862, 66)\n",
      "y val shape:  (14862, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X val shape: \", X_val.shape)\n",
    "print(\"y val shape: \", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim, dropout=0.1, l2_reg=0.01, regress=False):\n",
    "    # define our MLP network\n",
    "    model = Sequential();\n",
    "    \n",
    "#     model.add(Dense(32, input_dim=dim, activation=\"elu\"));\n",
    "#     model.add(Dropout(dropout));\n",
    "#     model.add(Dense(16, activation=\"elu\"));\n",
    "#     model.add(Dropout(dropout));\n",
    "#     model.add(Dense(8, activation=\"elu\"));\n",
    "#     model.add(Dropout(dropout));\n",
    "#     model.add(Dense(4, activation=\"elu\"));\n",
    "#     model.add(Dropout(dropout));\n",
    "#     model.add(Dense(2, activation=\"elu\"));\n",
    "#     model.add(Dropout(dropout));\n",
    "    \n",
    "    model.add(Dense(1024, input_dim=dim, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    "    model.add(Dense(512, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    "    model.add(Dense(256, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    "    model.add(Dense(128, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    "    model.add(Dense(64, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    "    model.add(Dense(32, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    " \n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"linear\"));\n",
    " \n",
    "    # return our model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0930 20:31:16.393298 4483118528 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0930 20:31:16.407402 4483118528 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0930 20:31:16.409677 4483118528 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0930 20:31:16.424854 4483118528 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0930 20:31:16.431332 4483118528 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0930 20:31:16.604063 4483118528 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = create_mlp(X_train.shape[1], dropout=0.1, l2_reg=0.001, regress=True)\n",
    "opt = keras.optimizers.Adam(lr=0.00001)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=opt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=10,\n",
    "                              verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0930 20:31:17.008172 4483118528 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49541 samples, validate on 14862 samples\n",
      "Epoch 1/500\n",
      "49541/49541 [==============================] - 7s 146us/step - loss: 6.3004 - val_loss: 5.2186\n",
      "Epoch 2/500\n",
      "49541/49541 [==============================] - 7s 131us/step - loss: 5.4673 - val_loss: 5.1484\n",
      "Epoch 3/500\n",
      "49541/49541 [==============================] - 7s 132us/step - loss: 5.3871 - val_loss: 5.1305\n",
      "Epoch 4/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 5.3433 - val_loss: 5.1158\n",
      "Epoch 5/500\n",
      "49541/49541 [==============================] - 7s 132us/step - loss: 5.3198 - val_loss: 5.1022\n",
      "Epoch 6/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 5.2826 - val_loss: 5.0977\n",
      "Epoch 7/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 5.2712 - val_loss: 5.0871\n",
      "Epoch 8/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 5.2630 - val_loss: 5.0814\n",
      "Epoch 9/500\n",
      "49541/49541 [==============================] - 6s 131us/step - loss: 5.2177 - val_loss: 5.0743\n",
      "Epoch 10/500\n",
      "49541/49541 [==============================] - 7s 132us/step - loss: 5.2238 - val_loss: 5.0686\n",
      "Epoch 11/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 5.2253 - val_loss: 5.0639\n",
      "Epoch 12/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 5.1948 - val_loss: 5.0570\n",
      "Epoch 13/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 5.1776 - val_loss: 5.0578\n",
      "Epoch 14/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 5.1764 - val_loss: 5.0477\n",
      "Epoch 15/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 5.1749 - val_loss: 5.0439\n",
      "Epoch 16/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 5.1709 - val_loss: 5.0409\n",
      "Epoch 17/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 5.1527 - val_loss: 5.0372\n",
      "Epoch 18/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 5.1319 - val_loss: 5.0326\n",
      "Epoch 19/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 5.1365 - val_loss: 5.0292\n",
      "Epoch 20/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 5.1386 - val_loss: 5.0238\n",
      "Epoch 21/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 5.1193 - val_loss: 5.0187\n",
      "Epoch 22/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 5.1177 - val_loss: 5.0142\n",
      "Epoch 23/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 5.1109 - val_loss: 5.0159\n",
      "Epoch 24/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 5.0969 - val_loss: 5.0079\n",
      "Epoch 25/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 5.0906 - val_loss: 5.0050\n",
      "Epoch 26/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 5.0991 - val_loss: 5.0013\n",
      "Epoch 27/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 5.0837 - val_loss: 4.9965\n",
      "Epoch 28/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 5.0742 - val_loss: 4.9972\n",
      "Epoch 29/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 5.0707 - val_loss: 4.9917\n",
      "Epoch 30/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 5.0650 - val_loss: 4.9885\n",
      "Epoch 31/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 5.0581 - val_loss: 4.9831\n",
      "Epoch 32/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 5.0462 - val_loss: 4.9797\n",
      "Epoch 33/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 5.0602 - val_loss: 4.9769\n",
      "Epoch 34/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 5.0429 - val_loss: 4.9687\n",
      "Epoch 35/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 5.0364 - val_loss: 4.9698\n",
      "Epoch 36/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 5.0271 - val_loss: 4.9690\n",
      "Epoch 37/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 5.0275 - val_loss: 4.9602\n",
      "Epoch 38/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 5.0160 - val_loss: 4.9580\n",
      "Epoch 39/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 5.0094 - val_loss: 4.9537\n",
      "Epoch 40/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 5.0171 - val_loss: 4.9529\n",
      "Epoch 41/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 5.0171 - val_loss: 4.9578\n",
      "Epoch 42/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.9894 - val_loss: 4.9456\n",
      "Epoch 43/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.9907 - val_loss: 4.9381\n",
      "Epoch 44/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.9948 - val_loss: 4.9367\n",
      "Epoch 45/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.9923 - val_loss: 4.9370\n",
      "Epoch 46/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.9771 - val_loss: 4.9276\n",
      "Epoch 47/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.9803 - val_loss: 4.9249\n",
      "Epoch 48/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.9830 - val_loss: 4.9215\n",
      "Epoch 49/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.9672 - val_loss: 4.9212\n",
      "Epoch 50/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.9620 - val_loss: 4.9214\n",
      "Epoch 51/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.9608 - val_loss: 4.9101\n",
      "Epoch 52/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.9563 - val_loss: 4.9149\n",
      "Epoch 53/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.9620 - val_loss: 4.9048\n",
      "Epoch 54/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.9506 - val_loss: 4.8999\n",
      "Epoch 55/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.9277 - val_loss: 4.8995\n",
      "Epoch 56/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.9405 - val_loss: 4.8938\n",
      "Epoch 57/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.9368 - val_loss: 4.8923\n",
      "Epoch 58/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.9243 - val_loss: 4.8839\n",
      "Epoch 59/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.9186 - val_loss: 4.8838\n",
      "Epoch 60/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.9151 - val_loss: 4.8773\n",
      "Epoch 61/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.9199 - val_loss: 4.8741\n",
      "Epoch 62/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.9124 - val_loss: 4.8739\n",
      "Epoch 63/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.9031 - val_loss: 4.8694\n",
      "Epoch 64/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.8983 - val_loss: 4.8622\n",
      "Epoch 65/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.8957 - val_loss: 4.8683\n",
      "Epoch 66/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.8908 - val_loss: 4.8637\n",
      "Epoch 67/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.8963 - val_loss: 4.8601\n",
      "Epoch 68/500\n",
      "49541/49541 [==============================] - 7s 143us/step - loss: 4.8850 - val_loss: 4.8520\n",
      "Epoch 69/500\n",
      "49541/49541 [==============================] - 7s 141us/step - loss: 4.8868 - val_loss: 4.8467\n",
      "Epoch 70/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.8755 - val_loss: 4.8456\n",
      "Epoch 71/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.8757 - val_loss: 4.8406\n",
      "Epoch 72/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.8646 - val_loss: 4.8396\n",
      "Epoch 73/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.8642 - val_loss: 4.8382\n",
      "Epoch 74/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.8551 - val_loss: 4.8322\n",
      "Epoch 75/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.8554 - val_loss: 4.8301\n",
      "Epoch 76/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49541/49541 [==============================] - 7s 133us/step - loss: 4.8465 - val_loss: 4.8261\n",
      "Epoch 77/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.8494 - val_loss: 4.8217\n",
      "Epoch 78/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.8384 - val_loss: 4.8174\n",
      "Epoch 79/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 4.8501 - val_loss: 4.8121\n",
      "Epoch 80/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.8361 - val_loss: 4.8108\n",
      "Epoch 81/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.8332 - val_loss: 4.8145\n",
      "Epoch 82/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.8267 - val_loss: 4.8081\n",
      "Epoch 83/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 4.8201 - val_loss: 4.8011\n",
      "Epoch 84/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.8232 - val_loss: 4.7994\n",
      "Epoch 85/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.8216 - val_loss: 4.7961\n",
      "Epoch 86/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.8148 - val_loss: 4.7963\n",
      "Epoch 87/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.8118 - val_loss: 4.7944\n",
      "Epoch 88/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.8067 - val_loss: 4.7884\n",
      "Epoch 89/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.7974 - val_loss: 4.7821\n",
      "Epoch 90/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.8035 - val_loss: 4.7792\n",
      "Epoch 91/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 4.7903 - val_loss: 4.7772\n",
      "Epoch 92/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.7771 - val_loss: 4.7731\n",
      "Epoch 93/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.7915 - val_loss: 4.7704\n",
      "Epoch 94/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.7877 - val_loss: 4.7671\n",
      "Epoch 95/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.7810 - val_loss: 4.7674\n",
      "Epoch 96/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.7742 - val_loss: 4.7606\n",
      "Epoch 97/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.7666 - val_loss: 4.7640\n",
      "Epoch 98/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.7730 - val_loss: 4.7554\n",
      "Epoch 99/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.7559 - val_loss: 4.7577\n",
      "Epoch 100/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.7605 - val_loss: 4.7534\n",
      "Epoch 101/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.7506 - val_loss: 4.7512\n",
      "Epoch 102/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.7547 - val_loss: 4.7504\n",
      "Epoch 103/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.7462 - val_loss: 4.7390\n",
      "Epoch 104/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.7437 - val_loss: 4.7417\n",
      "Epoch 105/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.7363 - val_loss: 4.7357\n",
      "Epoch 106/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.7366 - val_loss: 4.7333\n",
      "Epoch 107/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.7406 - val_loss: 4.7265\n",
      "Epoch 108/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.7255 - val_loss: 4.7252\n",
      "Epoch 109/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.7228 - val_loss: 4.7232\n",
      "Epoch 110/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.7270 - val_loss: 4.7161\n",
      "Epoch 111/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.7270 - val_loss: 4.7159\n",
      "Epoch 112/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.7190 - val_loss: 4.7174\n",
      "Epoch 113/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.7160 - val_loss: 4.7136\n",
      "Epoch 114/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.7130 - val_loss: 4.7132\n",
      "Epoch 115/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.7107 - val_loss: 4.7042\n",
      "Epoch 116/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.6976 - val_loss: 4.7018\n",
      "Epoch 117/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.7036 - val_loss: 4.6989\n",
      "Epoch 118/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.6940 - val_loss: 4.7039\n",
      "Epoch 119/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.6959 - val_loss: 4.6992\n",
      "Epoch 120/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.6857 - val_loss: 4.6942\n",
      "Epoch 121/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.6873 - val_loss: 4.6868\n",
      "Epoch 122/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.6852 - val_loss: 4.6862\n",
      "Epoch 123/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.6677 - val_loss: 4.6815\n",
      "Epoch 124/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.6736 - val_loss: 4.6812\n",
      "Epoch 125/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.6666 - val_loss: 4.6754\n",
      "Epoch 126/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.6670 - val_loss: 4.6740\n",
      "Epoch 127/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.6699 - val_loss: 4.6717\n",
      "Epoch 128/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.6660 - val_loss: 4.6695\n",
      "Epoch 129/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.6577 - val_loss: 4.6707\n",
      "Epoch 130/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.6515 - val_loss: 4.6701\n",
      "Epoch 131/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.6485 - val_loss: 4.6623\n",
      "Epoch 132/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.6515 - val_loss: 4.6526\n",
      "Epoch 133/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.6396 - val_loss: 4.6548\n",
      "Epoch 134/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.6406 - val_loss: 4.6499\n",
      "Epoch 135/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.6352 - val_loss: 4.6465\n",
      "Epoch 136/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.6252 - val_loss: 4.6516\n",
      "Epoch 137/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.6293 - val_loss: 4.6490\n",
      "Epoch 138/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.6276 - val_loss: 4.6433\n",
      "Epoch 139/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.6122 - val_loss: 4.6434\n",
      "Epoch 140/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.6262 - val_loss: 4.6341\n",
      "Epoch 141/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.6160 - val_loss: 4.6323\n",
      "Epoch 142/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.6139 - val_loss: 4.6296\n",
      "Epoch 143/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.6086 - val_loss: 4.6273\n",
      "Epoch 144/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.6146 - val_loss: 4.6288\n",
      "Epoch 145/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.6018 - val_loss: 4.6246\n",
      "Epoch 146/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.6026 - val_loss: 4.6203\n",
      "Epoch 147/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.6050 - val_loss: 4.6154\n",
      "Epoch 148/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.5887 - val_loss: 4.6196\n",
      "Epoch 149/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.5971 - val_loss: 4.6179\n",
      "Epoch 150/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.6066 - val_loss: 4.6166\n",
      "Epoch 151/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.5940 - val_loss: 4.6121\n",
      "Epoch 152/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 4.5816 - val_loss: 4.6108\n",
      "Epoch 153/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.5833 - val_loss: 4.6048\n",
      "Epoch 154/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.5780 - val_loss: 4.6065\n",
      "Epoch 155/500\n",
      "49541/49541 [==============================] - 7s 132us/step - loss: 4.5766 - val_loss: 4.6012\n",
      "Epoch 156/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.5802 - val_loss: 4.6007\n",
      "Epoch 157/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.5738 - val_loss: 4.5949\n",
      "Epoch 158/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.5787 - val_loss: 4.5910\n",
      "Epoch 159/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.5640 - val_loss: 4.5892\n",
      "Epoch 160/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.5734 - val_loss: 4.5887\n",
      "Epoch 161/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.5612 - val_loss: 4.5822\n",
      "Epoch 162/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.5537 - val_loss: 4.5866\n",
      "Epoch 163/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.5544 - val_loss: 4.5782\n",
      "Epoch 164/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.5558 - val_loss: 4.5759\n",
      "Epoch 165/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.5474 - val_loss: 4.5770\n",
      "Epoch 166/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.5504 - val_loss: 4.5737\n",
      "Epoch 167/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.5324 - val_loss: 4.5712\n",
      "Epoch 168/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.5300 - val_loss: 4.5701\n",
      "Epoch 169/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.5341 - val_loss: 4.5648\n",
      "Epoch 170/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.5346 - val_loss: 4.5621\n",
      "Epoch 171/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.5305 - val_loss: 4.5603\n",
      "Epoch 172/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.5298 - val_loss: 4.5557\n",
      "Epoch 173/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.5250 - val_loss: 4.5570\n",
      "Epoch 174/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.5297 - val_loss: 4.5529\n",
      "Epoch 175/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.5186 - val_loss: 4.5487\n",
      "Epoch 176/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.5203 - val_loss: 4.5500\n",
      "Epoch 177/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.5150 - val_loss: 4.5452\n",
      "Epoch 178/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.5152 - val_loss: 4.5451\n",
      "Epoch 179/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 4.5139 - val_loss: 4.5423\n",
      "Epoch 180/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.5023 - val_loss: 4.5370\n",
      "Epoch 181/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.5023 - val_loss: 4.5400\n",
      "Epoch 182/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.4950 - val_loss: 4.5325\n",
      "Epoch 183/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.4990 - val_loss: 4.5284\n",
      "Epoch 184/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.4928 - val_loss: 4.5415\n",
      "Epoch 185/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.4921 - val_loss: 4.5259\n",
      "Epoch 186/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.4907 - val_loss: 4.5210\n",
      "Epoch 187/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.4801 - val_loss: 4.5207\n",
      "Epoch 188/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.4906 - val_loss: 4.5229\n",
      "Epoch 189/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.4902 - val_loss: 4.5175\n",
      "Epoch 190/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.4724 - val_loss: 4.5156\n",
      "Epoch 191/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.4700 - val_loss: 4.5123\n",
      "Epoch 192/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.4696 - val_loss: 4.5136\n",
      "Epoch 193/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.4707 - val_loss: 4.5107\n",
      "Epoch 194/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.4628 - val_loss: 4.5147\n",
      "Epoch 195/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.4681 - val_loss: 4.5069\n",
      "Epoch 196/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.4634 - val_loss: 4.5025\n",
      "Epoch 197/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.4470 - val_loss: 4.5023\n",
      "Epoch 198/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.4546 - val_loss: 4.4976\n",
      "Epoch 199/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.4500 - val_loss: 4.4989\n",
      "Epoch 200/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.4597 - val_loss: 4.4975\n",
      "Epoch 201/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.4442 - val_loss: 4.4972\n",
      "Epoch 202/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.4504 - val_loss: 4.4957\n",
      "Epoch 203/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.4415 - val_loss: 4.4910\n",
      "Epoch 204/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.4488 - val_loss: 4.4887\n",
      "Epoch 205/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.4373 - val_loss: 4.4898\n",
      "Epoch 206/500\n",
      "49541/49541 [==============================] - 7s 146us/step - loss: 4.4419 - val_loss: 4.4893\n",
      "Epoch 207/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.4414 - val_loss: 4.4817\n",
      "Epoch 208/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.4318 - val_loss: 4.4808\n",
      "Epoch 209/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.4289 - val_loss: 4.4793\n",
      "Epoch 210/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.4235 - val_loss: 4.4742\n",
      "Epoch 211/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.4201 - val_loss: 4.4737\n",
      "Epoch 212/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.4291 - val_loss: 4.4698\n",
      "Epoch 213/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.4331 - val_loss: 4.4696\n",
      "Epoch 214/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.4144 - val_loss: 4.4678\n",
      "Epoch 215/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.4163 - val_loss: 4.4637\n",
      "Epoch 216/500\n",
      "49541/49541 [==============================] - 7s 141us/step - loss: 4.4038 - val_loss: 4.4687\n",
      "Epoch 217/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.4130 - val_loss: 4.4659\n",
      "Epoch 218/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.4095 - val_loss: 4.4588\n",
      "Epoch 219/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.4033 - val_loss: 4.4585\n",
      "Epoch 220/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.4058 - val_loss: 4.4601\n",
      "Epoch 221/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.4048 - val_loss: 4.4534\n",
      "Epoch 222/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.3964 - val_loss: 4.4500\n",
      "Epoch 223/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.3934 - val_loss: 4.4532\n",
      "Epoch 224/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.3932 - val_loss: 4.4455\n",
      "Epoch 225/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.4009 - val_loss: 4.4465\n",
      "Epoch 226/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.3924 - val_loss: 4.4451\n",
      "Epoch 227/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.3869 - val_loss: 4.4434\n",
      "Epoch 228/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 4.3984 - val_loss: 4.4372\n",
      "Epoch 229/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.3875 - val_loss: 4.4388\n",
      "Epoch 230/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.3873 - val_loss: 4.4367\n",
      "Epoch 231/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.3768 - val_loss: 4.4392\n",
      "Epoch 232/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.3788 - val_loss: 4.4373\n",
      "Epoch 233/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.3789 - val_loss: 4.4358\n",
      "Epoch 234/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.3736 - val_loss: 4.4280\n",
      "Epoch 235/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 4.3742 - val_loss: 4.4346\n",
      "Epoch 236/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.3614 - val_loss: 4.4289\n",
      "Epoch 237/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.3644 - val_loss: 4.4228\n",
      "Epoch 238/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.3539 - val_loss: 4.4218\n",
      "Epoch 239/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.3635 - val_loss: 4.4201\n",
      "Epoch 240/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.3564 - val_loss: 4.4176\n",
      "Epoch 241/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.3616 - val_loss: 4.4189\n",
      "Epoch 242/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.3523 - val_loss: 4.4131\n",
      "Epoch 243/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.3540 - val_loss: 4.4094\n",
      "Epoch 244/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.3576 - val_loss: 4.4122\n",
      "Epoch 245/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.3586 - val_loss: 4.4098\n",
      "Epoch 246/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.3397 - val_loss: 4.4078\n",
      "Epoch 247/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.3430 - val_loss: 4.4033\n",
      "Epoch 248/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.3379 - val_loss: 4.4072\n",
      "Epoch 249/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.3293 - val_loss: 4.4013\n",
      "Epoch 250/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.3291 - val_loss: 4.3982\n",
      "Epoch 251/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.3262 - val_loss: 4.4017\n",
      "Epoch 252/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.3338 - val_loss: 4.4031\n",
      "Epoch 253/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.3289 - val_loss: 4.3962\n",
      "Epoch 254/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.3237 - val_loss: 4.3961\n",
      "Epoch 255/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.3224 - val_loss: 4.3996\n",
      "Epoch 256/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.3211 - val_loss: 4.3877\n",
      "Epoch 257/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.3220 - val_loss: 4.3920\n",
      "Epoch 258/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.3343 - val_loss: 4.3875\n",
      "Epoch 259/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.3204 - val_loss: 4.3843\n",
      "Epoch 260/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.3194 - val_loss: 4.3812\n",
      "Epoch 261/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.3069 - val_loss: 4.3838\n",
      "Epoch 262/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.3115 - val_loss: 4.3889\n",
      "Epoch 263/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.3169 - val_loss: 4.3819\n",
      "Epoch 264/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.3027 - val_loss: 4.3852\n",
      "Epoch 265/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.2967 - val_loss: 4.3778\n",
      "Epoch 266/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.3033 - val_loss: 4.3731\n",
      "Epoch 267/500\n",
      "49541/49541 [==============================] - 7s 142us/step - loss: 4.3074 - val_loss: 4.3747\n",
      "Epoch 268/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.2939 - val_loss: 4.3749\n",
      "Epoch 269/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.2986 - val_loss: 4.3697\n",
      "Epoch 270/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.2966 - val_loss: 4.3684\n",
      "Epoch 271/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.2919 - val_loss: 4.3676\n",
      "Epoch 272/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.3003 - val_loss: 4.3682\n",
      "Epoch 273/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.2890 - val_loss: 4.3615\n",
      "Epoch 274/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.2889 - val_loss: 4.3652\n",
      "Epoch 275/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.2799 - val_loss: 4.3604\n",
      "Epoch 276/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.2835 - val_loss: 4.3630\n",
      "Epoch 277/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.2835 - val_loss: 4.3595\n",
      "Epoch 278/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.2828 - val_loss: 4.3554\n",
      "Epoch 279/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.2873 - val_loss: 4.3541\n",
      "Epoch 280/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.2742 - val_loss: 4.3514\n",
      "Epoch 281/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.2761 - val_loss: 4.3488\n",
      "Epoch 282/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.2735 - val_loss: 4.3509\n",
      "Epoch 283/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.2742 - val_loss: 4.3490\n",
      "Epoch 284/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.2620 - val_loss: 4.3459\n",
      "Epoch 285/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.2523 - val_loss: 4.3441\n",
      "Epoch 286/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.2579 - val_loss: 4.3453\n",
      "Epoch 287/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.2587 - val_loss: 4.3440\n",
      "Epoch 288/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.2629 - val_loss: 4.3468\n",
      "Epoch 289/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.2648 - val_loss: 4.3398\n",
      "Epoch 290/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.2631 - val_loss: 4.3356\n",
      "Epoch 291/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.2576 - val_loss: 4.3339\n",
      "Epoch 292/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.2509 - val_loss: 4.3371\n",
      "Epoch 293/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.2536 - val_loss: 4.3345\n",
      "Epoch 294/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.2520 - val_loss: 4.3318\n",
      "Epoch 295/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.2541 - val_loss: 4.3310\n",
      "Epoch 296/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.2373 - val_loss: 4.3329\n",
      "Epoch 297/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.2487 - val_loss: 4.3289\n",
      "Epoch 298/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.2497 - val_loss: 4.3288\n",
      "Epoch 299/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.2542 - val_loss: 4.3256\n",
      "Epoch 300/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.2396 - val_loss: 4.3264\n",
      "Epoch 301/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.2464 - val_loss: 4.3277\n",
      "Epoch 302/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.2421 - val_loss: 4.3223\n",
      "Epoch 303/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.2336 - val_loss: 4.3172\n",
      "Epoch 304/500\n",
      "49541/49541 [==============================] - 6s 129us/step - loss: 4.2318 - val_loss: 4.3164\n",
      "Epoch 305/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.2305 - val_loss: 4.3204\n",
      "Epoch 306/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.2292 - val_loss: 4.3157\n",
      "Epoch 307/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.2236 - val_loss: 4.3185\n",
      "Epoch 308/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.2225 - val_loss: 4.3157\n",
      "Epoch 309/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.2258 - val_loss: 4.3153\n",
      "Epoch 310/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.2247 - val_loss: 4.3132\n",
      "Epoch 311/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.2227 - val_loss: 4.3057\n",
      "Epoch 312/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.2218 - val_loss: 4.3071\n",
      "Epoch 313/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.2255 - val_loss: 4.3073\n",
      "Epoch 314/500\n",
      "49541/49541 [==============================] - 7s 132us/step - loss: 4.2087 - val_loss: 4.3035\n",
      "Epoch 315/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.2313 - val_loss: 4.3111\n",
      "Epoch 316/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.2036 - val_loss: 4.3086\n",
      "Epoch 317/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.2169 - val_loss: 4.3015\n",
      "Epoch 318/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.2055 - val_loss: 4.3024\n",
      "Epoch 319/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.2060 - val_loss: 4.2974\n",
      "Epoch 320/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.2037 - val_loss: 4.3003\n",
      "Epoch 321/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.1927 - val_loss: 4.2938\n",
      "Epoch 322/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.2040 - val_loss: 4.2936\n",
      "Epoch 323/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.1932 - val_loss: 4.2961\n",
      "Epoch 324/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.2053 - val_loss: 4.2984\n",
      "Epoch 325/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.2035 - val_loss: 4.2936\n",
      "Epoch 326/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.1917 - val_loss: 4.2900\n",
      "Epoch 327/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.1921 - val_loss: 4.2909\n",
      "Epoch 328/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.1956 - val_loss: 4.2922\n",
      "Epoch 329/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.1932 - val_loss: 4.2876\n",
      "Epoch 330/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.1850 - val_loss: 4.2884\n",
      "Epoch 331/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.1835 - val_loss: 4.2846\n",
      "Epoch 332/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.1779 - val_loss: 4.2831\n",
      "Epoch 333/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.1828 - val_loss: 4.2822\n",
      "Epoch 334/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.1787 - val_loss: 4.2826\n",
      "Epoch 335/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.1700 - val_loss: 4.2813\n",
      "Epoch 336/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.1813 - val_loss: 4.2777\n",
      "Epoch 337/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.1784 - val_loss: 4.2797\n",
      "Epoch 338/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.1692 - val_loss: 4.2752\n",
      "Epoch 339/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.1777 - val_loss: 4.2718\n",
      "Epoch 340/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.1608 - val_loss: 4.2751\n",
      "Epoch 341/500\n",
      "49541/49541 [==============================] - 7s 141us/step - loss: 4.1589 - val_loss: 4.2726\n",
      "Epoch 342/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.1761 - val_loss: 4.2708\n",
      "Epoch 343/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.1701 - val_loss: 4.2699\n",
      "Epoch 344/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.1681 - val_loss: 4.2711\n",
      "Epoch 345/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.1678 - val_loss: 4.2731\n",
      "Epoch 346/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.1672 - val_loss: 4.2663\n",
      "Epoch 347/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.1653 - val_loss: 4.2654\n",
      "Epoch 348/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.1633 - val_loss: 4.2665\n",
      "Epoch 349/500\n",
      "49541/49541 [==============================] - 7s 145us/step - loss: 4.1532 - val_loss: 4.2619\n",
      "Epoch 350/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.1695 - val_loss: 4.2624\n",
      "Epoch 351/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.1649 - val_loss: 4.2642\n",
      "Epoch 352/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.1487 - val_loss: 4.2597\n",
      "Epoch 353/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.1525 - val_loss: 4.2611\n",
      "Epoch 354/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.1534 - val_loss: 4.2634\n",
      "Epoch 355/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.1429 - val_loss: 4.2573\n",
      "Epoch 356/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.1486 - val_loss: 4.2582\n",
      "Epoch 357/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.1506 - val_loss: 4.2530\n",
      "Epoch 358/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.1487 - val_loss: 4.2533\n",
      "Epoch 359/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.1566 - val_loss: 4.2490\n",
      "Epoch 360/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.1470 - val_loss: 4.2473\n",
      "Epoch 361/500\n",
      "49541/49541 [==============================] - 7s 141us/step - loss: 4.1469 - val_loss: 4.2572\n",
      "Epoch 362/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.1397 - val_loss: 4.2464\n",
      "Epoch 363/500\n",
      "49541/49541 [==============================] - 7s 141us/step - loss: 4.1278 - val_loss: 4.2484\n",
      "Epoch 364/500\n",
      "49541/49541 [==============================] - 7s 141us/step - loss: 4.1309 - val_loss: 4.2437\n",
      "Epoch 365/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.1319 - val_loss: 4.2475\n",
      "Epoch 366/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.1316 - val_loss: 4.2401\n",
      "Epoch 367/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.1331 - val_loss: 4.2442\n",
      "Epoch 368/500\n",
      "49541/49541 [==============================] - 7s 141us/step - loss: 4.1348 - val_loss: 4.2401\n",
      "Epoch 369/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.1286 - val_loss: 4.2422\n",
      "Epoch 370/500\n",
      "49541/49541 [==============================] - 7s 141us/step - loss: 4.1200 - val_loss: 4.2426\n",
      "Epoch 371/500\n",
      "49541/49541 [==============================] - 7s 141us/step - loss: 4.1327 - val_loss: 4.2361\n",
      "Epoch 372/500\n",
      "49541/49541 [==============================] - 7s 142us/step - loss: 4.1212 - val_loss: 4.2528\n",
      "Epoch 373/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.1209 - val_loss: 4.2418\n",
      "Epoch 374/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.1202 - val_loss: 4.2385\n",
      "Epoch 375/500\n",
      "49541/49541 [==============================] - 7s 141us/step - loss: 4.1243 - val_loss: 4.2384\n",
      "Epoch 376/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.1243 - val_loss: 4.2365\n",
      "Epoch 377/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.1106 - val_loss: 4.2325\n",
      "Epoch 378/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.1102 - val_loss: 4.2301\n",
      "Epoch 379/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.1116 - val_loss: 4.2341\n",
      "Epoch 380/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.1091 - val_loss: 4.2301\n",
      "Epoch 381/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 4.1099 - val_loss: 4.2332\n",
      "Epoch 382/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.1171 - val_loss: 4.2268\n",
      "Epoch 383/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.1049 - val_loss: 4.2233\n",
      "Epoch 384/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.1130 - val_loss: 4.2269\n",
      "Epoch 385/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.1033 - val_loss: 4.2243\n",
      "Epoch 386/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.1015 - val_loss: 4.2240\n",
      "Epoch 387/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.1019 - val_loss: 4.2316\n",
      "Epoch 388/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.0949 - val_loss: 4.2203\n",
      "Epoch 389/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.0961 - val_loss: 4.2237\n",
      "Epoch 390/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.1003 - val_loss: 4.2153\n",
      "Epoch 391/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.1031 - val_loss: 4.2223\n",
      "Epoch 392/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.0957 - val_loss: 4.2199\n",
      "Epoch 393/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.0985 - val_loss: 4.2160\n",
      "Epoch 394/500\n",
      "49541/49541 [==============================] - 7s 142us/step - loss: 4.0898 - val_loss: 4.2257\n",
      "Epoch 395/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.0910 - val_loss: 4.2210\n",
      "Epoch 396/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.0889 - val_loss: 4.2222\n",
      "Epoch 397/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.0887 - val_loss: 4.2185\n",
      "Epoch 398/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.0884 - val_loss: 4.2209\n",
      "Epoch 399/500\n",
      "49541/49541 [==============================] - 7s 132us/step - loss: 4.0859 - val_loss: 4.2102\n",
      "Epoch 400/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.0866 - val_loss: 4.2109\n",
      "Epoch 401/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.0856 - val_loss: 4.2094\n",
      "Epoch 402/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.0865 - val_loss: 4.2130\n",
      "Epoch 403/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.0710 - val_loss: 4.2158\n",
      "Epoch 404/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.0818 - val_loss: 4.2087\n",
      "Epoch 405/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.0805 - val_loss: 4.2090\n",
      "Epoch 406/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.0713 - val_loss: 4.2046\n",
      "Epoch 407/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.0847 - val_loss: 4.2031\n",
      "Epoch 408/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.0672 - val_loss: 4.2034\n",
      "Epoch 409/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.0775 - val_loss: 4.2013\n",
      "Epoch 410/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.0726 - val_loss: 4.2053\n",
      "Epoch 411/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.0794 - val_loss: 4.1992\n",
      "Epoch 412/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.0743 - val_loss: 4.2038\n",
      "Epoch 413/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.0675 - val_loss: 4.1994\n",
      "Epoch 414/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.0703 - val_loss: 4.1993\n",
      "Epoch 415/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.0672 - val_loss: 4.2004\n",
      "Epoch 416/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.0612 - val_loss: 4.1984\n",
      "Epoch 417/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.0664 - val_loss: 4.1949\n",
      "Epoch 418/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.0587 - val_loss: 4.1937\n",
      "Epoch 419/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.0510 - val_loss: 4.1974\n",
      "Epoch 420/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.0564 - val_loss: 4.1927\n",
      "Epoch 421/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.0722 - val_loss: 4.2017\n",
      "Epoch 422/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.0573 - val_loss: 4.1960\n",
      "Epoch 423/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.0581 - val_loss: 4.1901\n",
      "Epoch 424/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.0501 - val_loss: 4.1950\n",
      "Epoch 425/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.0510 - val_loss: 4.1870\n",
      "Epoch 426/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.0619 - val_loss: 4.1922\n",
      "Epoch 427/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.0667 - val_loss: 4.1846\n",
      "Epoch 428/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.0534 - val_loss: 4.1869\n",
      "Epoch 429/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.0616 - val_loss: 4.1927\n",
      "Epoch 430/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.0531 - val_loss: 4.1863\n",
      "Epoch 431/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.0587 - val_loss: 4.1826\n",
      "Epoch 432/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.0465 - val_loss: 4.1800\n",
      "Epoch 433/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.0529 - val_loss: 4.1821\n",
      "Epoch 434/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.0412 - val_loss: 4.1879\n",
      "Epoch 435/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.0506 - val_loss: 4.1884\n",
      "Epoch 436/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.0469 - val_loss: 4.1836\n",
      "Epoch 437/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.0488 - val_loss: 4.1853\n",
      "Epoch 438/500\n",
      "49541/49541 [==============================] - 7s 141us/step - loss: 4.0408 - val_loss: 4.1826\n",
      "Epoch 439/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.0293 - val_loss: 4.1800\n",
      "Epoch 440/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.0397 - val_loss: 4.1841\n",
      "Epoch 441/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.0257 - val_loss: 4.1790\n",
      "Epoch 442/500\n",
      "49541/49541 [==============================] - 7s 141us/step - loss: 4.0351 - val_loss: 4.1816\n",
      "Epoch 443/500\n",
      "49541/49541 [==============================] - 7s 141us/step - loss: 4.0317 - val_loss: 4.1833\n",
      "Epoch 444/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.0349 - val_loss: 4.1802\n",
      "Epoch 445/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.0291 - val_loss: 4.1785\n",
      "Epoch 446/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.0325 - val_loss: 4.1733\n",
      "Epoch 447/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.0381 - val_loss: 4.1792\n",
      "Epoch 448/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.0319 - val_loss: 4.1670\n",
      "Epoch 449/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 4.0312 - val_loss: 4.1693\n",
      "Epoch 450/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.0274 - val_loss: 4.1704\n",
      "Epoch 451/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.0287 - val_loss: 4.1694\n",
      "Epoch 452/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.0258 - val_loss: 4.1707\n",
      "Epoch 453/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.0183 - val_loss: 4.1716\n",
      "Epoch 454/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.0265 - val_loss: 4.1634\n",
      "Epoch 455/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 4.0215 - val_loss: 4.1709\n",
      "Epoch 456/500\n",
      "49541/49541 [==============================] - 7s 142us/step - loss: 4.0177 - val_loss: 4.1716\n",
      "Epoch 457/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.0113 - val_loss: 4.1646\n",
      "Epoch 458/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 4.0058 - val_loss: 4.1750\n",
      "Epoch 459/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.0141 - val_loss: 4.1651\n",
      "Epoch 460/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 4.0187 - val_loss: 4.1653\n",
      "Epoch 461/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 4.0103 - val_loss: 4.1658\n",
      "Epoch 462/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 4.0165 - val_loss: 4.1674\n",
      "Epoch 463/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 4.0144 - val_loss: 4.1654\n",
      "Epoch 464/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.0189 - val_loss: 4.1611\n",
      "Epoch 465/500\n",
      "49541/49541 [==============================] - 6s 130us/step - loss: 3.9989 - val_loss: 4.1607\n",
      "Epoch 466/500\n",
      "49541/49541 [==============================] - 6s 130us/step - loss: 4.0218 - val_loss: 4.1599\n",
      "Epoch 467/500\n",
      "49541/49541 [==============================] - 6s 131us/step - loss: 4.0075 - val_loss: 4.1586\n",
      "Epoch 468/500\n",
      "49541/49541 [==============================] - 6s 129us/step - loss: 4.0130 - val_loss: 4.1554\n",
      "Epoch 469/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 4.0059 - val_loss: 4.1534\n",
      "Epoch 470/500\n",
      "49541/49541 [==============================] - 6s 129us/step - loss: 4.0059 - val_loss: 4.1572\n",
      "Epoch 471/500\n",
      "49541/49541 [==============================] - 7s 132us/step - loss: 4.0066 - val_loss: 4.1597\n",
      "Epoch 472/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 4.0097 - val_loss: 4.1562\n",
      "Epoch 473/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 3.9999 - val_loss: 4.1530\n",
      "Epoch 474/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 4.0077 - val_loss: 4.1529\n",
      "Epoch 475/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 3.9957 - val_loss: 4.1552\n",
      "Epoch 476/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 4.0007 - val_loss: 4.1514\n",
      "Epoch 477/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 3.9919 - val_loss: 4.1560\n",
      "Epoch 478/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 3.9942 - val_loss: 4.1510\n",
      "Epoch 479/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 3.9971 - val_loss: 4.1506\n",
      "Epoch 480/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 3.9987 - val_loss: 4.1547\n",
      "Epoch 481/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 3.9890 - val_loss: 4.1483\n",
      "Epoch 482/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 3.9904 - val_loss: 4.1520\n",
      "Epoch 483/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 3.9921 - val_loss: 4.1534\n",
      "Epoch 484/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 3.9992 - val_loss: 4.1479\n",
      "Epoch 485/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 3.9896 - val_loss: 4.1563\n",
      "Epoch 486/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 3.9834 - val_loss: 4.1517\n",
      "Epoch 487/500\n",
      "49541/49541 [==============================] - 6s 131us/step - loss: 3.9952 - val_loss: 4.1457\n",
      "Epoch 488/500\n",
      "49541/49541 [==============================] - 6s 130us/step - loss: 3.9889 - val_loss: 4.1453\n",
      "Epoch 489/500\n",
      "49541/49541 [==============================] - 7s 131us/step - loss: 3.9802 - val_loss: 4.1449\n",
      "Epoch 490/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 3.9867 - val_loss: 4.1453\n",
      "Epoch 491/500\n",
      "49541/49541 [==============================] - 7s 134us/step - loss: 3.9816 - val_loss: 4.1459\n",
      "Epoch 492/500\n",
      "49541/49541 [==============================] - 7s 135us/step - loss: 3.9668 - val_loss: 4.1453\n",
      "Epoch 493/500\n",
      "49541/49541 [==============================] - 7s 136us/step - loss: 3.9890 - val_loss: 4.1343\n",
      "Epoch 494/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 3.9837 - val_loss: 4.1395\n",
      "Epoch 495/500\n",
      "49541/49541 [==============================] - 7s 137us/step - loss: 3.9889 - val_loss: 4.1434\n",
      "Epoch 496/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 3.9786 - val_loss: 4.1409\n",
      "Epoch 497/500\n",
      "49541/49541 [==============================] - 7s 138us/step - loss: 3.9715 - val_loss: 4.1411\n",
      "Epoch 498/500\n",
      "49541/49541 [==============================] - 7s 139us/step - loss: 3.9710 - val_loss: 4.1382\n",
      "Epoch 499/500\n",
      "49541/49541 [==============================] - 7s 133us/step - loss: 3.9830 - val_loss: 4.1361\n",
      "Epoch 500/500\n",
      "49541/49541 [==============================] - 7s 140us/step - loss: 3.9720 - val_loss: 4.1467\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val),\n",
    "                    epochs=500, shuffle=True, batch_size=128, callbacks=[es], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(y_test[\"total_points\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_scaled);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'Actual': y_true.flatten(), 'Predicted': y_pred.flatten()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[\"in_range\"] = pred_df.apply(lambda row: True if \n",
    "                                    (row[\"Actual\"] - row[\"Predicted\"]) <= 1 else False, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In range: 0.87\n"
     ]
    }
   ],
   "source": [
    "print(\"In range: {0:.2f}\".format(pred_df[pred_df[\"in_range\"] == True].shape[0]/pred_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>in_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.125529</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231379</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121945</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.650470</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.836452</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.167470</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131989</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.682992</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.635103</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.756515</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.075645</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.263390</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.214271</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.256616</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.939074</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.483522</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.371740</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.281953</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082157</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.312881</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.498478</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.280813</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.087328</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.455120</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.824820</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual  Predicted  in_range\n",
       "0      0.0   3.125529      True\n",
       "1      0.0   0.231379      True\n",
       "2      0.0   0.121945      True\n",
       "3      9.0   2.650470     False\n",
       "4      1.0   0.836452      True\n",
       "5      0.0   0.167470      True\n",
       "6      0.0   0.131989      True\n",
       "7      1.0   0.682992      True\n",
       "8      1.0   1.635103      True\n",
       "9      0.0   2.756515      True\n",
       "10     0.0   1.075645      True\n",
       "11     2.0   0.263390     False\n",
       "12     0.0   1.214271      True\n",
       "13     2.0   3.256616      True\n",
       "14     2.0   1.939074      True\n",
       "15     3.0   2.483522      True\n",
       "16     0.0   0.371740      True\n",
       "17     1.0   0.281953      True\n",
       "18     0.0   0.082157      True\n",
       "19     0.0   0.312881      True\n",
       "20     0.0   1.498478      True\n",
       "21     2.0   3.280813      True\n",
       "22     0.0   0.087328      True\n",
       "23     1.0   3.455120      True\n",
       "24     0.0   3.824820      True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pred_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df.plot(kind='bar',figsize=(10,8))\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 1.1428835820051273\n",
      "R2 score: 0.2900606432426879\n",
      "Mean Squared Error: 3.6535758227605575\n",
      "Root Mean Squared Error: 1.9114329239501338\n"
     ]
    }
   ],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('R2 score:', metrics.r2_score(y_test, y_pred))\n",
    "\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_id_df = pd.read_csv(\"player_id_map.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_id_df = pd.read_csv(\"team_id_map.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and the test weekly fpl scrape\n",
    "gw=7\n",
    "weekly_scrape = pd.read_csv(\"gw{}-fpl-data.csv\".format(gw));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['at_home', 'player_id', 'opponent_id', 'champion_ls', 'top5_ls',\n",
       "       'bottom5_ls', 'promoted_ts', 'assists_pgw', 'assists_rlf',\n",
       "       'assists_rsf', 'bonus_pgw', 'bonus_rlf', 'bonus_rsf', 'bps_pgw',\n",
       "       'bps_rlf', 'bps_rsf', 'clean_sheets_pgw', 'clean_sheets_rlf',\n",
       "       'clean_sheets_rsf', 'creativity_pgw', 'creativity_rlf',\n",
       "       'creativity_rsf', 'goals_conceded_pgw', 'goals_conceded_rlf',\n",
       "       'goals_conceded_rsf', 'goals_scored_pgw', 'goals_scored_rlf',\n",
       "       'goals_scored_rsf', 'ict_index_pgw', 'ict_index_rlf', 'ict_index_rsf',\n",
       "       'influence_pgw', 'influence_rlf', 'influence_rsf', 'minutes_pgw',\n",
       "       'minutes_rlf', 'minutes_rsf', 'opponent_form', 'own_goals_pgw',\n",
       "       'own_goals_rlf', 'own_goals_rsf', 'penalties_missed_pgw',\n",
       "       'penalties_missed_rlf', 'penalties_missed_rsf', 'penalties_saved_pgw',\n",
       "       'penalties_saved_rlf', 'penalties_saved_rsf', 'player_form',\n",
       "       'red_cards_pgw', 'red_cards_rlf', 'red_cards_rsf', 'result_pgw',\n",
       "       'result_rlf', 'result_rsf', 'saves_pgw', 'saves_rlf', 'saves_rsf',\n",
       "       'threat_pgw', 'threat_rlf', 'threat_rsf', 'total_points_pgw',\n",
       "       'total_points_rlf', 'total_points_rsf', 'yellow_cards_pgw',\n",
       "       'yellow_cards_rlf', 'yellow_cards_rsf'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "weekly_scrape_scaled = scaler.transform(weekly_scrape[X_test.columns]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_weekly = model.predict(weekly_scrape_scaled);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_weekly_df = pd.DataFrame({'player_id': list(weekly_scrape[\"player_id\"]), \n",
    "                                 'position': list(weekly_scrape[\"position\"]),\n",
    "                                 'predicted': y_pred_weekly.flatten()});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_names = [];\n",
    "opponent_names = [];\n",
    "\n",
    "for index, row in weekly_scrape.iterrows():\n",
    "    player_id = row[\"player_id\"];\n",
    "    opponent_id = row[\"opponent_id\"];\n",
    "    \n",
    "    player_names.append(player_id_df[player_id_df[\"player_id\"] == player_id][\"actual_name\"].iloc[0]);\n",
    "    opponent_names.append(team_id_df[team_id_df[\"team_id\"] == opponent_id][\"team_name\"].iloc[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_weekly_df[\"player_name\"] = player_names;\n",
    "y_pred_weekly_df[\"opponent\"] = opponent_names;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_weekly_df = y_pred_weekly_df[[\"player_id\", \"position\", \"player_name\", \"opponent\", \"predicted\"]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_id</th>\n",
       "      <th>position</th>\n",
       "      <th>player_name</th>\n",
       "      <th>opponent</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>451</td>\n",
       "      <td>DEF</td>\n",
       "      <td>Martin Kelly</td>\n",
       "      <td>Norwich City</td>\n",
       "      <td>5.259672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1176</td>\n",
       "      <td>MID</td>\n",
       "      <td>Mason Mount</td>\n",
       "      <td>Brighton and Hove Albion</td>\n",
       "      <td>5.030162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>267</td>\n",
       "      <td>MID</td>\n",
       "      <td>Harry Wilson</td>\n",
       "      <td>West Ham United</td>\n",
       "      <td>4.970465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1206</td>\n",
       "      <td>MID</td>\n",
       "      <td>Todd Cantwell</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>3.591277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1204</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Teemu Pukki</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>3.325412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1154</td>\n",
       "      <td>DEF</td>\n",
       "      <td>John Lundstram</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>3.316150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>648</td>\n",
       "      <td>DEF</td>\n",
       "      <td>Trent Alexander-Arnold</td>\n",
       "      <td>Sheffield United</td>\n",
       "      <td>3.181467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>858</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Pierre-Emerick Aubameyang</td>\n",
       "      <td>Manchester United</td>\n",
       "      <td>3.084192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>947</td>\n",
       "      <td>GK</td>\n",
       "      <td>David Button</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>2.841447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>977</td>\n",
       "      <td>DEF</td>\n",
       "      <td>Issa Diop</td>\n",
       "      <td>Bournemouth</td>\n",
       "      <td>2.797502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>585</td>\n",
       "      <td>MID</td>\n",
       "      <td>Sadio Mané</td>\n",
       "      <td>Sheffield United</td>\n",
       "      <td>2.671776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176</td>\n",
       "      <td>GK</td>\n",
       "      <td>David de Gea</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>1.157742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>514</td>\n",
       "      <td>DEF</td>\n",
       "      <td>Nicolás Otamendi</td>\n",
       "      <td>Everton</td>\n",
       "      <td>0.745175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>385</td>\n",
       "      <td>MID</td>\n",
       "      <td>Kevin De Bruyne</td>\n",
       "      <td>Everton</td>\n",
       "      <td>0.452451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>882</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Tammy Abraham</td>\n",
       "      <td>Brighton and Hove Albion</td>\n",
       "      <td>0.066083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    player_id position                player_name                  opponent  \\\n",
       "5         451      DEF               Martin Kelly              Norwich City   \n",
       "9        1176      MID                Mason Mount  Brighton and Hove Albion   \n",
       "8         267      MID               Harry Wilson           West Ham United   \n",
       "11       1206      MID              Todd Cantwell            Crystal Palace   \n",
       "13       1204      FWD                Teemu Pukki            Crystal Palace   \n",
       "6        1154      DEF             John Lundstram                 Liverpool   \n",
       "2         648      DEF     Trent Alexander-Arnold          Sheffield United   \n",
       "14        858      FWD  Pierre-Emerick Aubameyang         Manchester United   \n",
       "1         947       GK               David Button                   Chelsea   \n",
       "4         977      DEF                  Issa Diop               Bournemouth   \n",
       "7         585      MID                 Sadio Mané          Sheffield United   \n",
       "0         176       GK               David de Gea                   Arsenal   \n",
       "3         514      DEF           Nicolás Otamendi                   Everton   \n",
       "10        385      MID            Kevin De Bruyne                   Everton   \n",
       "12        882      FWD              Tammy Abraham  Brighton and Hove Albion   \n",
       "\n",
       "    predicted  \n",
       "5    5.259672  \n",
       "9    5.030162  \n",
       "8    4.970465  \n",
       "11   3.591277  \n",
       "13   3.325412  \n",
       "6    3.316150  \n",
       "2    3.181467  \n",
       "14   3.084192  \n",
       "1    2.841447  \n",
       "4    2.797502  \n",
       "7    2.671776  \n",
       "0    1.157742  \n",
       "3    0.745175  \n",
       "10   0.452451  \n",
       "12   0.066083  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_weekly_df.sort_values([\"predicted\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_gk_min = 1;\n",
    "\n",
    "cond_def_min = 3;\n",
    "cond_def_max = 5;\n",
    "\n",
    "cond_mid_min = 2;\n",
    "cond_mid_max = 5;\n",
    "\n",
    "cond_fwd_min = 1;\n",
    "cond_fwd_max = 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectTeam(df, max_players=11):\n",
    "    best11 = y_pred_weekly_df.sort_values([\"predicted\"], ascending=False)\n",
    "    df_ = pd.DataFrame(columns=best11.columns);\n",
    "\n",
    "    for index, row in best11.iterrows():\n",
    "        num_gk = df_[df_[\"position\"] == \"GK\"].shape[0];\n",
    "        num_def = df_[df_[\"position\"] == \"DEF\"].shape[0];\n",
    "        num_mid = df_[df_[\"position\"] == \"MID\"].shape[0];\n",
    "        num_fwd = df_[df_[\"position\"] == \"FWD\"].shape[0];\n",
    "        num_players = df_.shape[0];\n",
    "        \n",
    "        position = row[\"position\"];\n",
    "    \n",
    "        if (position == \"GK\" and num_gk < cond_gk_min and num_players < max_players):\n",
    "            df_ = df_.append(row);\n",
    "        if (position == \"DEF\" and (num_def < cond_def_min or \n",
    "            (num_def >= cond_def_min and num_def <= cond_def_max)) and num_players < max_players):\n",
    "            df_ = df_.append(row);\n",
    "        if (position == \"MID\" and (num_mid < cond_mid_min or \n",
    "            (num_mid >= cond_mid_min and num_mid <= cond_mid_max)) and num_players < max_players):\n",
    "            df_ = df_.append(row);\n",
    "        if (position == \"FWD\" and (num_fwd < cond_fwd_min or \n",
    "            (num_fwd >= cond_fwd_min and num_fwd <= cond_fwd_max)) and num_players < max_players):\n",
    "            df_ = df_.append(row);\n",
    "    \n",
    "    gk = list(df_[df_[\"position\"] == \"GK\"][\"player_name\"])\n",
    "    defenders = list(df_[df_[\"position\"] == \"DEF\"][\"player_name\"])\n",
    "    mids = list(df_[df_[\"position\"] == \"MID\"][\"player_name\"])\n",
    "    fwds = list(df_[df_[\"position\"] == \"FWD\"][\"player_name\"])\n",
    "    \n",
    "    selectedTeam = pd.DataFrame(columns=df_.columns);\n",
    "    selectedTeam = selectedTeam.append(df_[df_[\"position\"] == \"GK\"]);\n",
    "    selectedTeam = selectedTeam.append(df_[df_[\"position\"] == \"DEF\"]);\n",
    "    selectedTeam = selectedTeam.append(df_[df_[\"position\"] == \"MID\"]);\n",
    "    selectedTeam = selectedTeam.append(df_[df_[\"position\"] == \"FWD\"]);\n",
    "    \n",
    "    selectedTeam[\"predicted\"] = selectedTeam[\"predicted\"].apply(lambda pred: round(pred, 0))\n",
    "    \n",
    "    return selectedTeam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "best11 = selectTeam(y_pred_weekly_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_id</th>\n",
       "      <th>position</th>\n",
       "      <th>player_name</th>\n",
       "      <th>opponent</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>947</td>\n",
       "      <td>GK</td>\n",
       "      <td>David Button</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>451</td>\n",
       "      <td>DEF</td>\n",
       "      <td>Martin Kelly</td>\n",
       "      <td>Norwich City</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1154</td>\n",
       "      <td>DEF</td>\n",
       "      <td>John Lundstram</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>648</td>\n",
       "      <td>DEF</td>\n",
       "      <td>Trent Alexander-Arnold</td>\n",
       "      <td>Sheffield United</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>977</td>\n",
       "      <td>DEF</td>\n",
       "      <td>Issa Diop</td>\n",
       "      <td>Bournemouth</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1176</td>\n",
       "      <td>MID</td>\n",
       "      <td>Mason Mount</td>\n",
       "      <td>Brighton and Hove Albion</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>267</td>\n",
       "      <td>MID</td>\n",
       "      <td>Harry Wilson</td>\n",
       "      <td>West Ham United</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1206</td>\n",
       "      <td>MID</td>\n",
       "      <td>Todd Cantwell</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>585</td>\n",
       "      <td>MID</td>\n",
       "      <td>Sadio Mané</td>\n",
       "      <td>Sheffield United</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1204</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Teemu Pukki</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>858</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Pierre-Emerick Aubameyang</td>\n",
       "      <td>Manchester United</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   player_id position                player_name                  opponent  \\\n",
       "1        947       GK               David Button                   Chelsea   \n",
       "5        451      DEF               Martin Kelly              Norwich City   \n",
       "6       1154      DEF             John Lundstram                 Liverpool   \n",
       "2        648      DEF     Trent Alexander-Arnold          Sheffield United   \n",
       "4        977      DEF                  Issa Diop               Bournemouth   \n",
       "9       1176      MID                Mason Mount  Brighton and Hove Albion   \n",
       "8        267      MID               Harry Wilson           West Ham United   \n",
       "11      1206      MID              Todd Cantwell            Crystal Palace   \n",
       "7        585      MID                 Sadio Mané          Sheffield United   \n",
       "13      1204      FWD                Teemu Pukki            Crystal Palace   \n",
       "14       858      FWD  Pierre-Emerick Aubameyang         Manchester United   \n",
       "\n",
       "    predicted  \n",
       "1         3.0  \n",
       "5         5.0  \n",
       "6         3.0  \n",
       "2         3.0  \n",
       "4         3.0  \n",
       "9         5.0  \n",
       "8         5.0  \n",
       "11        4.0  \n",
       "7         3.0  \n",
       "13        3.0  \n",
       "14        3.0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(best11[\"predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "best11.to_csv(\"gw{}-best11.csv\".format(gw), index=False);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
