{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(os.getcwd(), \"..\", \"data\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"combined-season-data.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>GW</th>\n",
       "      <th>at_home</th>\n",
       "      <th>player_id</th>\n",
       "      <th>opponent_id</th>\n",
       "      <th>champion_ls</th>\n",
       "      <th>top5_ls</th>\n",
       "      <th>bottom5_ls</th>\n",
       "      <th>promoted_ts</th>\n",
       "      <th>assists_pgw</th>\n",
       "      <th>...</th>\n",
       "      <th>own_goals_pgw</th>\n",
       "      <th>penalties_missed_pgw</th>\n",
       "      <th>penalties_saved_pgw</th>\n",
       "      <th>red_cards_pgw</th>\n",
       "      <th>result_pgw</th>\n",
       "      <th>saves_pgw</th>\n",
       "      <th>threat_pgw</th>\n",
       "      <th>yellow_cards_pgw</th>\n",
       "      <th>total_points_pgw</th>\n",
       "      <th>total_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  GW  at_home  player_id  opponent_id  champion_ls  top5_ls  \\\n",
       "0    2016   1        0          1            4            0        0   \n",
       "1    2016   1        1          2           17            0        1   \n",
       "2    2016   1        1          4            9            0        0   \n",
       "3    2016   1        0          6           13            0        0   \n",
       "4    2016   1        1          7           20            0        0   \n",
       "\n",
       "   bottom5_ls  promoted_ts  assists_pgw  ...  own_goals_pgw  \\\n",
       "0           0            0          0.0  ...            0.0   \n",
       "1           0            0          0.0  ...            0.0   \n",
       "2           0            0          0.0  ...            0.0   \n",
       "3           0            0          0.0  ...            0.0   \n",
       "4           0            0          0.0  ...            0.0   \n",
       "\n",
       "   penalties_missed_pgw  penalties_saved_pgw  red_cards_pgw  result_pgw  \\\n",
       "0                   0.0                  0.0            0.0         0.0   \n",
       "1                   0.0                  0.0            0.0         0.0   \n",
       "2                   0.0                  0.0            0.0         0.0   \n",
       "3                   0.0                  0.0            0.0         0.0   \n",
       "4                   0.0                  0.0            0.0         0.0   \n",
       "\n",
       "   saves_pgw  threat_pgw  yellow_cards_pgw  total_points_pgw  total_points  \n",
       "0        0.0         0.0               0.0               0.0           0.0  \n",
       "1        0.0         0.0               0.0               0.0           1.0  \n",
       "2        0.0         0.0               0.0               0.0           2.0  \n",
       "3        0.0         0.0               0.0               0.0           0.0  \n",
       "4        0.0         0.0               0.0               0.0           0.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "season                    int64\n",
       "GW                        int64\n",
       "at_home                   int64\n",
       "player_id                 int64\n",
       "opponent_id               int64\n",
       "champion_ls               int64\n",
       "top5_ls                   int64\n",
       "bottom5_ls                int64\n",
       "promoted_ts               int64\n",
       "assists_pgw             float64\n",
       "bonus_pgw               float64\n",
       "bps_pgw                 float64\n",
       "clean_sheets_pgw        float64\n",
       "creativity_pgw          float64\n",
       "goals_conceded_pgw      float64\n",
       "goals_scored_pgw        float64\n",
       "ict_index_pgw           float64\n",
       "influence_pgw           float64\n",
       "minutes_pgw             float64\n",
       "own_goals_pgw           float64\n",
       "penalties_missed_pgw    float64\n",
       "penalties_saved_pgw     float64\n",
       "red_cards_pgw           float64\n",
       "result_pgw              float64\n",
       "saves_pgw               float64\n",
       "threat_pgw              float64\n",
       "yellow_cards_pgw        float64\n",
       "total_points_pgw        float64\n",
       "total_points            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>season</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>2017.052312</td>\n",
       "      <td>0.903083</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>2019.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GW</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>19.487769</td>\n",
       "      <td>11.343298</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>38.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at_home</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>0.498588</td>\n",
       "      <td>0.500002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>player_id</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>487.164714</td>\n",
       "      <td>295.528292</td>\n",
       "      <td>1.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>701.0</td>\n",
       "      <td>1211.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opponent_id</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>12.015294</td>\n",
       "      <td>7.253878</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>champion_ls</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>0.050560</td>\n",
       "      <td>0.219099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top5_ls</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>0.251799</td>\n",
       "      <td>0.434050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bottom5_ls</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>0.248657</td>\n",
       "      <td>0.432238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promoted_ts</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>0.149261</td>\n",
       "      <td>0.356348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assists_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>0.037221</td>\n",
       "      <td>0.204883</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonus_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>0.099005</td>\n",
       "      <td>0.470323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bps_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>5.885078</td>\n",
       "      <td>9.731917</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>114.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clean_sheets_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>0.100844</td>\n",
       "      <td>0.301125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>creativity_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>4.675485</td>\n",
       "      <td>10.746984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>170.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goals_conceded_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>0.480412</td>\n",
       "      <td>0.952157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goals_scored_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>0.040798</td>\n",
       "      <td>0.221582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ict_index_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>1.674070</td>\n",
       "      <td>3.006404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>31.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>influence_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>7.018457</td>\n",
       "      <td>12.740176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>163.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minutes_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>31.208655</td>\n",
       "      <td>40.344227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>own_goals_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.037262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penalties_missed_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.029219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>penalties_saved_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.027696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>red_cards_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.043517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>result_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>-0.008632</td>\n",
       "      <td>0.862711</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saves_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>0.089837</td>\n",
       "      <td>0.613501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threat_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>5.061552</td>\n",
       "      <td>13.106646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>199.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yellow_cards_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>0.053022</td>\n",
       "      <td>0.224079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_points_pgw</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>1.284907</td>\n",
       "      <td>2.438780</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_points</th>\n",
       "      <td>69047.0</td>\n",
       "      <td>1.116105</td>\n",
       "      <td>1.861749</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count         mean         std     min     25%  \\\n",
       "season                69047.0  2017.052312    0.903083  2016.0  2016.0   \n",
       "GW                    69047.0    19.487769   11.343298     1.0     9.0   \n",
       "at_home               69047.0     0.498588    0.500002     0.0     0.0   \n",
       "player_id             69047.0   487.164714  295.528292     1.0   239.0   \n",
       "opponent_id           69047.0    12.015294    7.253878     1.0     5.0   \n",
       "champion_ls           69047.0     0.050560    0.219099     0.0     0.0   \n",
       "top5_ls               69047.0     0.251799    0.434050     0.0     0.0   \n",
       "bottom5_ls            69047.0     0.248657    0.432238     0.0     0.0   \n",
       "promoted_ts           69047.0     0.149261    0.356348     0.0     0.0   \n",
       "assists_pgw           69047.0     0.037221    0.204883     0.0     0.0   \n",
       "bonus_pgw             69047.0     0.099005    0.470323     0.0     0.0   \n",
       "bps_pgw               69047.0     5.885078    9.731917   -19.0     0.0   \n",
       "clean_sheets_pgw      69047.0     0.100844    0.301125     0.0     0.0   \n",
       "creativity_pgw        69047.0     4.675485   10.746984     0.0     0.0   \n",
       "goals_conceded_pgw    69047.0     0.480412    0.952157     0.0     0.0   \n",
       "goals_scored_pgw      69047.0     0.040798    0.221582     0.0     0.0   \n",
       "ict_index_pgw         69047.0     1.674070    3.006404     0.0     0.0   \n",
       "influence_pgw         69047.0     7.018457   12.740176     0.0     0.0   \n",
       "minutes_pgw           69047.0    31.208655   40.344227     0.0     0.0   \n",
       "own_goals_pgw         69047.0     0.001390    0.037262     0.0     0.0   \n",
       "penalties_missed_pgw  69047.0     0.000854    0.029219     0.0     0.0   \n",
       "penalties_saved_pgw   69047.0     0.000739    0.027696     0.0     0.0   \n",
       "red_cards_pgw         69047.0     0.001897    0.043517     0.0     0.0   \n",
       "result_pgw            69047.0    -0.008632    0.862711    -1.0    -1.0   \n",
       "saves_pgw             69047.0     0.089837    0.613501     0.0     0.0   \n",
       "threat_pgw            69047.0     5.061552   13.106646     0.0     0.0   \n",
       "yellow_cards_pgw      69047.0     0.053022    0.224079     0.0     0.0   \n",
       "total_points_pgw      69047.0     1.284907    2.438780    -4.0     0.0   \n",
       "total_points          69047.0     1.116105    1.861749    -1.5     0.0   \n",
       "\n",
       "                         50%     75%      max  \n",
       "season                2017.0  2018.0  2019.00  \n",
       "GW                      20.0    30.0    38.00  \n",
       "at_home                  0.0     1.0     1.00  \n",
       "player_id              478.0   701.0  1211.00  \n",
       "opponent_id             11.0    18.0    29.00  \n",
       "champion_ls              0.0     0.0     1.00  \n",
       "top5_ls                  0.0     1.0     1.00  \n",
       "bottom5_ls               0.0     0.0     1.00  \n",
       "promoted_ts              0.0     0.0     1.00  \n",
       "assists_pgw              0.0     0.0     3.00  \n",
       "bonus_pgw                0.0     0.0     3.00  \n",
       "bps_pgw                  0.0    10.0   114.00  \n",
       "clean_sheets_pgw         0.0     0.0     1.00  \n",
       "creativity_pgw           0.0     2.5   170.90  \n",
       "goals_conceded_pgw       0.0     1.0     7.00  \n",
       "goals_scored_pgw         0.0     0.0     4.00  \n",
       "ict_index_pgw            0.0     2.4    31.10  \n",
       "influence_pgw            0.0    10.4   163.60  \n",
       "minutes_pgw              0.0    90.0    90.00  \n",
       "own_goals_pgw            0.0     0.0     1.00  \n",
       "penalties_missed_pgw     0.0     0.0     1.00  \n",
       "penalties_saved_pgw      0.0     0.0     2.00  \n",
       "red_cards_pgw            0.0     0.0     1.00  \n",
       "result_pgw               0.0     1.0     1.00  \n",
       "saves_pgw                0.0     0.0    14.00  \n",
       "threat_pgw               0.0     2.0   199.00  \n",
       "yellow_cards_pgw         0.0     0.0     1.00  \n",
       "total_points_pgw         0.0     2.0    29.00  \n",
       "total_points             0.0     2.0    17.75  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"season\", \"GW\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != \"total_points\"];\n",
    "y = df.loc[:, df.columns == \"total_points\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.70, \n",
    "                                                    random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, train_size=0.70, \n",
    "                                                    random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape:  (48332, 26)\n",
      "y train shape:  (48332, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X train shape: \", X_train.shape)\n",
    "print(\"y train shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X test shape:  (6215, 26)\n",
      "y test shape:  (6215, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X test shape: \", X_test.shape)\n",
    "print(\"y test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X val shape:  (14500, 26)\n",
      "y val shape:  (14500, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X val shape: \", X_val.shape)\n",
    "print(\"y val shape: \", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim, dropout=0.1, l2_reg=0.01, regress=False):\n",
    "    # define our MLP network\n",
    "    model = Sequential();\n",
    "    \n",
    "#     model.add(Dense(32, input_dim=dim, activation=\"elu\"));\n",
    "#     model.add(Dropout(dropout));\n",
    "#     model.add(Dense(16, activation=\"elu\"));\n",
    "#     model.add(Dropout(dropout));\n",
    "#     model.add(Dense(8, activation=\"elu\"));\n",
    "#     model.add(Dropout(dropout));\n",
    "#     model.add(Dense(4, activation=\"elu\"));\n",
    "#     model.add(Dropout(dropout));\n",
    "#     model.add(Dense(2, activation=\"elu\"));\n",
    "#     model.add(Dropout(dropout));\n",
    "    \n",
    "    model.add(Dense(1024, input_dim=dim, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    "    model.add(Dense(512, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    "    model.add(Dense(256, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    "    model.add(Dense(128, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    "    model.add(Dense(64, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    "    model.add(Dense(32, kernel_regularizer=regularizers.l2(l2_reg), activation=\"elu\"));\n",
    "    model.add(Dropout(dropout));\n",
    " \n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"linear\"));\n",
    " \n",
    "    # return our model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0928 08:44:30.600147 4457973184 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0928 08:44:30.615557 4457973184 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0928 08:44:30.618648 4457973184 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0928 08:44:30.643406 4457973184 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0928 08:44:30.652335 4457973184 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0928 08:44:30.831280 4457973184 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = create_mlp(X_train.shape[1], dropout=0.3, l2_reg=0.1, regress=True)\n",
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=opt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=10,\n",
    "                              verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0928 08:44:31.236298 4457973184 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48332 samples, validate on 14500 samples\n",
      "Epoch 1/500\n",
      "48332/48332 [==============================] - 7s 147us/step - loss: 94.4880 - val_loss: 56.7470\n",
      "Epoch 2/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 36.1872 - val_loss: 21.3274\n",
      "Epoch 3/500\n",
      "48332/48332 [==============================] - 6s 128us/step - loss: 14.3847 - val_loss: 9.4731\n",
      "Epoch 4/500\n",
      "48332/48332 [==============================] - 6s 128us/step - loss: 7.2330 - val_loss: 5.5613\n",
      "Epoch 5/500\n",
      "48332/48332 [==============================] - 6s 129us/step - loss: 4.7704 - val_loss: 4.1180\n",
      "Epoch 6/500\n",
      "48332/48332 [==============================] - 6s 129us/step - loss: 3.8218 - val_loss: 3.5251\n",
      "Epoch 7/500\n",
      "48332/48332 [==============================] - 6s 130us/step - loss: 3.4184 - val_loss: 3.2647\n",
      "Epoch 8/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 3.2290 - val_loss: 3.1346\n",
      "Epoch 9/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 3.1353 - val_loss: 3.0701\n",
      "Epoch 10/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 3.0903 - val_loss: 3.0367\n",
      "Epoch 11/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 3.0480 - val_loss: 3.0189\n",
      "Epoch 12/500\n",
      "48332/48332 [==============================] - 6s 130us/step - loss: 3.0325 - val_loss: 3.0167\n",
      "Epoch 13/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 3.0264 - val_loss: 2.9981\n",
      "Epoch 14/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 3.0215 - val_loss: 2.9940\n",
      "Epoch 15/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 3.0214 - val_loss: 2.9970\n",
      "Epoch 16/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 3.0080 - val_loss: 2.9903\n",
      "Epoch 17/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 3.0106 - val_loss: 2.9894\n",
      "Epoch 18/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 3.0128 - val_loss: 2.9833\n",
      "Epoch 19/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 3.0036 - val_loss: 2.9744\n",
      "Epoch 20/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.9957 - val_loss: 2.9794\n",
      "Epoch 21/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.9991 - val_loss: 2.9734\n",
      "Epoch 22/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.9938 - val_loss: 2.9659\n",
      "Epoch 23/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.9885 - val_loss: 2.9705\n",
      "Epoch 24/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.9845 - val_loss: 2.9715\n",
      "Epoch 25/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.9837 - val_loss: 2.9612\n",
      "Epoch 26/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.9805 - val_loss: 2.9846\n",
      "Epoch 27/500\n",
      "48332/48332 [==============================] - 6s 128us/step - loss: 2.9774 - val_loss: 2.9646\n",
      "Epoch 28/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.9720 - val_loss: 2.9538\n",
      "Epoch 29/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.9751 - val_loss: 2.9471\n",
      "Epoch 30/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.9703 - val_loss: 2.9490\n",
      "Epoch 31/500\n",
      "48332/48332 [==============================] - 6s 130us/step - loss: 2.9677 - val_loss: 2.9576\n",
      "Epoch 32/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.9622 - val_loss: 2.9466\n",
      "Epoch 33/500\n",
      "48332/48332 [==============================] - 7s 137us/step - loss: 2.9620 - val_loss: 2.9419\n",
      "Epoch 34/500\n",
      "48332/48332 [==============================] - 6s 123us/step - loss: 2.9631 - val_loss: 2.9421\n",
      "Epoch 35/500\n",
      "48332/48332 [==============================] - 6s 128us/step - loss: 2.9575 - val_loss: 2.9393\n",
      "Epoch 36/500\n",
      "48332/48332 [==============================] - 6s 125us/step - loss: 2.9545 - val_loss: 2.9353\n",
      "Epoch 37/500\n",
      "48332/48332 [==============================] - 6s 126us/step - loss: 2.9535 - val_loss: 2.9457\n",
      "Epoch 38/500\n",
      "48332/48332 [==============================] - 6s 128us/step - loss: 2.9624 - val_loss: 2.9353\n",
      "Epoch 39/500\n",
      "48332/48332 [==============================] - 6s 129us/step - loss: 2.9534 - val_loss: 2.9324\n",
      "Epoch 40/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.9455 - val_loss: 2.9269\n",
      "Epoch 41/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.9470 - val_loss: 2.9275\n",
      "Epoch 42/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.9456 - val_loss: 2.9290\n",
      "Epoch 43/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.9473 - val_loss: 2.9244\n",
      "Epoch 44/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.9436 - val_loss: 2.9220\n",
      "Epoch 45/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.9450 - val_loss: 2.9263\n",
      "Epoch 46/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.9452 - val_loss: 2.9360\n",
      "Epoch 47/500\n",
      "48332/48332 [==============================] - 6s 130us/step - loss: 2.9412 - val_loss: 2.9379\n",
      "Epoch 48/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.9420 - val_loss: 2.9160\n",
      "Epoch 49/500\n",
      "48332/48332 [==============================] - 6s 129us/step - loss: 2.9293 - val_loss: 2.9237\n",
      "Epoch 50/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.9372 - val_loss: 2.9262\n",
      "Epoch 51/500\n",
      "48332/48332 [==============================] - 6s 128us/step - loss: 2.9334 - val_loss: 2.9137\n",
      "Epoch 52/500\n",
      "48332/48332 [==============================] - 6s 130us/step - loss: 2.9366 - val_loss: 2.9131\n",
      "Epoch 53/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.9345 - val_loss: 2.9097\n",
      "Epoch 54/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.9340 - val_loss: 2.9164\n",
      "Epoch 55/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.9244 - val_loss: 2.9687\n",
      "Epoch 56/500\n",
      "48332/48332 [==============================] - 6s 129us/step - loss: 2.9285 - val_loss: 2.9244\n",
      "Epoch 57/500\n",
      "48332/48332 [==============================] - 6s 129us/step - loss: 2.9261 - val_loss: 2.9224\n",
      "Epoch 58/500\n",
      "48332/48332 [==============================] - 6s 129us/step - loss: 2.9281 - val_loss: 2.9092\n",
      "Epoch 59/500\n",
      "48332/48332 [==============================] - 6s 124us/step - loss: 2.9199 - val_loss: 2.9237\n",
      "Epoch 60/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.9207 - val_loss: 2.9099\n",
      "Epoch 61/500\n",
      "48332/48332 [==============================] - 6s 128us/step - loss: 2.9262 - val_loss: 2.9018\n",
      "Epoch 62/500\n",
      "48332/48332 [==============================] - 6s 130us/step - loss: 2.9218 - val_loss: 2.9289\n",
      "Epoch 63/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.9236 - val_loss: 2.8997\n",
      "Epoch 64/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.9204 - val_loss: 2.9046\n",
      "Epoch 65/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.9153 - val_loss: 2.8975\n",
      "Epoch 66/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.9218 - val_loss: 2.8965\n",
      "Epoch 67/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.9169 - val_loss: 2.8964\n",
      "Epoch 68/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.9180 - val_loss: 2.9260\n",
      "Epoch 69/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.9102 - val_loss: 2.9181\n",
      "Epoch 70/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.9147 - val_loss: 2.9154\n",
      "Epoch 71/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.9097 - val_loss: 2.8958\n",
      "Epoch 72/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.9093 - val_loss: 2.8912\n",
      "Epoch 73/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.9124 - val_loss: 2.8935\n",
      "Epoch 74/500\n",
      "48332/48332 [==============================] - 6s 127us/step - loss: 2.9019 - val_loss: 2.9012\n",
      "Epoch 75/500\n",
      "48332/48332 [==============================] - 6s 128us/step - loss: 2.9122 - val_loss: 2.8871\n",
      "Epoch 76/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48332/48332 [==============================] - 6s 129us/step - loss: 2.9127 - val_loss: 2.8933\n",
      "Epoch 77/500\n",
      "48332/48332 [==============================] - 6s 129us/step - loss: 2.9009 - val_loss: 2.8889\n",
      "Epoch 78/500\n",
      "48332/48332 [==============================] - 6s 129us/step - loss: 2.9028 - val_loss: 2.8846\n",
      "Epoch 79/500\n",
      "48332/48332 [==============================] - 6s 128us/step - loss: 2.8962 - val_loss: 2.8851\n",
      "Epoch 80/500\n",
      "48332/48332 [==============================] - 6s 130us/step - loss: 2.9122 - val_loss: 2.8884\n",
      "Epoch 81/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.9021 - val_loss: 2.8881\n",
      "Epoch 82/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.8913 - val_loss: 2.8858\n",
      "Epoch 83/500\n",
      "48332/48332 [==============================] - 6s 130us/step - loss: 2.8977 - val_loss: 2.8929\n",
      "Epoch 84/500\n",
      "48332/48332 [==============================] - 6s 130us/step - loss: 2.9063 - val_loss: 2.8863\n",
      "Epoch 85/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.9008 - val_loss: 2.8838\n",
      "Epoch 86/500\n",
      "48332/48332 [==============================] - 6s 130us/step - loss: 2.9029 - val_loss: 2.8789\n",
      "Epoch 87/500\n",
      "48332/48332 [==============================] - 6s 130us/step - loss: 2.8945 - val_loss: 2.8926\n",
      "Epoch 88/500\n",
      "48332/48332 [==============================] - 6s 130us/step - loss: 2.8961 - val_loss: 2.8764\n",
      "Epoch 89/500\n",
      "48332/48332 [==============================] - 6s 130us/step - loss: 2.8901 - val_loss: 2.8821\n",
      "Epoch 90/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.8940 - val_loss: 2.8801\n",
      "Epoch 91/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.8895 - val_loss: 2.8824\n",
      "Epoch 92/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.8985 - val_loss: 2.8743\n",
      "Epoch 93/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8958 - val_loss: 2.8919\n",
      "Epoch 94/500\n",
      "48332/48332 [==============================] - 7s 142us/step - loss: 2.8927 - val_loss: 2.8706\n",
      "Epoch 95/500\n",
      "48332/48332 [==============================] - 7s 141us/step - loss: 2.8926 - val_loss: 2.8726\n",
      "Epoch 96/500\n",
      "48332/48332 [==============================] - 7s 146us/step - loss: 2.8944 - val_loss: 2.8723\n",
      "Epoch 97/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.8899 - val_loss: 2.8725\n",
      "Epoch 98/500\n",
      "48332/48332 [==============================] - 6s 127us/step - loss: 2.8923 - val_loss: 2.8732\n",
      "Epoch 99/500\n",
      "48332/48332 [==============================] - 6s 125us/step - loss: 2.8856 - val_loss: 2.8713\n",
      "Epoch 100/500\n",
      "48332/48332 [==============================] - 6s 124us/step - loss: 2.8923 - val_loss: 2.8712\n",
      "Epoch 101/500\n",
      "48332/48332 [==============================] - 6s 122us/step - loss: 2.8855 - val_loss: 2.8689\n",
      "Epoch 102/500\n",
      "48332/48332 [==============================] - 6s 126us/step - loss: 2.8907 - val_loss: 2.8800\n",
      "Epoch 103/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8804 - val_loss: 2.8706\n",
      "Epoch 104/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8896 - val_loss: 2.8771\n",
      "Epoch 105/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8786 - val_loss: 2.8652\n",
      "Epoch 106/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8859 - val_loss: 2.8722\n",
      "Epoch 107/500\n",
      "48332/48332 [==============================] - 6s 128us/step - loss: 2.8847 - val_loss: 2.8755\n",
      "Epoch 108/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8741 - val_loss: 2.8633\n",
      "Epoch 109/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8775 - val_loss: 2.8644\n",
      "Epoch 110/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.8793 - val_loss: 2.8621\n",
      "Epoch 111/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8855 - val_loss: 2.8723\n",
      "Epoch 112/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8746 - val_loss: 2.8607\n",
      "Epoch 113/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8803 - val_loss: 2.8639\n",
      "Epoch 114/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8765 - val_loss: 2.8591\n",
      "Epoch 115/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8749 - val_loss: 2.8575\n",
      "Epoch 116/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8743 - val_loss: 2.8767\n",
      "Epoch 117/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8785 - val_loss: 2.8591\n",
      "Epoch 118/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8740 - val_loss: 2.8622\n",
      "Epoch 119/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8734 - val_loss: 2.8586\n",
      "Epoch 120/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8680 - val_loss: 2.8622\n",
      "Epoch 121/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8700 - val_loss: 2.8571\n",
      "Epoch 122/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8799 - val_loss: 2.8707\n",
      "Epoch 123/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8661 - val_loss: 2.8809\n",
      "Epoch 124/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8726 - val_loss: 2.8657\n",
      "Epoch 125/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8740 - val_loss: 2.8698\n",
      "Epoch 126/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8774 - val_loss: 2.8531\n",
      "Epoch 127/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8732 - val_loss: 2.8730\n",
      "Epoch 128/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8762 - val_loss: 2.8511\n",
      "Epoch 129/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8661 - val_loss: 2.8507\n",
      "Epoch 130/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8744 - val_loss: 2.8553\n",
      "Epoch 131/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8732 - val_loss: 2.8507\n",
      "Epoch 132/500\n",
      "48332/48332 [==============================] - 7s 136us/step - loss: 2.8643 - val_loss: 2.8532\n",
      "Epoch 133/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8672 - val_loss: 2.8504\n",
      "Epoch 134/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8678 - val_loss: 2.8757\n",
      "Epoch 135/500\n",
      "48332/48332 [==============================] - 7s 136us/step - loss: 2.8665 - val_loss: 2.8478\n",
      "Epoch 136/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8650 - val_loss: 2.8621\n",
      "Epoch 137/500\n",
      "48332/48332 [==============================] - 7s 136us/step - loss: 2.8649 - val_loss: 2.8471\n",
      "Epoch 138/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8606 - val_loss: 2.8480\n",
      "Epoch 139/500\n",
      "48332/48332 [==============================] - 6s 130us/step - loss: 2.8662 - val_loss: 2.8465\n",
      "Epoch 140/500\n",
      "48332/48332 [==============================] - 7s 140us/step - loss: 2.8649 - val_loss: 2.8549\n",
      "Epoch 141/500\n",
      "48332/48332 [==============================] - 7s 141us/step - loss: 2.8693 - val_loss: 2.8495\n",
      "Epoch 142/500\n",
      "48332/48332 [==============================] - 7s 136us/step - loss: 2.8612 - val_loss: 2.8598\n",
      "Epoch 143/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8688 - val_loss: 2.8640\n",
      "Epoch 144/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8579 - val_loss: 2.8440\n",
      "Epoch 145/500\n",
      "48332/48332 [==============================] - 7s 136us/step - loss: 2.8620 - val_loss: 2.8436\n",
      "Epoch 146/500\n",
      "48332/48332 [==============================] - 7s 138us/step - loss: 2.8662 - val_loss: 2.8434\n",
      "Epoch 147/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8573 - val_loss: 2.8420\n",
      "Epoch 148/500\n",
      "48332/48332 [==============================] - 7s 139us/step - loss: 2.8644 - val_loss: 2.8473\n",
      "Epoch 149/500\n",
      "48332/48332 [==============================] - 7s 136us/step - loss: 2.8602 - val_loss: 2.8463\n",
      "Epoch 150/500\n",
      "48332/48332 [==============================] - 7s 137us/step - loss: 2.8613 - val_loss: 2.8453\n",
      "Epoch 151/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8655 - val_loss: 2.8534\n",
      "Epoch 152/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8621 - val_loss: 2.8408\n",
      "Epoch 153/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.8542 - val_loss: 2.8452\n",
      "Epoch 154/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8595 - val_loss: 2.8448\n",
      "Epoch 155/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8663 - val_loss: 2.8407\n",
      "Epoch 156/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8552 - val_loss: 2.8573\n",
      "Epoch 157/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.8571 - val_loss: 2.8376\n",
      "Epoch 158/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.8539 - val_loss: 2.8683\n",
      "Epoch 159/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8624 - val_loss: 2.8425\n",
      "Epoch 160/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8507 - val_loss: 2.8396\n",
      "Epoch 161/500\n",
      "48332/48332 [==============================] - 7s 140us/step - loss: 2.8531 - val_loss: 2.8398\n",
      "Epoch 162/500\n",
      "48332/48332 [==============================] - 7s 154us/step - loss: 2.8571 - val_loss: 2.8499\n",
      "Epoch 163/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8536 - val_loss: 2.8660\n",
      "Epoch 164/500\n",
      "48332/48332 [==============================] - 6s 129us/step - loss: 2.8526 - val_loss: 2.8411\n",
      "Epoch 165/500\n",
      "48332/48332 [==============================] - 6s 124us/step - loss: 2.8545 - val_loss: 2.8360\n",
      "Epoch 166/500\n",
      "48332/48332 [==============================] - 6s 127us/step - loss: 2.8482 - val_loss: 2.8351\n",
      "Epoch 167/500\n",
      "48332/48332 [==============================] - 7s 144us/step - loss: 2.8558 - val_loss: 2.8487\n",
      "Epoch 168/500\n",
      "48332/48332 [==============================] - 7s 137us/step - loss: 2.8561 - val_loss: 2.8350\n",
      "Epoch 169/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8517 - val_loss: 2.8464\n",
      "Epoch 170/500\n",
      "48332/48332 [==============================] - 7s 136us/step - loss: 2.8541 - val_loss: 2.8353\n",
      "Epoch 171/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8508 - val_loss: 2.8428\n",
      "Epoch 172/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8500 - val_loss: 2.8345\n",
      "Epoch 173/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8533 - val_loss: 2.8399\n",
      "Epoch 174/500\n",
      "48332/48332 [==============================] - 6s 130us/step - loss: 2.8503 - val_loss: 2.8351\n",
      "Epoch 175/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8467 - val_loss: 2.8333\n",
      "Epoch 176/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8416 - val_loss: 2.8356\n",
      "Epoch 177/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8475 - val_loss: 2.8447\n",
      "Epoch 178/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8459 - val_loss: 2.8340\n",
      "Epoch 179/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8398 - val_loss: 2.8287\n",
      "Epoch 180/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8547 - val_loss: 2.8396\n",
      "Epoch 181/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8511 - val_loss: 2.8424\n",
      "Epoch 182/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8471 - val_loss: 2.8592\n",
      "Epoch 183/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8396 - val_loss: 2.8323\n",
      "Epoch 184/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8377 - val_loss: 2.8273\n",
      "Epoch 185/500\n",
      "48332/48332 [==============================] - 7s 136us/step - loss: 2.8465 - val_loss: 2.8291\n",
      "Epoch 186/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8418 - val_loss: 2.8455\n",
      "Epoch 187/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8409 - val_loss: 2.8245\n",
      "Epoch 188/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8458 - val_loss: 2.8341\n",
      "Epoch 189/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8423 - val_loss: 2.8340\n",
      "Epoch 190/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8491 - val_loss: 2.8272\n",
      "Epoch 191/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8438 - val_loss: 2.8249\n",
      "Epoch 192/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8403 - val_loss: 2.8269\n",
      "Epoch 193/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8396 - val_loss: 2.8292\n",
      "Epoch 194/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8517 - val_loss: 2.8312\n",
      "Epoch 195/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8403 - val_loss: 2.8306\n",
      "Epoch 196/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.8395 - val_loss: 2.8245\n",
      "Epoch 197/500\n",
      "48332/48332 [==============================] - 7s 134us/step - loss: 2.8356 - val_loss: 2.8299\n",
      "Epoch 198/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8342 - val_loss: 2.8303\n",
      "Epoch 199/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8398 - val_loss: 2.8257\n",
      "Epoch 200/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8416 - val_loss: 2.8336\n",
      "Epoch 201/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8372 - val_loss: 2.8343\n",
      "Epoch 202/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8411 - val_loss: 2.8254\n",
      "Epoch 203/500\n",
      "48332/48332 [==============================] - 7s 136us/step - loss: 2.8418 - val_loss: 2.8371\n",
      "Epoch 204/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8372 - val_loss: 2.8234\n",
      "Epoch 205/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8360 - val_loss: 2.8241\n",
      "Epoch 206/500\n",
      "48332/48332 [==============================] - 7s 136us/step - loss: 2.8338 - val_loss: 2.8225\n",
      "Epoch 207/500\n",
      "48332/48332 [==============================] - 7s 138us/step - loss: 2.8312 - val_loss: 2.8241\n",
      "Epoch 208/500\n",
      "48332/48332 [==============================] - 7s 136us/step - loss: 2.8396 - val_loss: 2.8310\n",
      "Epoch 209/500\n",
      "48332/48332 [==============================] - 7s 137us/step - loss: 2.8375 - val_loss: 2.8215\n",
      "Epoch 210/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8375 - val_loss: 2.8208\n",
      "Epoch 211/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8372 - val_loss: 2.8254\n",
      "Epoch 212/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8317 - val_loss: 2.8267\n",
      "Epoch 213/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8378 - val_loss: 2.8189\n",
      "Epoch 214/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8329 - val_loss: 2.8264\n",
      "Epoch 215/500\n",
      "48332/48332 [==============================] - 7s 136us/step - loss: 2.8336 - val_loss: 2.8298\n",
      "Epoch 216/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8322 - val_loss: 2.8265\n",
      "Epoch 217/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8325 - val_loss: 2.8230\n",
      "Epoch 218/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8344 - val_loss: 2.8168\n",
      "Epoch 219/500\n",
      "48332/48332 [==============================] - 7s 134us/step - loss: 2.8291 - val_loss: 2.8276\n",
      "Epoch 220/500\n",
      "48332/48332 [==============================] - 7s 136us/step - loss: 2.8247 - val_loss: 2.8233\n",
      "Epoch 221/500\n",
      "48332/48332 [==============================] - 7s 136us/step - loss: 2.8304 - val_loss: 2.8316\n",
      "Epoch 222/500\n",
      "48332/48332 [==============================] - 7s 138us/step - loss: 2.8291 - val_loss: 2.8173\n",
      "Epoch 223/500\n",
      "48332/48332 [==============================] - 7s 136us/step - loss: 2.8334 - val_loss: 2.8190\n",
      "Epoch 224/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8392 - val_loss: 2.8198\n",
      "Epoch 225/500\n",
      "48332/48332 [==============================] - 7s 136us/step - loss: 2.8225 - val_loss: 2.8183\n",
      "Epoch 226/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.8363 - val_loss: 2.8137\n",
      "Epoch 227/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.8306 - val_loss: 2.8196\n",
      "Epoch 228/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8341 - val_loss: 2.8150\n",
      "Epoch 229/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8305 - val_loss: 2.8148\n",
      "Epoch 230/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.8309 - val_loss: 2.8142\n",
      "Epoch 231/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.8244 - val_loss: 2.8348\n",
      "Epoch 232/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8290 - val_loss: 2.8141\n",
      "Epoch 233/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8275 - val_loss: 2.8139\n",
      "Epoch 234/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8325 - val_loss: 2.8128\n",
      "Epoch 235/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.8249 - val_loss: 2.8186\n",
      "Epoch 236/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8184 - val_loss: 2.8141\n",
      "Epoch 237/500\n",
      "48332/48332 [==============================] - 6s 131us/step - loss: 2.8292 - val_loss: 2.8182\n",
      "Epoch 238/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8301 - val_loss: 2.8109\n",
      "Epoch 239/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8227 - val_loss: 2.8087\n",
      "Epoch 240/500\n",
      "48332/48332 [==============================] - 6s 134us/step - loss: 2.8324 - val_loss: 2.8208\n",
      "Epoch 241/500\n",
      "48332/48332 [==============================] - 7s 136us/step - loss: 2.8243 - val_loss: 2.8110\n",
      "Epoch 242/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8206 - val_loss: 2.8096\n",
      "Epoch 243/500\n",
      "48332/48332 [==============================] - 7s 135us/step - loss: 2.8204 - val_loss: 2.8149\n",
      "Epoch 244/500\n",
      "48332/48332 [==============================] - 6s 130us/step - loss: 2.8248 - val_loss: 2.8146\n",
      "Epoch 245/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8185 - val_loss: 2.8398\n",
      "Epoch 246/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8264 - val_loss: 2.8094\n",
      "Epoch 247/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8250 - val_loss: 2.8122\n",
      "Epoch 248/500\n",
      "48332/48332 [==============================] - 6s 133us/step - loss: 2.8251 - val_loss: 2.8098\n",
      "Epoch 249/500\n",
      "48332/48332 [==============================] - 6s 132us/step - loss: 2.8212 - val_loss: 2.8094\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val),\n",
    "                    epochs=500, shuffle=True, batch_size=128, callbacks=[es], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array(y_test[\"total_points\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_scaled);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'Actual': y_true.flatten(), 'Predicted': y_pred.flatten()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[\"in_range\"] = pred_df.apply(lambda row: True if \n",
    "                                    (row[\"Actual\"] - row[\"Predicted\"]) <= 1 else False, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In range: 0.88\n"
     ]
    }
   ],
   "source": [
    "print(\"In range: {0:.2f}\".format(pred_df[pred_df[\"in_range\"] == True].shape[0]/pred_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>in_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>17.750000</td>\n",
       "      <td>2.539378</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>11.250000</td>\n",
       "      <td>2.161139</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5731</th>\n",
       "      <td>11.250000</td>\n",
       "      <td>1.833284</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2192</th>\n",
       "      <td>11.250000</td>\n",
       "      <td>1.756652</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.624561</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5130</th>\n",
       "      <td>9.500000</td>\n",
       "      <td>2.888612</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3501</th>\n",
       "      <td>9.500000</td>\n",
       "      <td>2.485424</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3896</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.865676</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>8.333333</td>\n",
       "      <td>2.903783</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3522</th>\n",
       "      <td>8.250000</td>\n",
       "      <td>2.486121</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4917</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.705496</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3343</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.189545</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.455125</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.394060</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2947</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.605904</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.598819</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3527</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.372150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5383</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.845011</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.634088</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.318590</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6182</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.082910</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.032830</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3306</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.183168</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3675</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.107722</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.911185</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3674</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.888502</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3736</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.750052</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.236531</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5864</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.828312</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.220313</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2584</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472178</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435257</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2577</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.247777</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2618</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562062</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.573687</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.673126</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2574</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.602252</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500470</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2572</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.643798</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2570</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.597512</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481318</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.553756</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.590853</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2606</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481028</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2616</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.471241</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2615</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.519744</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2613</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.486732</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370704</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.789117</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.957345</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2605</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262162</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2602</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.060205</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.116401</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.780946</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2598</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.224651</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2596</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622777</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3107</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.701312</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4740</th>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.240277</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4486</th>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.303681</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5539</th>\n",
       "      <td>-0.500000</td>\n",
       "      <td>2.366282</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6215 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Actual  Predicted  in_range\n",
       "2608  17.750000   2.539378     False\n",
       "1748  11.250000   2.161139     False\n",
       "5731  11.250000   1.833284     False\n",
       "2192  11.250000   1.756652     False\n",
       "1823  10.500000   0.624561     False\n",
       "5130   9.500000   2.888612     False\n",
       "3501   9.500000   2.485424     False\n",
       "3896   9.000000   2.865676     False\n",
       "1507   8.333333   2.903783     False\n",
       "3522   8.250000   2.486121     False\n",
       "4917   8.000000   2.705496     False\n",
       "3343   8.000000   2.189545     False\n",
       "138    8.000000   2.455125     False\n",
       "2453   8.000000   0.394060     False\n",
       "2947   8.000000   0.605904     False\n",
       "3400   8.000000   2.598819     False\n",
       "3527   8.000000   2.372150     False\n",
       "5383   8.000000   2.845011     False\n",
       "2281   8.000000   2.634088     False\n",
       "1387   8.000000   2.318590     False\n",
       "6182   8.000000   2.082910     False\n",
       "3081   8.000000   2.032830     False\n",
       "3306   8.000000   2.183168     False\n",
       "3675   8.000000   2.107722     False\n",
       "2370   8.000000   1.911185     False\n",
       "3674   8.000000   2.888502     False\n",
       "3736   8.000000   0.750052     False\n",
       "4228   8.000000   2.236531     False\n",
       "5864   8.000000   1.828312     False\n",
       "1289   8.000000   3.220313     False\n",
       "...         ...        ...       ...\n",
       "2584   0.000000   0.472178      True\n",
       "2583   0.000000   0.435257      True\n",
       "2577   0.000000   0.247777      True\n",
       "2618   0.000000   0.562062      True\n",
       "2576   0.000000   0.573687      True\n",
       "2575   0.000000   0.673126      True\n",
       "2574   0.000000   0.602252      True\n",
       "2573   0.000000   0.500470      True\n",
       "2572   0.000000   0.643798      True\n",
       "2570   0.000000   0.597512      True\n",
       "2591   0.000000   0.481318      True\n",
       "2592   0.000000   0.553756      True\n",
       "2593   0.000000   0.590853      True\n",
       "2606   0.000000   0.481028      True\n",
       "2616   0.000000   0.471241      True\n",
       "2615   0.000000   0.519744      True\n",
       "2613   0.000000   0.486732      True\n",
       "2611   0.000000   0.370704      True\n",
       "2595   0.000000   0.789117      True\n",
       "2609   0.000000   1.957345      True\n",
       "2605   0.000000   0.262162      True\n",
       "2602   0.000000   2.060205      True\n",
       "2601   0.000000   2.116401      True\n",
       "2599   0.000000   0.780946      True\n",
       "2598   0.000000   3.224651      True\n",
       "2596   0.000000   0.622777      True\n",
       "3107   0.000000   0.701312      True\n",
       "4740  -0.333333   0.240277      True\n",
       "4486  -0.500000   0.303681      True\n",
       "5539  -0.500000   2.366282      True\n",
       "\n",
       "[6215 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.sort_values([\"Actual\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>in_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.410312</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.345942</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.167372</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.254446</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.455655</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.446317</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.506247</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.491785</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.131821</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.634412</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.731798</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.182044</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.412735</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.238803</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.327410</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.355342</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.539669</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.424788</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.441315</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.237771</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.953036</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.491679</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.263645</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609788</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.111869</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual  Predicted  in_range\n",
       "0      0.0   0.410312      True\n",
       "1      2.0   2.345942      True\n",
       "2      0.0   1.167372      True\n",
       "3      2.0   2.254446      True\n",
       "4      0.0   0.455655      True\n",
       "5      1.0   2.446317      True\n",
       "6      0.0   0.506247      True\n",
       "7      0.0   0.491785      True\n",
       "8      0.0   1.131821      True\n",
       "9      0.0   0.634412      True\n",
       "10     2.0   2.731798      True\n",
       "11     2.0   2.182044      True\n",
       "12     0.0   0.412735      True\n",
       "13     0.0   0.238803      True\n",
       "14     1.0   1.327410      True\n",
       "15     0.0   0.355342      True\n",
       "16     0.0   0.539669      True\n",
       "17     3.0   2.424788      True\n",
       "18     1.0   3.441315      True\n",
       "19     0.0   0.237771      True\n",
       "20     1.0   1.953036      True\n",
       "21     0.0   0.491679      True\n",
       "22     2.0   2.263645      True\n",
       "23     0.0   0.609788      True\n",
       "24     5.0   2.111869     False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pred_df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAHYCAYAAABQudw1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XuYXWVhLvD3k4DhElHiDaUlVIu0hRJIbIlIjVoVtYeWUo7aVsV6mvbhQXh6qoLSngynKHjgeDltxdKi8VJCPXhB8ZZyYEqtF0wwChK5aGONqaKgESpYwO/8sXfiZDJJvpk9azI7+f2eZz/Zs/aad39Zs2fPu7+19tql1hoAAHbsYbt6AAAAw0BpAgBooDQBADRQmgAAGihNAAANlCYAgAZKEwBAA6UJAKCB0gQA0GBOF6GPfvSj64IFC5rWvetHd2X+fvOnfQzDlttl9rDldpk9bLldZg9bbpfZw5bbZbbc7rOHLbfL7NmSu2bNmu/VWh+z0xVrrdN+WbRoUW21/LrlzetOxrDldpk9bLldZg9bbpfZw5bbZfaw5XaZLbf77GHL7TJ7tuQmWV0b+o3dcwAADZQmAIAGShMAQINODgSfyAMPPJANGzbk/vvv32r58w58XtatWzft9zdsua3Zc+fOzSGHHJK99967kzEAABObsdK0YcOGzJs3LwsWLEgpZcvyjfdszBPmPWHa72/Ycluya6256667smHDhhx22GGdjAEAmNiM7Z67//77M3/+/K0KE5NTSsn8+fO3ma0DALo3o8c0KUyDsw0BYNdwIDgAQIMZO6ZpvAXnfGzMV18cOG/9hS9sWu/DH/5wTj755Kxbty5HHHHEdtdbsWJFnvvc5+YJT5ja8Uujo6O5+OKLc/XVV0/p+wGA2WWPm2lauXJlnv70p2flypU7XG/FihXZuHHjDI0KAJjt9qjSdO+99+bTn/50LrvsslxxxRVblr/pTW/KUUcdlaOPPjrnnHNOrrzyyqxevTq/93u/l4ULF+a+++7LggUL8r3vfS9Jsnr16ixdujRJcsMNN2TJkiU55phj8rSnPS233nrrrvivAQAd22W753aFq666KieeeGIOP/zwzJ8/P2vWrMmdd96Zq666Kp///Oez33775e67785BBx2Uv/qrv8rFF1+cxYsX7zDziCOOyD//8z9nzpw5ueaaa/L6178+H/jAB2bofwQAzJQ9qjStXLkyZ511VpLkxS9+cVauXJlaa17xildkv/32S5IcdNBBk8rctGlTXv7yl+f2229PKSUPPPDAtI8bANj19pjSdPfdd+faa6/NTTfdlFJKHnrooZRScuqppzZ9/5w5c/KTn/wkSbY6T9Kf//mf55nPfGY+9KEPZf369Vt22wEAu5c95pimK6+8Mi996UvzjW98I+vXr883v/nNHHbYYTnwwAPzrne9Kz/60Y+S9MpVksybNy/33HPPlu9fsGBB1qxZkyRb7X7btGlTnvjEJybpHTwOAOyemmaaSinrk9yT5KEkD9Zad3ygT4PNpwjo8mNJxlq5cmXOPvvsrZadcsopWbduXU466aQsXrw4++yzT17wghfkjW98Y0477bT88R//cfbdd9989rOfzfLly/PKV74y+x6wb57zrOdsyXjta1+bl7/85Tn//PPzwhe2nfYAABg+k9k998xa6/c6G0nHrrvuum2WnXnmmVuun3POOVvddsopp+SUU07Z8vUJJ5yQ2267bZuSt2TJktx2221bvj7//POTJEuXLrWrDgB2I3vM7jkAgEG0lqaaZFUpZU0pZVmXAwIAmI1KrXXnK5XyxFrrt0opj03yj0leVWu9ftw6y5IsS5L5B89fdMblZ2yV8bwDn5dDn3zoNtn3/PiezHv4vKn/D7Zj2HInk/2NO76RT236VHPu6PrRLF2wdICRzWxul9nDlttl9rDldpk9bLldZsvtPnvYcrvMno7ct15z2zbLFj5546Ryz3vmeWuajteutU7qkmQkyat3tM6iRYvqeLfccss2y2qt9Vs//NaEywc1bLmTyd7ettye5dctn8Jodl1ul9nDlttl9rDldpk9bLldZsvtPnvYcrvMno7cQ8++epvLZHOTrK4NHWinu+dKKfuXUuZtvp7kuUlubq5vAAC7gZZ3zz0uyYdKKZvXv7zW+slORwUAMMvstDTVWr+e5Ohpv+eRA5Mk03aGppFNO11lr732ylFHHZUHH3wwv/ALv5B3v/vdWz4+ZbJGR0dz8cUX5+qrr85HPvKR3HLLLductmCzH/zgB7n88stz+umnT+o+RkZGcsABB+TVr371lMYIAEyfPeqUA/vuu2/Wrl2bm2++Ofvss0/e8Y53bHV7rXXLR6VMxkknnbTdwpT0StPb3/72SecCALPHHlWaxjrhhBNyxx13ZP369XnKU56Sl73sZTnyyCPzzW9+M6tWrcqSJUty7LHH5tRTT829996bJPnkJz+ZX1v0azn22GPzwQ9+cEvWihUrcsYZvXcLfuc738nJJ5+co48+OkcffXQ+85nP5JxzzsnXvva1LFy4MK95zWuSJBdddFGe+tSn5pd/+ZezfPnyLVlveMMbcvjhh+fpT396br311hncIgDAjuyRpenBBx/MJz7xiRx11FFJkttvvz2nn356vvKVr2T//ffP+eefn2uuuSY33nhjFi9enDe/+c25//7784d/+IdZ8Q8rsmbNmnz729+eMPvMM8/MM57xjHzpS1/KjTfemF/6pV/KhRdemCc96UlZu3ZtLrrooqxatSq33357brjhhqxduzZr1qzJ9ddfny9/8cu54oorsnbt2nz84x/PF77whZncLADADkzmY1SG3n333ZeFCxcm6c00vfKVr8zGjRtz6KGH5rjjjkuSfO5zn8stt9yS448/Pknyn//5n1myZEm++tWv5rDDDsvPPfnnUkrJ7//+7+fSSy/d5j6uvfbavOc970nSO4bqwAMPzPe///2t1lm1alVWrVqVY445Jkly77335vbbb8+G727IySefvOU4q5NOOqmbDQEATNoeVZo2H9M03v7777/leq01z3nOc7Jy5cqt1pno+6aq1prXve51+aM/+qOtlp934XnTdh8AwPTaI3fP7chxxx2Xf/mXf8kdd9yRJPmP//iP3HbbbTniiCOyfv36rP/6+iTZplRt9uxnPzuXXHJJkuShhx7Kpk2bMm/evNxzzz1b1nne856Xd77znVuOlfrWt76VO++8M8cdf1w+/OEP57777ss999yTj370ox3+TwGAydh1M039UwRsvGdjnjBv2k48MLDHPOYxWbFiRV7ykpfkxz/+cZLk/PPPz+GHH55LL700Lzv1ZXnEAY/ICSecsFUR2uxtb3tbli1blssuuyx77bVXLrnkkixZsiTHH398jjzyyDz/+c/PRRddlHXr1mXJkiVJkgMOOCDve9/7ctTCo/KiF70oRx99dB772MfmqU996oz+3wGA7dujds9tntkZa8GCBbn55q1PcP6sZz1rwoOwTzzxxFy/5vptSt5pp52W0047LUnyuMc9LlddddU233v55Zdv9fVZZ52Vs846a6tlG+/ZmHPPPTfnnntu0/8HAJg5ds8BADRQmgAAGsxoaep9kDCDsA0BYNeYsdI0d+7c3HXXXf7oD6DWmrvuuitz587d1UMBgD3OjB0Ifsghh2TDhg357ne/u9XyH9z/g2yau/MP252sYcttzZ47d24OOeSQTu4fANi+GStNe++9dw477LBtlo+MjmTkmJFpv79hy+06GwAYjAPBAQAaKE0AAA2UJgCABkoTAEADpQkAoIHSBADQQGkCAGigNAEANFCaAAAaKE0AAA2UJgCABkoTAEADpQkAoIHSBADQQGkCAGigNAEANFCaAAAaKE0AAA2UJgCABkoTAEADpQkAoIHSBADQQGkCAGigNAEANFCaAAAaKE0AAA2UJgCABkoTAEADpQkAoIHSBADQQGkCAGigNAEANFCaAAAaKE0AAA2UJgCABkoTAEADpQkAoIHSBADQQGkCAGigNAEANFCaAAAaKE0AAA2UJgCABkoTAEADpQkAoIHSBADQQGkCAGigNAEANFCaAAAaKE0AAA2UJgCABkoTAEADpQkAoIHSBADQQGkCAGigNAEANFCaAAAaKE0AAA2UJgCABkoTAECD5tJUStmrlPLFUsrVXQ4IAGA2msxM01lJ1nU1EACA2aypNJVSDknywiR/1+1wAABmp9aZprcmeW2Sn3Q4FgCAWavUWne8Qim/keQFtdbTSylLk7y61vobE6y3LMmyJJl/8PxFZ1x+RtMARtePZumCpZMc9u6X22X2sOV2mT1suV1mD1tul9nDlttlttzus4ctt8vs6ch96zW3bbNs4ZM3Tir3vGeet6bWuninK9Zad3hJckGSDUnWJ/l2kh8led+OvmfRokW11fLrljevOxnDlttl9rDldpk9bLldZg9bbpfZw5bbZbbc7rOHLbfL7OnIPfTsq7e5TDY3yeq6kz5Ua9357rla6+tqrYfUWhckeXGSa2utv99c3wAAdgPO0wQA0GDOZFautY4mGe1kJAAAs5iZJgCABkoTAEADpQkAoIHSBADQQGkCAGigNAEANFCaAAAaKE0AAA2UJgCABkoTAEADpQkAoIHSBADQQGkCAGigNAEANFCaAAAaKE0AAA2UJgCABkoTAEADpQkAoIHSBADQQGkCAGigNAEANFCaAAAaKE0AAA2UJgCABkoTAEADpQkAoIHSBADQQGkCAGigNAEANFCaAAAaKE0AAA2UJgCABkoTAEADpQkAoIHSBADQQGkCAGigNAEANFCaAAAaKE0AAA2UJgCABnN29QAAYMpGL0hG37Lt8pFNMz8WdntmmgAAGihNAAANlCYAgAZKEwBAA6UJAKCB0gQA0EBpAgBooDQBADRQmgAAGihNAAANlCYAgAZKEwBAA6UJAKCB0gQA0EBpAgBooDQBADRQmgAAGihNAAANlCYAgAZKEwBAA6UJAKCB0gQA0EBpAgBooDQBADRQmgAAGihNAAANlCYAgAZKEwBAA6UJAKCB0gQA0EBpAgBooDQBADTYaWkqpcwtpdxQSvlSKeUrpZTzZmJgAACzyZyGdX6c5Fm11ntLKXsn+XQp5RO11s91PDYAgFljp6Wp1lqT3Nv/cu/+pXY5KACA2abpmKZSyl6llLVJ7kzyj7XWz3c7LACA2aX0JpIaVy7lkUk+lORVtdabx922LMmyJJl/8PxFZ1x+RlPm6PrRLF2wtHkMrYYtt8vsYcvtMnvYcrvMHrbcLrOHLbfL7OnIfes1t22zbOGTN3Yz3tG/yNKJdposfd3g2bN4G89kbpfZs+Xxdt4zz1tTa1280xVrrZO6JPkfSV69o3UWLVpUWy2/bnnzupMxbLldZg9bbpfZw5bbZfaw5XaZPWy5XWZPR+6hZ1+9zaWz8S7fp9blj9j2Mh3Zs3gbz2Rul9mz5fGWZHVt6EAt7557TH+GKaWUfZM8J8lXm+sbAMBuoOXdcwcneXcpZa/0joF6f6316m6HBQAwu7S8e+7LSY6ZgbEAAMxazggOANBAaQIAaKA0AQA0UJoAABooTQAADZQmAIAGShMAQAOlCQCggdIEANBAaQIAaKA0AQA0UJoAABooTQAADZQmAIAGShMAQAOlCQCggdIEANBgzq4eALCLjF6QjL5l2+Ujm2Z+LABDwEwTAEADpQkAoIHSBADQQGkCAGigNAEANFCaAAAaKE0AAA2UJgCABkoTAEADpQkAoIHSBADQQGkCAGigNAEANFCaAAAaKE0AAA2UJgCABkoTAEADpQkAoIHSBADQQGkCAGigNAEANFCaAAAazNnVA2CIjV6QjL5l2+Ujm2Z+LADQMTNNAAANzDQB0D0z0+wGzDQBADRQmgAAGihNAAANlCYAgAZKEwBAA6UJAKDB7nvKAW9vBQCmkZkmAIAGShMAQAOlCQCggdIEANBAaQIAaKA0AQA02H1POTCMJjpNglMkALCr+fuUxEwTAEATpQkAoIHSBADQQGkCAGigNAEANPDuOWB6+bBsYDdlpgkAoIHSBADQQGkCAGigNAEANFCaAAAaePccALB76ehdvGaaAAAaKE0AAA2UJgCABkoTAEADpQkAoMFOS1Mp5WdKKdeVUm4ppXyllHLWTAwMAGA2aTnlwINJ/rTWemMpZV6SNaWUf6y13tLx2AAAZo2dzjTVWv+91npj//o9SdYleWLXAwMAmE1KrbV95VIWJLk+yZG11h+Ou21ZkmVJMv/g+YvOuPyMpszR9aNZumBp8xjGe+s1t024fOGcf8jSiSbSlr5uyveVDD7eZJJjHnC8yfSMecLc0b/oZBsnHY55yHKnI7vL35GJsrv63UuG7+c3mx8XXebO5OPC89D05g7j36fpeLyd98zz1tRaF+/svppLUynlgCT/lOQNtdYP7mjdxYsX19WrVzfljoyOZGTpSNO6E1lwzscmXH7a3N/OSOZOcIeDnQ100PEmkxzzgONNpmfME+aOPLyTbZx0OOYhy52O7C5/RybK7up3Lxm+n99sflx0mTuTjwvPQ9ObO4x/n6bj8VZKaSpNTe+eK6XsneQDSf5+Z4UJAGB31PLuuZLksiTraq1v7n5IAACzT8u7545P8tIkN5VS1vaXvb7W+vHuhgUAu6mOPkyW7u20NNVaP52kzMBYAABmLWcEBwBooDQBADRQmgAAGihNAAANlCYAgAZKEwBAA6UJAKCB0gQA0EBpAgBooDQBADRQmgAAGihNAAANlCYAgAZKEwBAA6UJAKCB0gQA0EBpAgBooDQBADRQmgAAGihNAAANlCYAgAZKEwBAA6UJAKCB0gQA0EBpAgBooDQBADRQmgAAGihNAAANlCYAgAZKEwBAA6UJAKCB0gQA0EBpAgBooDQBADRQmgAAGihNAAANlCYAgAZKEwBAA6UJAKCB0gQA0EBpAgBooDQBADRQmgAAGihNAAANlCYAgAZKEwBAA6UJAKCB0gQA0EBpAgBooDQBADSYs6sHALDbGr0gGX3LtstHNs38WICBmWkCAGigNAEANFCaAAAaKE0AAA2UJgCABkoTAEADpQkAoIHSBADQQGkCAGigNAEANFCaAAAaKE0AAA18YO9k+QBOANgjmWkCAGigNAEANLB7DqaD3bYAuz0zTQAADZQmAIAGShMAQAOlCQCggdIEANBAaQIAaKA0AQA02GlpKqW8s5RyZynl5pkYEADAbNQy07QiyYkdjwMAYFbbaWmqtV6f5O4ZGAsAwKw1bR+jUkpZlmRZksw/eH5GRkeavm90/WjzuhP5wZzbJs7NgxnJ/RPcMPX7mq7cSY15wPEmg2/jJHnrNduOeeGcbrZxMj1jntHcjh5vyez+HZkoezZviy5z/Y78VFePi2HcxjM55q5+dsns/vs0k89Dpda685VKWZDk6lrrkS2hixcvrqtXr24awMjoSEaWjjStO5EF53xswuWnzf3tjGTuBHc42GeBjYw8fODcSY15Gj67bNBtnEw85q62cTI9Y57R3Gl4XGw3exb/jnhc/JRt8VNdbYth3MYzOeauxpvs/n+fSilraq2Ld3Zf3j0HANBAaQIAaNByyoGVST6b5CmllA2llFd2PywAgNllpweC11pfMhMDAQCYzeyeAwBooDQBADRQmgAAGihNAAANlCYAgAZKEwBAA6UJAKCB0gQA0EBpAgBooDQBADRQmgAAGihNAAANlCYAgAZKEwBAA6UJAKCB0gQA0EBpAgBooDQBADRQmgAAGihNAAANlCYAgAZKEwBAA6UJAKCB0gQA0GDOrh4AsBOjFySjb9l62cimXTMWgD2YmSYAgAZKEwBAA7vnAIBdY6LDD5JZewiCmSYAgAZKEwBAA6UJAKCB0gQA0MCB4AD8lPOCwXaZaQIAaKA0AQA0sHuO2ckuAgBmGaWJPcuQnUgNoJnnt87ZPQcA0EBpAgBoYPfcnsCULQAMzEwTAEADpQkAoIHSBADQYNcf0+R8PEArzxfALmSmCQCggdIEANBAaQIAaKA0AQA02PUHggMweQ6KhxlnpgkAoIHSBADQQGkCAGigNAEANFCaAAAaKE0AAA2UJgCABkoTAEADJ7cEcKJIJuJxwThmmgAAGihNAAANlCYAgAZKEwBAA6UJAKCB0gQA0EBpAgBooDQBADRQmgAAGihNAAANlCYAgAZKEwBAA6UJAKCB0gQA0EBpAgBooDQBADRoKk2llBNLKbeWUu4opZzT9aAAAGabnZamUspeSf46yfOT/GKSl5RSfrHrgQEAzCYtM02/kuSOWuvXa63/meSKJL/Z7bAAAGaXUmvd8Qql/E6SE2ut/63/9UuT/Gqt9Yxx6y1Lsqz/5VOS3No4hkcn+d5kBr2b5naZPWy5XWYPW26X2cOW22X2sOV2mS23++xhy+0ye7bkHlprfczOVpoz9fFsrdZ6aZJLJ/t9pZTVtdbF0zWOYc3tMnvYcrvMHrbcLrOHLbfL7GHL7TJbbvfZw5bbZfaw5bbsnvtWkp8Z8/Uh/WUAAHuMltL0hSQ/X0o5rJSyT5IXJ/lIt8MCAJhddrp7rtb6YCnljCSfSrJXknfWWr8yjWOY9C693TS3y+xhy+0ye9hyu8wettwus4ctt8tsud1nD1tul9lDlbvTA8EBAHBGcACAJkoTAEADpQkAoMG0naepRSnliPTOJv7E/qJvJflIrXXdTI5jMvpjfmKSz9da7x2z/MRa6ycHyP2VJLXW+oX+x9KcmOSrtdaPDzzore/nPbXWl01nZj/36emdLf7mWuuqAXJ+Ncm6WusPSyn7JjknybFJbknyxlrrpgGyz0zyoVrrN6easZ3cze8i3VhrvaaU8rtJnpZkXZJLa60PDJD9c0l+O73TfDyU5LYkl9dafzj4yAH2DKWUx9Za75zu3BmbaSqlnJ3eR7CUJDf0LyXJyi4/BLiU8ooBvvfMJFcleVWSm0spYz8+5o0D5C5P8n+SXFJKuSDJXyXZP8k5pZRzB8j9yLjLR5P89uavp5rbz75hzPU/7I95XpLlA/783pnkR/3rb0tyYJI39Ze9a4DcJPmLJJ8vpfxzKeX0UspOz/ba6F1JXpjkrFLKe5OcmuTzSZ6a5O+mGtp/vL0jydx+1sPTK0+fK6UsHXDM7AKllMfu6jFMVill/q4ew+6klHJgKeXCUspXSyl3l1LuKqWs6y97ZEf3+YkBv/8RpZQLSinv7b8oHHvb2wfIfXwp5ZJSyl+XUuaXUkZKKTeVUt5fSjl4gNyDxl3mJ7mhlPKoUspBU82dUK11Ri7pvWLee4Ll+yS5vcP7/bcBvvemJAf0ry9IsjrJWf2vvzhg7l5J9kvywySP6C/fN8mXB8i9Mcn7kixN8oz+v//ev/6MAbfjF8dc/0KSx/Sv75/kpgFy140d/7jb1g465vReGDw3yWVJvpvkk0lenmTeALlf7v87J8l3kuzV/7oM+PO7aUzWfklG+9d/dpDHWz/jwCQXJvlqkruT3JXezNiFSR45SPYO7vMTA37/I5JckOS9SX533G1vHyD38UkuSe+DyOcnGelv+/cnOXiA3IPGXeYnWZ/kUUkOGnBbnDjuZ3lZki8nuTzJ4wbIvTDJo/vXFyf5epI7knxjkOeM/nPRnyV5UgePq8VJrus/1/1Mkn9Msqn/vHTMALkHJPmfSb7Sz/tuks8lOW3A8X4qydlJHj/uMXh2klUD5B67ncuiJP8+4Jg/0H9s/FZ652X8QJKHb/7ZDpD7yfQmIc7pP37P7v8MX5XkqgFyf5LkX8ddHuj/+/XpfPzN5O65nyR5Qnq/jGMd3L9tykopX97eTUkeN0D0w2p/l1ytdX3/1f6VpZRD+9lT9WCt9aEkPyqlfK32d73UWu8rpQyyLRYnOSvJuUleU2tdW0q5r9b6TwNkbvawUsqj0ishpdb63SSptf5HKeXBAXJvLqW8otb6riRfKqUsrrWuLqUcnt6DfhC11vqTJKuSrCql7J3k+UlekuTiJFOdeXpYfxfd/umVmwPTKyIPT7L3gGOek95uuYen9ySeWuu/9cc+iPcnuTbJ0lrrt5Peq770CuT70yuWk1ZKOXZ7NyVZOJXMMd6V5Pb0nrD/oJRySnrl6cdJjhsgd0WSj6X387suyd8neUF6fyDekal/IPn3su3z2xPTKxA1yc9NMTfpzWxvPhzgf6f3Yui/pLcr92/SG/tUvLDWunmm+KIkL6q9QwYOT6+QTfVjKB6V5JFJriulfDvJyiT/UGvdOMW8sd6eZHk//zNJ/qTW+pxSyrP7ty2ZYu7fJ/lQkucl+a/pPT6uSPJnpZTDa62vn2Luglrrm8Yu6P8OvqmU8gdTzEx6JfGfMvHfokFnsJ5Uaz2lf/3D/T0g15ZSThow93G11r9MklLK6WO2y1+WUl45QO5rkjwnvb97N/Xz/7XWethgw53AdDawnTTBE9N7BfOJ9E46dWl6TwJ3ZMyrqClmfye9J+hDx10WpHfcyVRzr02ycNyyOUnek+ShAXI/n2S//vWHjVl+YAZo8WNyDknyf9PbhTblmbZxmevTexX6r/1/D+4vPyADzAj1/88rknytv10e6Of/U5KjBxzzdmdnNm//Keb+SX+M30hyZpL/l+Rv05utWD5A7lnpvfr62/RmhF7RX/6YJNcPuC1uncptDbkP9X9Prpvgct+AY1477utzk/xLejM4g7zaHTtr+m87us9J5v5p/zntqDHL/nWQbTAm58Yx18dvl0HGvC7JnP71z427bZAZ5LHjPSG9MvPt/uNi2YDbYkc/v0H2AHxp3Ndf6P/7sPSON51q7qokr82YGcH0XsyfneSaAXJvTvLz27ntmwNu43UZ87epv+y09GbhvjEd2zjJ+dP1eOt//+a/e29O79CRaZ1h2nI/XYTu4D/1sPReIZ7SvxyX/u6IAXMvS/L07dx2+YA/hMdv57bjB8h9+HaWP3rsE+40bJcXpncwdZc/0/2SHDYNOY9IcnR6U8tT3t0wLvPwDv/fT0jyhP71Ryb5nSS/Mg25v9TPOmKax+uJ+6cZQ/fEnWRDkv+eXjH7evonJu7fNsgu4Vf1HxvPSm835dvS251/XpL3DpC7TalPudxYAAACOklEQVRN75CEE5O8a8Bt8dn0ZkZPTe+Fy2/1lz8jyeoBcj+z+e9IkpOSfGrMbYO8sHhUesdpfjXJ99OblV7XXzbl3bb954mnbOe23xpwG/+vJL8+wfITM8DhNOnt/jxgguVPTnLlIGMek3VSertVvz0dedvkdxHq4uIyuy7jnrjvHvfE/agBcj1xT3wf0/rEnd7uqLGXzccUPj7JewbMXprkH9I7BvCmJB9Psiz9GagpZl4xHf/v7WQfnd5xQp9IckS/6P0gvTL9tAFyfzm9Nyh9P8mn03/Rld5M75kDjvmIJL8+/nGXwfeyHJHk2dOdu5Ps58/GMY/NTe/44COna1tsdT/TGebi4jJ8l/R3Aw5L7rCMedwT9x69Lfbkx0V6u/BvTfLh9A5z+M0xtw2ym7mT3P73v6qjMXeV29m2GH/x2XOwhyul/Fut9WeHJbfL7GHL7TJ72HK7zB4kt5RyU5IltdZ7SykLklyZ3q7Pt5VSvlhrPWY25Q7jmLvcFuPN6MktgV2jq3eYdvjO1aEbs23RfW6X2UP4LuyucodxzF1ui60oTbBneFx6b6X+/rjlJb0DYGdbbpfZw5bbZfaw5XaZ3VXud0opC2uta5OkPxvyG+md2PeoWZg7jGPucltsRWmCPcPV6R14uXb8DaWU0VmY22X2sOV2mT1suV1md5X7siRbncuu1vpgkpeVUv5mFuZ2mT1sudtwTBMAQIMZ++w5AIBhpjQBADRQmgAAGihNAAANlCYAgAb/H5v1ws8yarbLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df.plot(kind='bar',figsize=(10,8))\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 1.0046572461568208\n",
      "R2 score: 0.26065893405899865\n",
      "Mean Squared Error: 2.4957033070207864\n",
      "Root Mean Squared Error: 1.5797795121537646\n"
     ]
    }
   ],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('R2 score:', metrics.r2_score(y_test, y_pred))\n",
    "\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_id_df = pd.read_csv(\"player_id_map.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_id_df = pd.read_csv(\"team_id_map.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and the test weekly fpl scrape\n",
    "gw=7\n",
    "weekly_scrape = pd.read_csv(\"gw{}-fpl-data.csv\".format(gw));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "weekly_scrape_scaled = scaler.transform(weekly_scrape[X_test.columns]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_weekly = model.predict(weekly_scrape_scaled);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_weekly_df = pd.DataFrame({'player_id': list(weekly_scrape[\"player_id\"]), \n",
    "                                 'position': list(weekly_scrape[\"position\"]),\n",
    "                                 'predicted': y_pred_weekly.flatten()});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_names = [];\n",
    "opponent_names = [];\n",
    "\n",
    "for index, row in weekly_scrape.iterrows():\n",
    "    player_id = row[\"player_id\"];\n",
    "    opponent_id = row[\"opponent_id\"];\n",
    "    \n",
    "    player_names.append(player_id_df[player_id_df[\"player_id\"] == player_id][\"actual_name\"].iloc[0]);\n",
    "    opponent_names.append(team_id_df[team_id_df[\"team_id\"] == opponent_id][\"team_name\"].iloc[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_weekly_df[\"player_name\"] = player_names;\n",
    "y_pred_weekly_df[\"opponent\"] = opponent_names;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_weekly_df = y_pred_weekly_df[[\"player_id\", \"position\", \"player_name\", \"opponent\", \"predicted\"]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_id</th>\n",
       "      <th>position</th>\n",
       "      <th>player_name</th>\n",
       "      <th>opponent</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>385</td>\n",
       "      <td>MID</td>\n",
       "      <td>Kevin De Bruyne</td>\n",
       "      <td>Everton</td>\n",
       "      <td>3.586869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>514</td>\n",
       "      <td>DEF</td>\n",
       "      <td>NicolÃ¡s Otamendi</td>\n",
       "      <td>Everton</td>\n",
       "      <td>2.996146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>858</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Pierre-Emerick Aubameyang</td>\n",
       "      <td>Manchester United</td>\n",
       "      <td>2.766384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>648</td>\n",
       "      <td>DEF</td>\n",
       "      <td>Trent Alexander-Arnold</td>\n",
       "      <td>Sheffield United</td>\n",
       "      <td>2.700203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176</td>\n",
       "      <td>GK</td>\n",
       "      <td>David de Gea</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>2.552576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1154</td>\n",
       "      <td>DEF</td>\n",
       "      <td>John Lundstram</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>2.354345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>977</td>\n",
       "      <td>DEF</td>\n",
       "      <td>Issa Diop</td>\n",
       "      <td>Bournemouth</td>\n",
       "      <td>2.343517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>267</td>\n",
       "      <td>MID</td>\n",
       "      <td>Harry Wilson</td>\n",
       "      <td>West Ham United</td>\n",
       "      <td>2.217601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1206</td>\n",
       "      <td>MID</td>\n",
       "      <td>Todd Cantwell</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>2.201609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1204</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Teemu Pukki</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>2.169032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1176</td>\n",
       "      <td>MID</td>\n",
       "      <td>Mason Mount</td>\n",
       "      <td>Brighton and Hove Albion</td>\n",
       "      <td>2.159362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>882</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Tammy Abraham</td>\n",
       "      <td>Brighton and Hove Albion</td>\n",
       "      <td>2.026057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>585</td>\n",
       "      <td>MID</td>\n",
       "      <td>Sadio ManÃ©</td>\n",
       "      <td>Sheffield United</td>\n",
       "      <td>1.811376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>451</td>\n",
       "      <td>DEF</td>\n",
       "      <td>Martin Kelly</td>\n",
       "      <td>Norwich City</td>\n",
       "      <td>0.687089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>947</td>\n",
       "      <td>GK</td>\n",
       "      <td>David Button</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>0.213091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    player_id position                player_name                  opponent  \\\n",
       "10        385      MID            Kevin De Bruyne                   Everton   \n",
       "3         514      DEF           NicolÃ¡s Otamendi                   Everton   \n",
       "14        858      FWD  Pierre-Emerick Aubameyang         Manchester United   \n",
       "2         648      DEF     Trent Alexander-Arnold          Sheffield United   \n",
       "0         176       GK               David de Gea                   Arsenal   \n",
       "6        1154      DEF             John Lundstram                 Liverpool   \n",
       "4         977      DEF                  Issa Diop               Bournemouth   \n",
       "8         267      MID               Harry Wilson           West Ham United   \n",
       "11       1206      MID              Todd Cantwell            Crystal Palace   \n",
       "13       1204      FWD                Teemu Pukki            Crystal Palace   \n",
       "9        1176      MID                Mason Mount  Brighton and Hove Albion   \n",
       "12        882      FWD              Tammy Abraham  Brighton and Hove Albion   \n",
       "7         585      MID                 Sadio ManÃ©          Sheffield United   \n",
       "5         451      DEF               Martin Kelly              Norwich City   \n",
       "1         947       GK               David Button                   Chelsea   \n",
       "\n",
       "    predicted  \n",
       "10   3.586869  \n",
       "3    2.996146  \n",
       "14   2.766384  \n",
       "2    2.700203  \n",
       "0    2.552576  \n",
       "6    2.354345  \n",
       "4    2.343517  \n",
       "8    2.217601  \n",
       "11   2.201609  \n",
       "13   2.169032  \n",
       "9    2.159362  \n",
       "12   2.026057  \n",
       "7    1.811376  \n",
       "5    0.687089  \n",
       "1    0.213091  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_weekly_df.sort_values([\"predicted\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_gk_min = 1;\n",
    "\n",
    "cond_def_min = 3;\n",
    "cond_def_max = 5;\n",
    "\n",
    "cond_mid_min = 2;\n",
    "cond_mid_max = 5;\n",
    "\n",
    "cond_fwd_min = 1;\n",
    "cond_fwd_max = 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectTeam(df, max_players=11):\n",
    "    best11 = y_pred_weekly_df.sort_values([\"predicted\"], ascending=False)\n",
    "    df_ = pd.DataFrame(columns=best11.columns);\n",
    "\n",
    "    for index, row in best11.iterrows():\n",
    "        num_gk = df_[df_[\"position\"] == \"GK\"].shape[0];\n",
    "        num_def = df_[df_[\"position\"] == \"DEF\"].shape[0];\n",
    "        num_mid = df_[df_[\"position\"] == \"MID\"].shape[0];\n",
    "        num_fwd = df_[df_[\"position\"] == \"FWD\"].shape[0];\n",
    "        num_players = df_.shape[0];\n",
    "        \n",
    "        position = row[\"position\"];\n",
    "    \n",
    "        if (position == \"GK\" and num_gk < cond_gk_min and num_players < max_players):\n",
    "            df_ = df_.append(row);\n",
    "        if (position == \"DEF\" and (num_def < cond_def_min or \n",
    "            (num_def >= cond_def_min and num_def <= cond_def_max)) and num_players < max_players):\n",
    "            df_ = df_.append(row);\n",
    "        if (position == \"MID\" and (num_mid < cond_mid_min or \n",
    "            (num_mid >= cond_mid_min and num_mid <= cond_mid_max)) and num_players < max_players):\n",
    "            df_ = df_.append(row);\n",
    "        if (position == \"FWD\" and (num_fwd < cond_fwd_min or \n",
    "            (num_fwd >= cond_fwd_min and num_fwd <= cond_fwd_max)) and num_players < max_players):\n",
    "            df_ = df_.append(row);\n",
    "    \n",
    "    gk = list(df_[df_[\"position\"] == \"GK\"][\"player_name\"])\n",
    "    defenders = list(df_[df_[\"position\"] == \"DEF\"][\"player_name\"])\n",
    "    mids = list(df_[df_[\"position\"] == \"MID\"][\"player_name\"])\n",
    "    fwds = list(df_[df_[\"position\"] == \"FWD\"][\"player_name\"])\n",
    "    \n",
    "    selectedTeam = pd.DataFrame(columns=df_.columns);\n",
    "    selectedTeam = selectedTeam.append(df_[df_[\"position\"] == \"GK\"]);\n",
    "    selectedTeam = selectedTeam.append(df_[df_[\"position\"] == \"DEF\"]);\n",
    "    selectedTeam = selectedTeam.append(df_[df_[\"position\"] == \"MID\"]);\n",
    "    selectedTeam = selectedTeam.append(df_[df_[\"position\"] == \"FWD\"]);\n",
    "    \n",
    "    selectedTeam[\"predicted\"] = selectedTeam[\"predicted\"].apply(lambda pred: round(pred, 0))\n",
    "    \n",
    "    return selectedTeam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "best11 = selectTeam(y_pred_weekly_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_id</th>\n",
       "      <th>position</th>\n",
       "      <th>player_name</th>\n",
       "      <th>opponent</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176</td>\n",
       "      <td>GK</td>\n",
       "      <td>David de Gea</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>514</td>\n",
       "      <td>DEF</td>\n",
       "      <td>NicolÃ¡s Otamendi</td>\n",
       "      <td>Everton</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>648</td>\n",
       "      <td>DEF</td>\n",
       "      <td>Trent Alexander-Arnold</td>\n",
       "      <td>Sheffield United</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1154</td>\n",
       "      <td>DEF</td>\n",
       "      <td>John Lundstram</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>977</td>\n",
       "      <td>DEF</td>\n",
       "      <td>Issa Diop</td>\n",
       "      <td>Bournemouth</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>385</td>\n",
       "      <td>MID</td>\n",
       "      <td>Kevin De Bruyne</td>\n",
       "      <td>Everton</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>267</td>\n",
       "      <td>MID</td>\n",
       "      <td>Harry Wilson</td>\n",
       "      <td>West Ham United</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1206</td>\n",
       "      <td>MID</td>\n",
       "      <td>Todd Cantwell</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1176</td>\n",
       "      <td>MID</td>\n",
       "      <td>Mason Mount</td>\n",
       "      <td>Brighton and Hove Albion</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>858</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Pierre-Emerick Aubameyang</td>\n",
       "      <td>Manchester United</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1204</td>\n",
       "      <td>FWD</td>\n",
       "      <td>Teemu Pukki</td>\n",
       "      <td>Crystal Palace</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   player_id position                player_name                  opponent  \\\n",
       "0        176       GK               David de Gea                   Arsenal   \n",
       "3        514      DEF           NicolÃ¡s Otamendi                   Everton   \n",
       "2        648      DEF     Trent Alexander-Arnold          Sheffield United   \n",
       "6       1154      DEF             John Lundstram                 Liverpool   \n",
       "4        977      DEF                  Issa Diop               Bournemouth   \n",
       "10       385      MID            Kevin De Bruyne                   Everton   \n",
       "8        267      MID               Harry Wilson           West Ham United   \n",
       "11      1206      MID              Todd Cantwell            Crystal Palace   \n",
       "9       1176      MID                Mason Mount  Brighton and Hove Albion   \n",
       "14       858      FWD  Pierre-Emerick Aubameyang         Manchester United   \n",
       "13      1204      FWD                Teemu Pukki            Crystal Palace   \n",
       "\n",
       "    predicted  \n",
       "0         3.0  \n",
       "3         3.0  \n",
       "2         3.0  \n",
       "6         2.0  \n",
       "4         2.0  \n",
       "10        4.0  \n",
       "8         2.0  \n",
       "11        2.0  \n",
       "9         2.0  \n",
       "14        3.0  \n",
       "13        2.0  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(best11[\"predicted\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
